

## 原子性

- 实现事务原子性的两种协议

  - **面向应用层的** **TCC**
  - **数据库领域最常用的** **2PC**

- 2PC的三大问题

  - 相比于 TCC，2PC 的优点是借助了数据库的提交和回滚操作，不侵入业务逻辑。但是，它 也存在一些明显的问题:
    - **同步阻塞**。执行过程中，数据库要锁定对应的数据行。如果其他事务刚好也要操作这些数据行，那它 们就只能等待。其实同步阻塞只是设计方式，真正的问题在于这种设计会导致分布式事务 出现高延迟和性能的显著下降。
    - **单点故障**。事务管理器非常重要，一旦发生故障，数据库会一直阻塞下去。尤其是在第二阶段发生故障的话，所有数据库还都处于锁定事务资源的状态中，从而无法继续完成事务操作。
    - **数据不一致**。在第二阶段，当事务管理器向参与者发送 Commit 请求之后，发生了局部网络异常，导致 只有部分数据库接收到请求，但是其他数据库未接到请求所以无法提交事务，整个系统就 会出现数据不一致性的现象。**比如，小明的余额已经能够扣减，但是小红的余额没有增 加，这样就不符合原子性的要求了。**
  - **事实上，多数分布式数据库都是在 2PC 协议基础上改进，来保证分布式事务的原子性。**

- **NewSQL** **阵营:Percolator**

  - Percolator 模型同时涉及了隔离性和原子性的处理
  - 使用 Percolator 模型的前提是事务的参与者，即数据库，要**支持多版本并发控制 (MVCC)**。
  - Percolator 的流程
    - **第一，准备阶段**
      - 事务管理器向分片发送 Prepare 请求，包含了具体的数据操作要求。
      - **分片接到请求后要做两件事，写日志和添加私有版本。**关于私有版本，你可以简单理解 为，在 lock 字段上写入了标识信息的记录就是私有版本，只有当前事务能够操作，通常其 他事务不能读写这条记录。
      - 准备阶段结束的时候，两个分片都增加了私有版本记录，余额正好是转账顺利执行后的数 字。
      - **主锁的选择是随机的**，参与事务的记录都可能拥有主锁，但一个事务只能有一条记录拥有 主锁，其他参与事务的记录在 lock 字段记录了指针信息“primary@Ming.bal”，指向主 锁记录。
    - **第二，提交阶段**
      - 事务管理器只需要和拥有主锁的分片通讯，发送 Commit 指令，且不用 附带其他信息。
      - 分片 P1 增加了一条新记录时间戳为 8，指向时间戳为 7 的记录，后者在准备阶段写入的主锁也被抹去。这时候 7、8 两条记录不再是私有版本，所有事务都可以看到小明的余额变 为 2,700 元，事务结束。
      - **为什么在提交阶段不用更新小红的记录**
        - **Percolator 最有趣的设计就是这里，因为分片 P2 的最后一条记录，保存了指向主锁的指 针。**其他事务读取到 Hong7 这条记录时，会根据指针去查找 Ming.bal，发现记录已经提交，所以小红的记录虽然是私有版本格式，但仍然可视为已经生效了。
        - 当然，这种通过指针查找的方式，会给读操作增加额外的工作。如果每个事务都照做，性 能损耗就太大了。**所以，还会有其他异步线程来更新小红的余额记录**。
  - 对比 2PC 的问题，来看看 Percolator 模型有哪些改进
    - **数据不一致**
      - 2PC 的一致性问题主要缘自第二阶段，不能确保事务管理器与多个参与者的通讯始终正 常。
      - **但在 Percolator 的第二阶段，事务管理器只需要与一个分片通讯，这个 Commit 操作本身就是原子的**。所以，事务的状态自然也是原子的，一致性问题被完美解决了。
    - **单点故障**
      - Percolator 通过**日志和异步线程**的方式弱化了这个问题。
      - **一是，Percolator 引入的异步线程可以在事务管理器宕机后，回滚各个分片上的事务，提供了善后手段**，不会让分片上被占用的资源无法释放。
      - **二是，事务管理器可以用记录日志的方式使自身无状态化，日志通过共识算法同时保存在 系统的多个节点上。**这样，事务管理器宕机后，可以在其他节点启动新的事务管理器，基 于日志恢复事务操作。
    - **Percolator 模型在分布式数据库的工程实践中被广泛借鉴。比如，分布式数据库 TiDB**，完 全按照该模型实现了事务处理;CockroachDB 也从 Percolator 模型获得灵感，设计了自 己的 2PC 协议。
    - **CockroachDB** 的变化在于没有随机选择主锁，而是引入了一张**全局事务表**，所有分片记录 的指针指向了这个事务表中对应的事务记录。

- **GoldenDB** **的一阶段提交**

  - **GoldenDB 遵循 PGXC 架构，包含了四种角色:协调节点、数据节点、全局事务器和管理节点**，其中协调节点和数据节点均有多个。GoldenDB 的数据节点由 MySQL 担任，后者 是独立的单体数据库。
  - 虽然名字叫“一阶段提交”，但 GoldenDB 的流程依然可以分为两个阶段。
    - **第一阶段，GoldenDB 的协调节点接到事务后，在全局事务管理器(GTM)的全局事务列表中将事务标记成活跃的状态。**
      - 这个标记过程是 GoldenDB 的主要改进点，实质是通过全局事务列表来申请资源，规避可能存在的事务竞争。
      - 这样的好处是避免了与所有参与者的通讯，也减少了很多无效的资源锁定动作。
    - **第二阶段，协调节点把一个全局事务分拆成若干子事务，分配给对应的 MySQL 去执行。** 如果所有操作成功，协调者节点会将全局事务列表中的事务标记为结束，整个事务处理完 成。如果失败，子事务在单机上自动回滚，而后反馈给协调者节点，后者向所有数据节点 下发回滚指令。
  - GoldenDB 的“一阶段提交”，本质上是改变了资源的申请方式，更准确的说法是，**并发控制手段从锁调度变为时间戳排序(Timestamp Ordering)**。**这样，在正常情况下协调节点与数据节点只通讯一次**，降低了网络不确定性的影响，数据库的整体性能有明显提 升。**因为第一阶段不涉及数据节点的操作，也就弱化了数据一致性和单点故障的问题。**

- **事务延迟估算**

  - 整个 2PC 的事务延迟由两个阶段组成，可以用公式表达为:

    - $$
      L_{txn}=L_{prep}+L_{commit}
      $$

    - 其中，*L_prep* 是准备阶段的延迟，*L_commit* 是提交阶段的延迟。

  - 准备阶段，它是事务操作的主体，包含若干读操作和若干写操作。

    - 我们选一种最乐观的情况， CockroachDB。
      - 因为它采用 P2P 架构，每个节点既承担了客户端服务接入的工作，也有 请求处理和数据存储的职能。所以，最理想的情况是，读操作的客户端接入节点，同时是 当前事务所访问数据的 Leader 节点，那么所有读取就都是本地操作。
      - 磁盘操作相对网络延迟来说是极短的，所以我们可以忽略掉读取时间。那么，准备阶段的 延迟主要由写入操作决定
    - 我们都知道，分布式数据库的写入，并不是简单的本地操作，而是使用共识算法同时在多个节点上写入数据。所以，一次写入操作延迟等于一轮共识算法开销

  - 第二阶段，提交阶段。

    - Percolator 模型，它的提交阶段只需要写入一次数据，修改整个事务的状态。**对于 CockroachDB，这个事务标识可以保存在本地。那么提交操作的延迟也是一轮共识算法**

  - 小明给小红转账，金额是 500 元

    - 在这个转账事务中，包含两条写操作 SQL，分别是扣减小明账户余额和增加小红账户余 额，W 等于 2。再加上提交操作，一共有 3 个 *L_c*(用 *L_c* 代表一 轮共识算法的用时)。我们可以看到，这个公式里事务的延 迟是与写操作 SQL 的数量线性相关的，而真实场景中通常都会包含多个写操作，那事务延 迟肯定不能让人满意。

- 优化方法

  - **缓存写提交**
    - 第一个办法是**将所有写操作缓存起来，直到 commit 语句时一起执行**，这种方式称为 Buffering Writes until Commit，我把它翻译为**“缓存写提交”**。而 TiDB 的事务处理中 就采用这种方式。
      - 所有从 Client 端提交的 SQL 首先会缓存在 TiDB 节点，只有当客户端发起 Commit 时， TiDB 节点才会启动两阶段提交，将 SQL 被转换为 TiKV 的操作。这样，显然可以压缩第一阶段的延迟，把多个写操作 SQL 压缩到大约一轮共识算法的时间。
    - 但缓存写提交存在两个明显的缺点。
      - 首先是在客户端发送 Commit 前，SQL 要被缓存起来，**如果某个业务场景同时存在长事务和海量并发的特点，那么这个缓存就可能被撑爆或者成为瓶颈。**
      - 其次是客户端看到的 **SQL 交互过程发生了变化**，在 MySQL 中如果出现事务竞争，判断优 先级的规则是 First Write Win，也就是对同一条记录先执行写操作的事务获胜。**而 TiDB 因为缓存了所有写 SQL，所以就变成了 First Commit Win，也就是先提交的事务获胜**。
  - **管道**
    - 既能缩短延迟，又能保持交互事务
    - 这就是 CockroachDB 采用的方式，称为 Pipeline。**具体过程就是在准备阶段是按照顺序将 SQL 转换为 K/V 操作并执行，但是并不等待返回结果，直接执行下一个 K/V 操作。**
      - 这样，准备阶段的延迟，等于最慢的一个写操作延迟，也就是一轮共识算法的开销
      - 同样的操作，按照 Pipeline 方式，增加小红账户余额时并不等待小明扣减账户的动作结 束，两条 SQL 的执行时间约等于 1 个 *L_c*。加上提交阶段的 1 个 *L_c*，一共是 2 个 *L_c*，并 且延迟也不再随着 SQL 数量增加而延长。
  - **并行提交**
    - 但是，像 CockroachDB、YugabyteDB 这样分布式数据库，它们的目标就是全球化部署，所以还要努力去压缩事务延迟。
      - 可是，还能怎么压缩呢?**准备阶段的操作已经压缩到极限了，commit 这个动作也不能少 呀，那就只有一个办法，让这两个动作并行执行。**
    - 并行执行的过程是这样的。
      - **准备阶段的操作，在 CockroachDB 中被称为意向写。这个并行执行就是在执行意向写的同时，就写入事务标志**，当然这个时候不能确定事务是否提交成功的，所以要引入一个新 的状态“Staging”，表示事务正在进行。那么这个记录事务状态的落盘操作和意向写大致 是同步发生的，所以只有一轮共识算法开销。
        - **Writes 部分是意向写的 Key。这是留给异步进程的线索，通过这些 Key 是否写成功，可以 倒推出事务是否提交成功。**
      - 而**客户端得到所有意向写的成功反馈后，可以直接返回调用方事务提交成功**。**注意!这个 地方就是关键了**，**客户端只在当前进程内判断事务提交成功后，不维护事务状态，而直接返回调用方;事后由异步线程根据事务表中的线索，再次确认事务的状态，并落盘维护状态记录。**这样事务操作中就减少了一轮共识算法开销。
    - 你有没有发现，并行提交的优化思路其实和 Percolator 很相似，那就是**不要纠结于在一次 事务中搞定所有事情，可以只做最少的工作，留下必要的线索，就可以达到极致的速度。** 而后续的异步进程，只要根据线索，完成收尾工作就可以了。
    - 并行提交不是BASE，仍然强一致的。因为，**负责发起Pipeline写入的线程是明确知道 这些写入都成功了**，注意，这个成功是说事务涉及的每个Raft组都写入成功了，那么此时线程可以判定事务已经提交成功了。但是，如果接下来它写盘记录事务的最新状态，就会带来新一轮共识算法开销，而不写盘也不会影响事务状态，所以它直接返回客户端，使得延迟更短。
    - 异步化的只是事务状态的落盘操作，而事务状态的确认仍然是同步完成的。







## 隔离性

- 多版本并发控制(Multi-Version Concurrency Control，MVCC)就是**通过记录数据项历史版本的方式，来提升系统应对多事务访问的并发处理能力**。

  - 事务 T1、T2 先后启动，分别对数据库执行写操作和读操作。
    - 果先执行的是 T1 写事务，除了磁盘写入数据的时间， 由**于要保证数据库的高可靠，至少还有一个备库同步复制主库的变更内容。**这样，阻塞时 间就要再加上一次网络通讯的开销。
    - 如果先执行的是 T2 只读事务，情况也好不到哪去，虽然不用考虑复制问题，但是读操作通 常会涉及更大范围的数据，这样一来加锁的记录会更多，被阻塞的写操作也就更多。而 且，**只读事务往往要执行更加复杂的计算，阻塞的时间也就更长。**
    - 所以说，用锁解决读写冲突问题，带来的事务阻塞开销还是不小的。相比之下，用 MVCC 来解决读写冲突，就不存在阻塞问题，要优雅得多了。
  - **PGXC 的 MVCC 实现方式其实就 是单体数据库的实现方式。**

- 单体数据库的MVCC

  - **MVCC 有三类存储方式，一类是将历史版本直接存在数据表中的，称为 Appane-Only**， 典型代表是 PostgreSQL。另外两类都是在独立的表空间存储历史版本，它们区别在于存 储的方式是全量还是增量。**增量存储就是只存储与版本间变更的部分，这种方式称为 Delta**，典型代表是 MySQL 和 Oracle。**全量 存储则是将每个版本的数据全部存储下来，这种方式称为 Time-Travle**，典型代表是 HANA。

  - 三种存储方式的优缺点

    - | 存储方式 | Appane-Only**                                  | Delta                        | Time-Travle    |
      | -------- | ---------------------------------------------- | ---------------------------- | -------------- |
      | 优点     | 可快速回滚、磁盘操作开销小、不会出现回滚段耗尽 | 当前版本查询快、存储要求低   | 当前版本查询快 |
      | 缺点     | 当前版本查询慢，存储要求高                     | 计算开销大、有回滚段耗尽问题 | 存储要求高     |

- **PGXC** **读写冲突处理**

  - PGXC 在实现 RR 时遇到的两个挑战，也就是实现快照的两个挑战
    - **一是如何保证产生单调递增事务 ID。**每个数据节点自行处理显然不行，这就需要由一个集 中点来统一生成。
    - **二是如何提供全局快照。**每个事务要把自己的状态发送给一个集中点，由它维护一个全局 事务列表，并向所有事务提供快照。
  - **所以，PGXC 风格的分布式数据库都有这样一个集中点，通常称为全局事务管理器 (GTM)。**又因为事务 ID 是单调递增的，用来衡量事务发生的先后顺序，和时间戳作用相近，所以全局事务管理器也被称为“全局时钟”。

- **NewSQL** **读写冲突处理**

  - TiDB
    - TiDB 底层是分布式键值系统，我们假设两个事务操作同一个数据项。其中，事务 T1 执行 写操作，由 Prewrite 和 Commit 两个阶段构成，对应了我们之前介绍的两阶段提交协议 (2PC)。**这里你也可以简单理解为 T1的写操作分成了两个阶段，T2 在这两个阶段之间试图执行读操作，但是 T2 会被阻塞，直 到 T1 完成后，T2 才能继续执行。**
    - **你肯定会非常惊讶，这不是 MVCC 出现前的读写阻塞吗?**
      - **TiDB 为什么没有使用快照读取历史版本呢? TiDB 官方文档并没有说明背后的思路，我猜 问题出在全局事务列表上，**因为 TiDB 根本没有设计全局事务列表。当然这应该不是设计上 的疏忽，我更愿意把它理解为一种权衡，是在读写效率和全局事务列表的维护代价之间的 选择。
    - **事实上，PGXC 中的全局事务管理器就是一个单点，很容易成为性能的瓶颈，而分布式系 统一个普遍的设计思想就是要避免对单点的依赖**。当然，TiDB 的这个设计付出的代价也是 巨大的。虽然，TiDB 在 3.0 版本后增加了悲观锁，设计稍有变化，但大体仍是这样。
  - CockroachDB
    - **依旧是 T1 事务先执行写操作，中途 T2 事务启动，执行读操作，此时 T2 会被优先执行。 待 T2 完成后，T1 事务被重启。**重启的意思是 T1 获得一个新的时间戳(等同于事务 ID) 并重新执行。
    - CockroachDB 没有使用快照，不是因为没有全局事务列表，而是因为**它的隔离级别目标不 是 RR，而是 SSI，也就是可串行化。**
    - **总之，CockroachDB 对于读写 冲突、写写冲突采用了几乎同样的处理方式。**
    - 事实上，被重启的事务 并不一定是执行写操作的事务。CockroachDB 的每个事务都有一个优先级，出现事务冲突 时会比较两个事务的**优先级**，高优先级的事务继续执行，低优先级的事务则被重启。而被 重启事务的优先级也会提升，避免总是在竞争中失败，最终被“饿死”。

- **不确定时间窗口**

  - 共有 7 个数据库事务，T1 到 T7，其中 T6 是读事 务，其他都是写事务。**事务 T2 结束的时间点(记为 T2-C)早于事务 T6 启动的时间点 (记为 T6-S)，这是基于数据记录上的时间戳得出的判断，但实际上这个判断很可能是错 的。**
    - **这是因为时间误差的存在，T2-C 时间点附近会形成一个不确定时间窗 口，也称为置信区间或可信区间。**严格来说，我们只能确定 T2-C 在这个时间窗口内，但 无法更准确地判断具体时间点。同样，T6-S 也只是一个时间窗口。时间误差不能消除，但 可以通过工程方式控制在一定范围内，例如在 Spanner 中这个不确定时间窗口(记为ɛ) 最大不超过 7 毫秒，平均是 4 毫秒。
  - 在这个案例中，当我们还原两个时间窗口后，发现两者存在重叠，所以无法判断 T2-C 与 T6-S 的先后关系，怎么解决呢?
    - 只有避免时间窗口出现重叠。 
    - 答案是等待。“waiting out the uncertainty”，用等待来消除不确定性。
    - **在实践中，我们看到有两种方式可供选择，分别是写等待和读等待。**

- **写等待:**Spanner

  - Spanner 选择了写等待方式，更准确地说是用**提交等待(commit-wait)**来消除不确定性。
  - **Spanner 是直接将时间误差暴露出来的**，所以调用当前时间函数 TT.now() 时，会获得的 是一个区间对象 TTinterval。它的两个边界值 earliest 和 latest 分别代表了最早可能时间 和最晚可能时间，而绝对时间就在这两者之间。另外，Spanner 还提供了 TT.before() 和 TT.after() 作为辅助函数，其中 TT.after() 用于判断当前时间是否晚于指定时间。
  - **理论等待时间**
    - **事务 Ta 在获得“提交时间戳”S 后，再等待ɛ时间后才写盘 并提交事务。真正的提交时间是晚于“提交时间戳”的，中间这段时间就是等待。**这样 Tb 事务启动后，能够得到的最早时间 TT2.earliet 肯定不会早于 S 时刻，所以 Tb 就一定能够 读取到 Ta。这样就符合线性一致性的要求了。
    - 综上，事务获得“提交时间戳”后必须等待ɛ时间才能写入磁盘，即 commit-wait。
  - 有什么不对劲的地方吗
    - 对，就是那个绝对时间 S。都说了半天时间有误差，那又怎么可能拿到一个绝对时间呢? 这不是自相矛盾吗?
    - Spanner 确实拿不到绝对时间
  - **实际等待时间**
    - **Spanner 将含有写操作的事务定义为读写事务。**读写事务的写操作会以两阶段提交 (2PC)的方式执行。
      - 2PC 的第一阶段是预备阶段，每个参与者都会获取一个**“预备时间戳”**，与数据一起写入 日志。第二阶段，协调节点写入日志时需要一个**“提交时间戳”**，而它必须要大于任何参 与者的“预备时间戳”。
      - 所以，协调节点调用 TT.now() 函数后，要取该时间区间的lastest 值(记为 s)，而且 s 必须大于所有参与者的“预备时间戳”，作为“提交时间 戳”。
    - 针对同一个数据项，事务 T8 和 T9 分别对进行写入和读取操作。T8 在绝对时间 100ms 的 时候，调用 TT.now() 函数，得到一个时间区间[99,103]，选择最大值 103 作为提交时间 戳，而后等待 8 毫秒(即 2ɛ)后提交。
      - **这样，无论如何 T9 事务启动时间都晚于 T8 的“提交时间戳”，也就能读取到 T8 的更 新。**
    - **回顾一下这个过程，第一个时间差是 2PC 带来的**，如果换成其他事务模型也许可以避免， **而第二个时间差是真正的 commit-wait，来自时间的不确定性，是不能避免的**。
    - TrueTime 的平均误差是 4 毫秒，commit-wait 需要等待两个周期，那 Spanner 读写事 务的平均延迟必然大于等于 8 毫秒。为啥有人会说 Spanner 的 TPS 是 125 呢?原因就是 这个了。其实，这只是事务操作数据出现重叠时的吞吐量，而无关的读写事务是可以并行 处理的。

- **读等待:**CockroachDB

  - 因为 CockroachDB 采用混合逻辑时钟(HLC)，所以对于没有直接关联的事务，只能用 物理时钟比较先后关系。CockroachDB 各节点的物理时钟使用 NTP 机制同步，误差在几 十至几百毫秒之间
  - **写等待模式下，所有包含写操作的事务都受到影响，要延后提交;而读等待只在特殊条件 下才被触发，影响的范围要小得多。**
  - 事务 T6 启动获得了一个时间戳 T6-S1，此时虽然事务 T2 已经在 T2-C 提交，但 **T2-C 与 T6-S1 的间隔小于集群的时间偏移量，所以无法判断 T6 的提交是否真的早于 T2。**
    - 这时，CockroachDB 的办法是**重启(Restart)读操作的事务，就是让 T6 获得一个更晚 的时间戳 T6-S2，使得 T6-S2 与 T2-C 的间隔大于 offset，那么就能读取 T2 的写入了。**
    - **不过，接下来又出现更复杂的情况， T6-S2 与 T3 的提交时间戳 T3-C 间隔太近，又落入 了 T3 的不确定时间窗口，所以 T6 事务还需要再次重启。**而 T3 之后，T6 还要重启越过 T4 的不确定时间窗口。
    - **最后，当 T6 拿到时间戳 T6-S4 后，终于跳过了所有不确定时间窗口，读等待过程到此结 束**，T6 可以正式开始它的工作了。
  - 在这个过程中，可以看到**读等待的两个特点:一是偶发**，只有当读操作与已提交事务间隔 小于设置的时间误差时才会发生;**二是等待时间的更长**，因为事务在重启后可能落入下一 个不确定时间窗口，所以也许需要经过多次重启。

- **并发控制技术的分类**

  - 要想让系统支持海量并发，很重要的基础就是数据库的并发处理能力，而这里面最重要的就是对写写冲突的并发控制。
  - 在这个技术体系中，虽然有多种不同的划分方式，**但最为大家熟知就是悲观协议和乐观协议两大类。**
  - 落到实现机制上，有一个广泛被提到的定义版本。**乐观协议就是直接提交**，遇 到冲突就回滚;**悲观协议**就是在真正提交事务前，**先尝试对需要修改的资源上锁**，只有在 确保事务一定能够执行成功后，才开始提交。
  - 总之，这个版本的核心就是，悲观协议是使用锁的，而乐观协议是不使用锁的

- **乐观锁:TiDB**

  - TiDB 的乐观锁基本上就是 Percolator 模型，它的运行过程可以分为三个阶段。
    - **选择 Primary Row**
      - 收集所有参与修改的行，从中随机选择一行，作为这个事务的 Primary Row，这一行是拥有锁的，称为 Primary Lock，而且**这个锁会负责标记整个事务的完成状态。**所有其他修改 行也有锁，称为 Secondary Lock，都会保留指向 Primary Row 的指针。
    - **写入阶段**
      - **按照两阶段提交的顺序，执行第一阶段。**每个修改行都会执行上锁并执行“prewrite”， prewrite 就是将数据写入私有版本，其他事务不可见。注意这时候，每个修改行都可能碰 到锁冲突的情况，**如果冲突了，就终止事务，返回给 TiDB，那么整个事务也就终止了。如 果所有修改行都顺利上锁，完成 prewrite，第一阶段结束。**
    - **提交阶段**
      - **这是两阶段提交的第二阶段**，提交 Primary Row，也就是写入新版本的提交记录并清除 Primary Lock，如果顺利完成，那么这个事务整体也就完成了，反之就是失败。**而 Secondary Rows 上的锁，则会交给异步线程根据 Primary Lock 的状态去清理。**

- 你看这个过程中不仅有锁，而且锁的数量还不少。那么，为什么又说它是乐观协议呢?

  - **并发控制的三个阶段**
    - **乐观协议 和悲观协议的操作，都统一成四个阶段，分别是有效性验证(V)、读(R)、计算(C)和写(W)。两者的区别就是这四个阶段的顺序不同**:悲观协议的操作顺序是 VRCW，而 乐观协议的操作顺序则是 RCVW。因为在比较两种协议时，计算(C)这个阶段没有实质 影响，可以忽略掉。那么简化后，悲观协议的顺序是 VRW，而乐观协议的顺序就是 RVW。
  - RVW 的三阶段划分
    - **读阶段(Read Pharse)**，每个事务对数据项的局部拷贝进行更新。
    - **有效性确认阶段(Validation Pharse)**，验证准备提交的事务。
    - **写阶段(Write Pharse)**，将读阶段的更新结果写入到数据库中，接受事务的提交结果。
  - 还有一种关于乐观与悲观的表述，也与三阶段的顺序相呼应。乐观，重在事后检测，在事 务提交时检查是否满足隔离级别，如果满足则提交，否则回滚并自动重新执行。悲观，重在事前预防，在事务执行时检查是否满足隔离级别，如果满足则继续执行，否则等待或回 滚。
  - 我们再回到 TiDB 的乐观锁。**虽然对于每一个修改行来说，TiDB 都做了有效性验证，而且 顺序是 VRW，可以说是悲观的，但这只是局部的有效性验证**;从整体看，**TiDB 没有做全 局有效性验证，不符合 VRW 顺序，所以还是相对乐观的。**
    - 局部性是说，只要局部加锁成功，就开始局部的写入了。TiDB的乐观锁也不是在所有 记录加锁成功以后，才写入新的数据版本。如果是，那还会因为事务冲突多而频繁回滚吗?再想 想:)
    - 怎么加锁才是全局有效性验证?TiDB后来增加的悲观锁就是例子。这个加锁动作是统一的，只要 有一条记录加锁不成功，就不会启动任何写入动作。

  



## **全球化部署**

- 我们不妨给全球化部署起一个更接地气的小名，**“异地多活”**。

  - 首先，异地多活是高可用架 构的一种实现方式，它是以整个应用系统为单位，一般来说会分为应用和数据库两部分。
    - **应用部分通常是无状态的，这个无状态就是说应用处理每个请求时是不需要从本地加载上下文数据的。**这样启动多个应用服务器就没有什么额外的成本，应用之间也没有上下文依赖，所以就很容易做到多活。
    - **数据库节点要最终持久化数据，所有的服务都要基于已有的数据，并且这些数据内容还在 不断地变化。**任何新的服务节点在接入这个体系后，相互之间还会存在影响，所以数据库 服务有逻辑很重的上下文。因此数据库的多活的难度就大多了，也就产生了不同版本的解读。

- **单体数据库**

  - **异地容灾**
    - 异地容灾是异地多活的低配版，它往往是这样的架构。
      - 整个方案涉及同城和异地两个机房，都部署了同样的应用服务器和数据库，**其中应用服务 器都处于运行状态可以处理请求，也就是应用多活。只有同城机房的数据库处于运行状 态，异地机房数据库并不启动**，不过会通过底层存储设备向异地机房同步数据。然后，所 有应用都链接到同城机房的数据库。
    - 显然，这个多活只是应用服务器的多活，两地的数据库并不同时提供服务。这种模式下， 异地应用虽然靠近用户，但仍然要访问远端的数据库，对延迟的改善并不大。**在系统正常 运行情况下，异地数据库并没有实际产出，造成了资源的浪费。**
  - **异地读写分离**
    - 在异地读写分离模式下，异地数据库和主机房数据库同时对外提供服务，**但服务类型限制为只读服务，但只读服务的数据一致性是不保证的。**
    - 读写分离模式下，异地数据库也投入了使用，不再是闲置的资源。但是很多场景下，**只读 服务在业务服务中的占比还是比较低的，再加上不能保证数据的强一致性，适用范围又被 进一步缩小。**所以，对于部分业务场景，异地数据库节点可能还是运行在低负载水平下。
  - **双向同步**
    - **双向同步模式下，同城和异地的数据库同时提供服务，并且是读写服务均开放。但有一个 重要的约束，就是两地读写的对象必须是不同的数据**，区分方式可以是不同的表或者表内 的不同记录，这个约束是为了保证两地数据操作不会冲突。因为不需要处理跨区域的事务冲突，所以两地数据库之间就可以采用异步同步的方式。
    - 这个模式下，两地处理的数据是泾渭分明的，所以实质上是两个独立的应用实例，或者可以说是两个独立的单元，而两个单元之间又相互备 份数据，有了这些数据可以容灾，也可以开放只读服务。当然，这个只读服务同样是不保证数据一致性的。
    - 可以说，双向同步是单元化架构和异地读写分离的混合，异地机房的资源被充分使用了。 **但双向同步没有解决一个根本问题，就是两地仍然不能处理同样的数据，对于一个完整的 系统来说，这还是有很大局限性的。**

- **分布式数据库**

  - 分布式数据库的数据组织单位是更细粒度的分片，又有了 Raft 协议的加持，所以就有了更 加灵活的模式。
  - **机房级容灾(两地三中心五副本)**
    - 这种模式下，每个分片都有 5 个 副本，在同城的双机房各部署两个副本，异地机房部署一个副本。
    - 这个模式有三个特点:
      - 异地备份。保留了“异地容灾”模式下的数据同步功能，但因为同样要保证低延迟，所以也做不到 RPO(Recovery Point Objective， 恢复点目标)为零。
      - 容灾能力。如果同城机房有一个不可用或者是同城机房间的网络出现故障，异地机房节点的投票就会 发挥作用，依然可以和同城可用的那个机房共同达成多数投票，那么数据库的服务就仍然 可以正常运行，当然这时提交过程必须要异地通讯，所以延迟会受到一定程度影响。
      - 同城访问低延迟。由于 Raft 或 Paxos 都是多数派协议，那么任何写操作时，同城的四个副本就能够超过半 数完成提交，这样就不会因为与异地机房通讯时间长而推高数据库的操作延迟。
    - 两地三中心虽然可以容灾，但对于异地机房来说 RPO 不为零，在更加苛刻的场景下，仍然 受到挑战。这也就催生了三地五副本模式，来实现 RPO 为零的城市级容灾。
  - **城市级容灾(三地五副本)**
    - 两个同城机房变成两个相临城市的机房，这 样总共是在三个城市部署。
    - 这种模式在容灾能力和延迟之间做了权衡，牺牲一点延迟，换来城市级别的容灾。比如， 在北京和天津建立两座机房，两个城市距离不算太远，延迟在 10 毫秒以内，还是可以接受 的。不过，距离较近的城市对区域性灾难的抵御能力不强，还不是真正意义上的异地。
    - 顺着这思路，还有更大规模的三地五中心七副本。**但无论如何，只要不放弃低延迟，真正 异地机房就无法做到 RPO 为零。**
  - **无论是两地三中心五副本还是三地五副本，它们更像是单体数据库异地容灾的加强版。**因 为，其中的异地机房始终是一个替补的角色，而那些异地的应用服务器也依然花费很多时 间访问远端的数据库。

- **架构问题**

  - 为什么会这样呢?因为**有些分布式数据库会有一个限制条件，就是所有的 Leader 节点必须固定在同城主机房**，而这就导致了资源使用率大幅下降。TiDB 和 OceanBase 都是这种情 况。
  - Raft 协议下，所有读写都是发送到 Leader 节点，Follower 节点是没有太大负载的。**Raft 协议的复制单位是分片级的，所以理论上一个节点可以既是一些分片的 Leader，又是另一 些分片的 Follower。也就是说，通过 Leader 和 Follower 混合部署可以充分利用硬件资 源。**
  - **但是如果主副本只能存在同一个机房，那就意味着另外三个机房的节点**，也就是有整个集 群五分之三的资源，**在绝大多数时候都处于低负载状态。**这显然是不经济的。
    - 其实，**这个限制条件就是全局时钟导致的**。具体来说，就是单时间 源的授时服务器不能距离 Leader 太远，否则会增加通讯延迟，性能就受到很大影响，极端 情况下还会出现异常。
    - 增加延迟比较好理解，因为距离远了嘛。那异常是怎么回事呢?
      - 我们把这种异常称为“**远端写入时间戳异常**”
      - 如果远端计算节点距离时钟节点过远，那么当并发较大且事务冲突较多 时，异地机房就会出现频繁的写入失败。这种业务场景并不罕见，**当我们网购付款时就会出现多个事务在短时间内竞争修改商户的账户余额的情况。**

- **全球化部署**

  - 全球化部署的**前提是多时间源、多点授时，这样不同分片的主副本就可以分散在多个机房**。那么数据库服务可以尽量靠近用户，而应用系统也可以访问本地的分片主副本，整体 效果达到等同于单元化部署的效果。
  - 当出现跨多地分片参与同一个分布式事务的情况，全球化部署模式也可以很好地支持。**由于参与分片跨越更大的物理范围，所以延迟就受到影响，这一点是无法避免的。还有刚刚 提到的“远端写入时间戳异常”，因为每个机房都可以就近获得时钟，那么发生异常的概 率也会大幅下降。**
  - **全球化部署模式下，异地机房所有的节点都是处于运行状态的，异地机房不再是替补角色**，和同城机房一样提供对等的支持能力，所有机房同等重要。
  - **在任何机房发生灾难时，主副本会漂移到其他机房，整个系统处于较为稳定的高可用状 态。**而应用系统通过访问本地机房的数据库分片主副本就可以完成多数操作，这些发生在 距离用户最近的机房，所以延迟可以控制到很低。这样就做到了我们说的永不宕机和近在 咫尺。

  

  

  

  

##  容灾与备份

- **作为一个数据库的高可用方案，首先要解决的是数据恢复的完整性**，也就是我们提过多次 的 **RPO**。**这就需要及时将数据复制到逃生库**。通常异构数据库间的数据复制有三种可选方式:

  - 数据文件
    - 数据文件是指，在数据库之间通过文件导入导出的方式同步数据。这是一种针对全量数据 的批量操作，如果要实现增量加载，则需要在数据库表的结构上做特殊设计。显然，数据 文件的方式速度比较慢，而且对表结构设计有侵入性，所以不是最优选择。
  - ETL(Extract-Transform-Load) 
    - ETL 是指包含了抽取(Extract)、转换(Transform)和加载(Load)三个阶段的数据加 工过程，可以直连两端的数据库，也可以配合数据文件一起用，同样必须依赖表结构的特 殊设计才能实现增量抽取，而且在两个数据库之间的耦合更加紧密。
  - CDC(Change Data Capture)
    - CDC 其实是一个更合适的选择。**CDC 的功能可以直接按照字面意思理解，就是用来捕获变更数据的组件，它的工作原理是通过读取上游的 redo log 来生成 SQL 语句发送给下游。**它能够捕捉所有 DML 的变化，比如 delete 和 update，而且可以配合 redo log 的设置记录前值和后值。
    - 相比之下，CDC 的业务侵入性非常小，不改动表结构的情况下，下游系统就可以准确同步数据的增量变化。另外，CDC 的时效性较好，对源端数据库的资源占用也不大，通常在 5%-10% 之间。

- **逃生方案**

  - 对于分布式数据库来说，要选择一个更加成熟、稳定的逃生库，答案显然就是单体数据库 了。
  - **日志格式适配**
    - 逃生方案的关键设计就是数据异步复制，而载体就是日志文件。既然是异构数据库，那日 志文件就有差别吧，要怎么处理呢?
    - **其中 PGXC 风格分布式数据库，它的数据节点是直接复用了 MySQL 和 PostgreSQL 单体数据库。**这就意味着，如果本来你熟悉的单体数据库是 MySQL，现在又恰好采用了 MySQL 为内核的分布式数据库，比如 GoldenDB、TDSQL，那么这个方案处理起来就容易些，因为两者都是**基于 Binlog 完成数据复制的。**
    - 而如果你选择了 NewSQL 分布式数据库也没关系。**虽然 NewSQL 没有复用单体数据库， 但为了与技术生态更好的融合，它们通常都会兼容某类单体数据库，大多数是在 MySQL 和 PostgreSQL 中选择一个**，比如 TiDB 兼容 MySQL，而 CockroachDB 兼容 PostgreSQL。TiDB 还扩展了日志功能，通过 Binlog 组件直接输出 SQL，可以加载到下 游的 MySQL。
  - **处理性能适配**
    - **用单体数据库来做逃生库，这里其实有一个悖论。那就是， 我们选择分布式的多数原因就是单体不能满足性能需求**，那么这个高可用方案要再切换回 单体，那单体能够扛得住这个性能压力呢?
    - **当然，性能问题还有缓释手段，就是增加一个分布式消息队列来缓存数据，降低逃生库的 性能压力。**
    - 另外，有了分布式消息队列的缓冲，我们就可以更加方便地完成异构数据的格式转换。当 然，日志格式的处理是没有统一解决方案的，如果你没有能力做二次开发，就只能寄希望 于产品厂商提供相应的工具。
  - **事务一致性**
    - 我们知道数据库事务中操作顺序是非常重要的，而**日志中记录顺序必须与事务实际操作顺 序严格一致，这样才能确保通过重放日志恢复出相同的数据库**。可以说，数据库的备份恢 复完全建立在日志的有序性基础上。逃生库也是一样的，**要想准确的体现数据，必须得到 顺序严格一致的日志，只不过这个日志不再是文件的形式，而是实时动态推送的变更消息流(Change Feed)。**
    - 对于分布式数据库来说，就是一个设计上的 挑战了。
    - **如果事务都在分片内完成**，那么每个分片的处理逻辑和单体数据库就是完全一样的，将各自的日志信息发送给 Kafka 即可。
      - 对于 NewSQL 来说，因为每个分片就是一个独立的 Raft Group，有对应的 WAL 日志， 所以要按分片向 Kafka 发送增量变化。比如，**CockroachDB 就是直接将每个分片 Leader 的 Raft 日志发送出去，这样处理起来最简单。**
    - 但是，分布式数据库的复杂性就在于跨分片操作，尤其是跨分片事务，也就是分布式事务。
      - 一个分布式事务往往涉及很多数据项，这些数据都可能被记录在不同的分片中，理论上可以包含集群内的所有分片。**如果每个分片独立发送数据，下游收到数据的顺序就很可 能与数据产生的顺序不同，那么逃生库就会看到不同的执行结果。**
      - **产生这个问题原因在于，变更消息中的时间戳只是标识了数据产生的时间，这并不代表逃生库能够在同一时间收到所有相同时间戳的变更消息**，也就不能用更晚的时间戳来代表前一个事务的变更消息已经接受完毕。那么，**要怎么知道同一时间戳的数据已经接受完毕了呢**?
        - 为了解决这个问题，CockroachDB 引入了一个**特殊时间戳标志“Resolved”**，用来表示 当前节点上这个时间戳已经关闭。**结合上面的例子，它的意思就是一旦 Node1 发 出“ts1:Resolved”消息后，则 Node1 就不会再发出任何时间戳小于或者等于 ts1 的变 更消息。**
        - **在每个节点的变更消息流中增加了 Resolved 消息后，逃生库就可以在 T2 时间判断，所有 ts1 的变更消息已经发送完毕，可以执行整个事务操作了。**

  ​	

