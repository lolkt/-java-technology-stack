## 分布式系统

- 异常
  - **机器宕机**
  - 网络异常
    - **消息丢失，两片节点之间彼此完全无法通信**，即出现了“网络分化”；
    - **消息乱序**，有一定的概率不是按照发送时的顺序依次到达目的节点，考虑使用序列号等机制处理网络消息的乱序问题，使得无效的、过期的网络消息不影响系统的正确性；
    - **数据错误**；不可靠的TCP，TCP 协议为应用层提供了可靠的、面向连接的传输服务，但在分布式系统的协议设计中不能认为所有网络通信都基于TCP 协议则通信就是可靠的。
      - **TCP协议只能保证同一个TCP 链接内的网络消息不乱序，TCP 链接之间的网络消息顺序则无法保证。**
  - **分布式三态**：如果某个节点向另一个节点发起RPC(Remote procedure call)调用，即某个节点A 向另一个节点B 发送一个消息，节点B 根据收到的消息内容完成某些操作，并将操作的结果通过另一个消息返回给节点A，那么这个RPC 执行的结果有三种状态：“成功”、“失败”、“超时（未知）”，称之为分布式系统的三态。
  - **存储数据丢失**:对于有状态节点来说，数据丢失意味着状态丢失，通常只能从其他节点读取、恢复存储的状态。
  - **异常处理原则**：被大量工程实践所检验过的异常处理黄金原则是：任何在设计阶段考虑到的异常情况一定会在系统实际运行中发生，但在系统实际运行遇到的异常却很有可能在设计时未能考虑，所以，除非需求指标允许，在系统设计时不能放过任何异常情况。
- **副本**
  - **副本（replica/copy）指在分布式系统中为数据或服务提供的冗余**。对于数据副本指在不同的节点上持久化同一份数据，当出现某一个节点的存储的数据丢失时，可以从副本上读到数据。
  - **数据副本是分布式系统解决数据丢失异常的唯一手段**。另一类副本是服务副本，指数个节点提供某种相同的服务，这种服务一般并不依赖于节点的本地存储，其所需数据一般来自其他节点。
  - **副本协议是贯穿整个分布式系统的理论核心。**
- **衡量分布式系统的指标**
  - **性能**：系统的吞吐能力，指系统在某一时间可以处理的数据总量，通常可以用系统每秒处理的总的数据量来衡量；
    - 系统的响应延迟，指系统完成某一功能需要使用的时间；
    - 系统的并发能力，指系统可以同时完成某一功能的能力，通常也用QPS(query per second)来衡量。
    - **上述三个性能指标往往会相互制约，追求高吞吐的系统，往往很难做到低延迟；系统平均响应时间较长时，也很难提高QPS。**
  - **可用性**：系统的可用性(availability)指系统在面对各种异常时可以正确提供服务的能力。
    - 系统的可用性可以用系统停服务的时间与正常服务的时间的比例来衡量，也可以用某功能的失败次数与成功次数的比例来衡量。可用性是分布式的重要指标，衡量了系统的鲁棒性，是系统容错能力的体现。
  - **可扩展性**：系统的可扩展性(scalability)指分布式系统通过扩展集群机器规模提高系统性能（吞吐、延迟、并发）、存储容量、计算能力的特性。**好的分布式系统总在追求“线性扩展性”，也就是使得系统的某一指标可以随着集群中的机器数量线性增长。**
  - **一致性**：分布式系统为了提高可用性，总是不可避免的使用副本的机制，从而引发副本一致性的问题。越是强的一致的性模型，对于用户使用来说使用起来越简单。
-  **数据分布方式**
  - 所谓**分布式系统**顾名思义就是**利用多台计算机**协同解决单台计算机所不能解决的计算、存储等问题。
    - 单机系统与分布式系统的**最大的区别在于问题的规模，即计算、存储的数据量的区别。**





## CAP理论

- 一致性（Consistency）
  - 一致性说的是客户端的每次读操作，不管访问哪个节点，要么读到的都是同一份最新的数据，要么读取失败。
  - 你可以把一致性看作是分布式系统对访问本系统的客户端的一种承诺：不管你访问哪个节点，要么我给你返回的都是绝对一致的数据，要么你都读取失败。你可以看到，一致性强调的不是数据完整，而是各节点间的数据一致。
- 可用性（Availability）
  - 可用性说的是任何来自客户端的请求，不管访问哪个节点，都能得到响应数据，但不保证是同一份最新数据。你也可以把可用性看作是分布式系统对访问本系统的客户端的另外一种承诺：我尽力给你返回数据，不会不响应你，但是我不保证每个节点给你的数据都是最新的。这个指标强调的是服务可用，但不保证数据的一致
- 分区容错性（Partition Tolerance）
  - 当节点间出现任意数量的消息丢失或高延迟的时候，系统仍然可以继续提供服务。也就是说，分布式系统在告诉访问本系统的客户端：不管我的内部出现什么样的数据同步问题，我会一直运行，提供服务。这个指标，强调的是集群对分区故障的容错能力。
  - 当节点 1 和节点 2 通信出问题的时候，如果系统仍能提供服务，那么，2个节点是满足分区容错性的。
  - 因为分布式系统与单机系统不同，它涉及到多节点间的通讯和交互，节点间的分区故障是必然发生的，所以我要提醒你，在分布式系统中分区容错性是必须要考虑的。

## **CAP权衡**

通过CAP理论，我们知道无法同时满足一致性、可用性和分区容错性这三个特性，那要舍弃哪个呢？

> CA without P：如果不要求P（不允许分区），则C（强一致性）和A（可用性）是可以保证的。但其实分区不是你想不想的问题，而是始终会存在，因此CA的系统更多的是允许分区后各子系统依然保持CA。
> CP without A：如果不要求A（可用），相当于每个请求都需要在Server之间强一致，而P（分区）会导致同步时间无限延长，如此CP也是可以保证的。很多传统的数据库分布式事务都属于这种模式。
> AP wihtout C：要高可用并允许分区，则需放弃一致性。一旦分区发生，节点之间可能会失去联系，为了高可用，每个节点只能用本地数据提供服务，而这样会导致全局数据的不一致性。现在众多的NoSQL都属于此类。





## ACID理论

- ACID 理论是对事务特性的抽象和总结，方便我们实现事务。你可以理解成：如果实现了操作的 ACID 特性，那么就实现了事务。而大多数人觉得比较难，是因为分布式系统涉及多个节点间的操作。加锁、时间序列等机制，只能保证单个节点上操作的 ACID 特性，无法保证节点间操作的 ACID 特性。
- TCC
  - 不管是原始的二阶段提交协议，还是 XA 协议，都存在一些问题
    - 在提交请求阶段，需要预留资源，在资源预留期间，其他人不能操作（比如，XA 在第一阶段会将相关资源锁定）；
    - 在第一个阶段，每个参与者投票表决事务是放弃还是提交。一旦参与者投票要求提交事务，那么就不允许放弃事务。也就是说，在一个参与者投票要求提交事务之前，它必须保证能够执行提交协议中它自己那一部分，即使参与者出现故障或者中途被替换掉。这个特性，是我们需要在代码实现时保障的。
    - 崩溃后协调者会不断的重试，这时候这个事务使用到的相关资源都会被锁住，没办法使用，直到节点恢复。
    - 参与者在阶段二挂掉，重启之后是没办法知道别的节点是否commit还是rollback，事务处于悬挂状态，必须等待协调者的异步重试消息，来执行commit或rollback操作
  - 在TCC中，数据是最终一致。
    - 2PC用在集群间一致性数据同步，所有参与者完成的是同一件事，可以理解为它们在一个start transaction--commit里面，具有强一致性
    - TCC是对业务过程的拆分，一致性弱于2PC

- Paxos、Raft 等强一致性算法，也采用了二阶段提交操作，在“提交请求阶段”，只要大多数节点确认就可以，而具有 ACID 特性的事务，则要求全部节点确认可以。所以可以将具有 ACID 特性的操作，理解为最强的一致性。
- 事务系统缺乏节点故障容错能力，性能也是痛点。
  - 需要将提交相关信息保存到持久存储上，用于故障后恢复，超时，也要不断重试







## BASE理论

- BASE 理论是 CAP 理论中的 AP 的延伸，是对互联网大规模分布式系统的实践总结，强调可用性。
  - 几乎所有的互联网后台分布式系统都有 BASE 的支持，这个理论很重要，地位也很高。一旦掌握它，你就能掌握绝大部分场景的分布式系统的架构技巧，设计出适合业务场景特点的、高可用性的分布式系统。
  - 而它的核心就是基本可用（Basically Available）和最终一致性（Eventually consistent）。也有人会提到软状态（Soft state），在我看来，软状态描述的是实现服务可用性的时候系统数据的一种过渡状态，也就是说不同节点间，数据副本存在短暂的不一致。你只需要知道软状态是一种过渡状态就可以了，我们不多说。
- 实现基本可用的 4 板斧
  - 基本可用是说，当分布式系统在出现不可预知的故障时，允许损失部分功能的可用性，保障核心功能的可用性。
  - 在春运期间，深圳出发的火车票在 8 点开售，北京出发的火车票在 9 点开售。这就是我们常说的**流量削峰**。
  - 在春运期间，自己提交的购票请求，往往会在队列中排队等待处理，可能几分钟或十几分钟后，系统才开始处理，然后响应处理结果，这就是你熟悉的**延迟响应**。
  - **体验降级**， 比如用小图片来替代原始图片，通过降低图片的清晰度和大小，提升系统的处理能力。
  - 然后你还能想到**过载保护**， 比如把接收到的请求放在指定的队列中排队处理，如果请求等待时间超时了（假设是 100ms），这个时候直接拒绝超时请求；再比如队列满了之后，就清除队列中一定数量的排队请求，保护系统不过载，实现系统的基本可用。

- 最终的一致
  - 在数据一致性上，存在一个短暂的延迟
  - 一般来说，在实际工程实践中有这样几种方式：
    - 以最新写入的数据为准，比如 AP 模型的 KV 存储采用的就是这种方式；
    - 以第一次写入的数据为准，如果你不希望存储的数据被更改，可以以它为准。
  - 在这里，我想强调的是因为写时修复（在写入数据，检测数据的不一致时，进行修复）不需要做数据一致性对比，性能消耗比较低，对系统运行影响也不大，所以我推荐你在实现最终一致性时优先实现这种方式。而读时修复（在读取数据时，检测数据的不一致，进行修复）和异步修复（这个是最常用的方式，通过定时对账检测副本数据的一致性，并修复）因为需要做数据的一致性对比，性能消耗比较多，在开发实际系统时，你要尽量优化一致性对比的算法，降低性能消耗，避免对系统运行造成影响。
  - 在实现最终一致性的时候，我推荐同时实现自定义写一致性级别（All、Quorum、One、Any）， 让用户可以自主选择相应的一致性级别，比如可以通过设置一致性级别为 All，来实现强一致性。
- 从微服务的角度来考虑, 有这些方式能够尽可能地保证系统的基本可用:
  - 使用消息队列, 对偶然的高并发写操作进行削峰填谷;
    - MQ能很好的弥补ES写性能差、支持突发流量能力弱的痛点。
  2. 对进程间的服务调用做好熔断保护;
  3. 在系统能力无法支撑高并发访问时, 对非核心业务降级;
  4. 对关键服务做好限流.



## Paxos算法

- Paxos 算法包含 2 个部分
  - 一个是 Basic Paxos 算法，描述的是多节点之间如何就某个值（提案 Value）达成共识；
  - 另一个是 Multi-Paxos 思想，描述的是执行多个 Basic Paxos 实例，就一系列值达成共识。
  - 在我看来，Basic Paxos 是 Multi-Paxos 思想的核心，说白了，Multi-Paxos 就是多执行几次 Basic Paxos
- 你需要了解的三种角色
  - 提议者（Proposer）：提议一个值，用于投票表决。在绝大多数场景中，集群中收到客户端请求的节点，才是提议者
  - 接受者（Acceptor）：对每个提议的值进行投票，并存储接受的值，比如 A、B、C 三个节点。 一般来说，集群中的所有节点都在扮演接受者的角色，参与共识协商，并接受和存储数据
  - 学习者（Learner）：被告知投票的结果，接受达成共识的值，存储保存，不参与投票的过程。一般来说，学习者是数据备份节点，比如“Master-Slave”模型中的 Slave，被动地接受数据，容灾备份
- 如何达成共识
  - 为了方便演示，我使用[n, v]表示一个提案，其中 n 为提案编号，v 为提议值。
  - 准备（Prepare）阶段
    - 你要注意，在准备请求中是不需要指定提议的值的，只需要携带提案编号就可以了
    - 先来看第一个阶段，首先客户端 1、2 作为提议者，分别向所有接受者发送包含提案编号的准备请求
      - 接着，当节点 A、B 收到提案编号为 1 的准备请求，节点 C 收到提案编号为 5 的准备请求后，将进行下面这样的处理
      - 由于之前没有通过任何提案，所以节点 A、B 将返回一个 “尚无提案”的响应。也就是说节点 A 和 B 在告诉提议者，我之前没有通过任何提案呢，并承诺以后不再响应提案编号小于等于 1 的准备请求，不会通过编号小于 1 的提案。
      - 节点 C 也是如此，它将返回一个 “尚无提案”的响应，并承诺以后不再响应提案编号小于等于 5 的准备请求，不会通过编号小于 5 的提案
  - 接受（Accept）阶段
    - 第二个阶段也就是接受阶段，首先客户端 1、2 在收到大多数节点的准备响应之后，会分别发送接受请求
    - 当节点 A、B、C 收到接受请求[1, 3]的时候，由于提案的提案编号 1 小于三个节点承诺能通过的提案的最小提案编号 5，所以提案[1, 3]将被拒绝。
    - 当节点 A、B、C 收到接受请求[5, 7]的时候，由于提案的提案编号 5 不小于三个节点承诺能通过的提案的最小提案编号 5，所以就通过提案[5, 7]，也就是接受了值 7，三个节点就 X 值为 7 达成了共识。
  - 通过上面的演示过程，你可以看到，最终各节点就 X 的值达成了共识。那么在这里我还想强调一下，Basic Paxos 的容错能力，源自“大多数”的约定，你可以这么理解：当少于一半的节点出现故障的时候，共识协商仍然在正常工作。
  - 如果接受者之前有通过提案，那么接受者将承诺，会在准备请求的响应中，包含已经通过的最大编号的提案信息
- 注意到这么两点
  - 首先在Basic Paxos的准备阶段，是会发现之前通过的提案的值；
  - 另外，Basic Paxos是一个共识算法，就一个值达成共识后，共识的这个值，就不会再变了。
- 如果我们直接通过多次执行 Basic Paxos 实例，来实现一系列值的共识，就会存在这样几个问题：
  - 如果多个提议者同时提交提案，可能出现因为提案冲突，在准备阶段没有提议者接收到大多数准备响应，协商失败，需要重新协商。你想象一下，一个 5 节点的集群，如果 3个节点作为提议者同时提案，就可能发生因为没有提议者接收大多数响应（比如 1 个提议者接收到 1 个准备响应，另外 2 个提议者分别接收到 2 个准备响应）而准备失败，需要重新协商
  - 2 轮 RPC 通讯（准备阶段和接受阶段）往返消息多、耗性能、延迟大。你要知道，分布式系统的运行是建立在 RPC 通讯的基础之上的，因此，延迟一直是分布式系统的痛点，是需要我们在开发分布式系统时认真考虑和优化的。
- 我们可以通过引入领导者节点，也就是说，领导者节点作为唯一提议者，这样就不存在多个提议者同时提交提案的情况，也就不存在提案冲突的情况了
  - 我们可以采用“当领导者处于稳定状态时，省掉准备阶段，直接进入接受阶段”这个优化机制，优化 Basic Paxos 执行。也就是说，领导者节点上，序列中的命令是最新的，不再需要通过准备请求来发现之前被大多数节点通过的提案，领导者可以独立指定提案中的值。这时，领导者在提交命令时，可以省掉准备阶段，直接进入到接受阶段
- Chubby 的 Multi-Paxos 实现
  - 在 Chubby 中，主节点是通过执行 Basic Paxos 算法，进行投票选举产生的，并且在运行过程中，主节点会通过不断续租的方式来延长租期（Lease）。比如在实际场景中，几天内都是同一个节点作为主节点。如果主节点故障了，那么其他的节点又会投票选举出新的主节点，也就是说主节点是一直存在的，而且是唯一的。
  - Chubby 只能在主节点上执行读操作，这个设计有什么局限呢？
    - 只能在主节点进行读操作，效果相当于单机，对吞吐量和性能有所影响
    - 写也是在主节点进行，性能也有问题







## Raft算法

- 概述

  - Raft 算法属于 Multi-Paxos 算法，它是在兰伯特 Multi-Paxos 思想的基础上，做了一些简化和限制，比如增加了日志必须是连续的，只支持领导者、跟随者和候选人三种状态，在理解和算法实现上都相对容易许多。
  - 除此之外，Raft 算法是现在分布式系统开发首选的共识算法。
  - 掌握这个算法，可以得心应手地处理绝大部分场景的容错和一致性需求，比如分布式配置系统、分布式 NoSQL 存储等等，轻松突破系统的单机限制。
  - 如果要用一句话概括 Raft 算法：从本质上说，Raft 算法是通过一切以领导者为准的方式，实现一系列值的共识和各节点日志的一致。

- 选举领导者的过程

  - Raft 算法实现了随机超时时间的特性。也就是说，每个节点等待领导者节点心跳信息的超时时间间隔是随机的。假设节点 A 的等待超时时间最小（150ms），它会最先因为没有等到领导者的心跳信息，发生超时。
    - 这个时候，节点 A 就增加自己的任期编号，并推举自己为候选人，先给自己投上一张选票，然后向其他节点发送请求投票 RPC 消息，请它们选举自己为领导者。
    - 如果其他节点接收到候选人 A 的请求投票 RPC 消息，在编号为 1 的这届任期内，也还没有进行过投票，那么它将把选票投给节点 A，并增加自己的任期编号
    - 如果候选人在选举超时时间内赢得了大多数的选票，那么它就会成为本届任期内新的领导者。

- 节点间是如何通讯的

  - 在 Raft 算法中，服务器节点间的沟通联络采用的是远程过程调用（RPC），在领导者选举中，需要用到这样两类的 RPC：
    - 请求投票（RequestVote）RPC，是由候选人在选举期间发起，通知各节点进行投票；
    - 日志复制（AppendEntries）RPC，是由领导者发起，用来复制日志和提供心跳消息。

- 任期

  - 我想强调的是，与现实议会选举中的领导者的任期不同，Raft 算法中的任期不只是时间段，而且任期编号的大小，会影响领导者选举和请求的处理。
    - 在 Raft 算法中约定，如果一个候选人或者领导者，发现自己的任期编号比其他节点小，那么它会立即恢复成跟随者状态。比如分区错误恢复后，任期编号为 3 的领导者节点B，收到来自新领导者的，包含任期编号为 4 的心跳消息，那么节点 B 将立即恢复成跟随者状态
    - 还约定如果一个节点接收到一个包含较小的任期编号值的请求，那么它会直接拒绝这个请求。比如节点 C 的任期编号为 4，收到包含任期编号为 3 的请求投票 RPC 消息，那么它将拒绝这个消息

- 选举有哪些规则

  - 在一个任期内，领导者一直都会是领导者，直到它自身出现问题（比如宕机），或者因为网络延迟，其他节点发起一轮新的选举。
  - 其实在选举中，除了选举规则外，我们还需要避免一些会导致选举失败的情况，比如同一任期内，多个候选人同时发起选举，导致选票被瓜分，选举失败。那么在 Raft 算法中，如何避免这个问题呢？答案就是随机超时时间。
  - 随机超时时间
    - 其实，Raft 算法巧妙地使用随机选举超时时间的方法，把超时时间都分散开来，在大多数情况下只有一个服务器节点先发起选举，而不是同时发起选举，这样就能减少因选票瓜分导致选举失败的情况。
    - 在 Raft 算法中，随机超时时间是有 2 种含义的
      - 跟随者等待领导者心跳信息超时的时间间隔，是随机的；
      - 当没有候选人赢得过半票数，选举无效了，这时需要等待一个随机时间间隔，也就是说，等待选举超时的时间间隔，是随机的。

- 日志

  - 在 Raft 算法中，副本数据是以日志的形式存在的，领导者接收到来自客户端写请求后，处理写请求的过程就是一个复制和提交日志项的过程。

    - 日志项是一种数据格式，它主要包含用户指定的数据，也就是指令，还包含一些附加信息，比如索引值、任期编号。

  - 如何复制日志

    - 首先，领导者进入第一阶段，通过日志复制（AppendEntries）RPC 消息，将日志项复制到集群其他节点上。
    - 接着，如果领导者接收到大多数的“复制成功”响应后，它将日志项提交到它的状态机，并返回成功给客户端。如果领导者没有接收到大多数的“复制成功”响应，那么就返回错误给客户端
      - 领导者将日志项提交到它的状态机，怎么没通知跟随者提交日志项呢？
      - 这是 Raft 中的一个优化，领导者不直接发送消息通知其他节点提交指定日志项。因为领导者的日志复制 RPC 消息或心跳消息，包含了当前最大的，将会被提交的日志项索引值。所以通过日志复制 RPC 消息或心跳消息，跟随者就可以知道领导者的日志提交位置信息。
      - 因此，当其他节点接受领导者的心跳消息，或者新的日志复制 RPC 消息后，如果跟随者发现领导者已经提交了某条日志项，而它还没提交，就会将这条日志项提交到它的状态机。而这个优化，降低了处理客户端请求的延迟，将二阶段提交优化为了一段提交，降低了一半的消息延迟。

  - 如何实现日志的一致

    - 具体有 2 个步骤

      - 首先，领导者通过日志复制 RPC 的一致性检查，找到跟随者节点上，与自己相同日志项的最大索引值。也就是说，这个索引值之前的日志，领导者和跟随者是一致的，之后的日志是不一致的了。
      - 然后，领导者强制跟随者更新覆盖的不一致日志项，实现日志的一致。

    - > PrevLogEntry：表示当前要复制的日志项，前面一条日志项的索引值。
      > PrevLogTerm：表示当前要复制的日志项，前面一条日志项的任期编号
      >
      > 
      >
      > 领导者通过日志复制 RPC 消息，发送当前最新日志项到跟随者（为了演示方便，假设当前需要复制的日志项是最新的），这个消息的 PrevLogEntry 值为 7，PrevLogTerm 值为 4。
      > 如果跟随者在它的日志中，找不到与 PrevLogEntry 值为 7、PrevLogTerm 值为 4 的日志项，也就是说它的日志和领导者的不一致了，那么跟随者就会拒绝接收新的日志项，并返回失败信息给领导者。
      > 这时，领导者会递减要复制的日志项的索引值，并发送新的日志项到跟随者，这个消息的 PrevLogEntry 值为 6，PrevLogTerm 值为 3。

- 如何通过单节点变更解决成员变更的问题

  - 单节点变更，就是通过一次变更一个节点实现成员变更。如果需要变更多个节点，那你需要执行多次单节点变更。比如将 3 节点集群扩容为 5 节点集群，这时你需要执行 2 次单节点变更，先将 3 节点集群变更为 4 节点集群，然后再将 4 节点集群变更为 5 节点集群

- 有什么办法能突破 Raft 集群的写性能瓶颈

  - leader可以合并请求
  - leader提交日志和日志复制RPC两个步骤可以并行和批量处理
  - 每个节点启动多个raft实例，对请求进行hash或者range后，让每个raft实例负责部分请求







## 一致哈希算法

- 一致哈希算法也用了取模运算，但与哈希算法不同的是，哈希算法是对节点的数量进行取模运算，而一致哈希算法是对 2^32 进行取模运算。你可以想象下，一致哈希算法，将整个哈希值空间组织成一个虚拟的圆环，也就是哈希环
  - 首先，将 key 作为参数执行hash() 计算哈希值，并确定此 key 在环上的位置；
  - 然后，从这个位置沿着哈希环顺时针“行走”，遇到的第一节点就是 key 对应的节点。
- 在一致哈希中，如果节点太少，容易因为节点分布不均匀造成数据访问的冷热不均，也就是说大多数访问请求都会集中少量几个节点上
  - 其实，就是对每一个服务器节点计算多个哈希值，在每个计算结果位置上，都放置一个虚拟节点，并将虚拟节点映射到实际节点。比如，可以在主机名的后面增加编号，分别计算“Node-A-01”“Node-A-02”“Node-B-01”“Node-B-02”“Node-C-01”“Node-C-02”的哈希值，于是形成 6 个虚拟节点
  - 这时，如果有访问请求寻址到“Node-A-01”这个虚拟节点，将被重定位到节点 A。
- 一致哈希是一种特殊的哈希算法，在使用一致哈希算法后，节点增减变化时只影响到部分数据的路由寻址，也就是说我们只要迁移部分数据，就能实现集群的稳定了。
- 为什么一个节点可以算出多个hash值
  - 引入虚拟节点，比如，将包含主机名和虚拟节点编号的字符串，作为参数，来计算哈希值。







## **Gossip**协议

- 有一部分同学的业务在可用性上比较敏感，比如监控主机和业务运行的告警系统。这个时 候，相信你**希望自己的系统能在极端情况下(比如集群中只有一个节点在运行)也能运行。** 回忆了二阶段提交协议和 Raft 算法之后，你发现它们都需要全部节点或者大多数节点正常 运行，才能稳定运行，那么它们就不适合了。

  - Gossip 协议，顾名思义，就像流言蜚语一样，利用一种随机、带有传染性的方式，将信息 传播到整个网络中，并在一定时间内，使得系统内的所有节点数据一致

- **Gossip** **的三板斧**

  - Gossip 的三板斧分别是:直接邮寄(Direct Mail)、反熵(Anti-entropy)和谣言传播(Rumor mongering)。
    - **直接邮寄**:就是直接发送更新数据，当数据发送失败时，将数据缓存下来，然后重传。
      - 直接邮寄虽然实现起来比较容易，数据同步也很及时，但可能会因为缓存队列满了而丢数据。也就是说，**只采用直接邮寄是无法实现最终一致性的。**
    - 那如何实现最终一致性呢?答案就是反熵。本质上，反熵是一种通过异步修复实现最终一致性的方法。常见的最终一致性系统(比如 Cassandra)，都实现了反熵功能。
      - **反熵指的是集群中的节点，每隔段时间就随机选择某个其他节点，然后通过互相交换自己的所有数据来消除两者之间的差异，实现数据的最终一致性。**
      - **其实，在实现反熵的时候，主要有推、拉和推拉三种方式**
      - 因为反熵需要节点两两交换和比对自己所有的数据，执行反熵时通讯 成本会很高，所以我不建议你在实际场景中频繁执行反熵，并且可以通过**引入校验和 (Checksum)等机制，降低需要对比的数据量和通讯消息等。**
        - 直接对比内容签名
        - merkle tree可以用来减少比较差异负担的开销
        - 把数据生成一个hash值 每次比较这个hash值 我之前看过微服务中拉取注册中心的配置时,不是拉取所有的配置和本地比较 而是通过生成的hash值比较的
      - **需要你注意的是，因为反熵需要做一致性对比，很消耗系统性能，所以建议你将是否启用反熵功能、执行 一致性检测的时间间隔等，做成可配置的，能在不同场景中按需使用。**
      - 绝大部分时候，系统是稳定运行的，如果出现了节点故障这种异常情况，一般而言，咱们需要先处理节点故障，然后再执行反熵，实现副本数据的最终一致。
    - 虽然反熵很实用，但是执行反熵时，相关的节点都是已知的，而且节点数量不能太多，如果 是一个动态变化或节点数比较多的分布式环境(比如在 DevOps 环境中检测节点故障，并 动态维护集群节点状态)，这时反熵就不适用了。**那么当你面临这个情况要怎样实现最终一 致性呢?答案就是谣言传播**
      - 谣言传播，广泛地散播谣言，它指的是当一个节点有了新数据后，这个节点变成活跃状态，并周期性地联系其他节点向其发送新数据，直到所有的节点都存储了该新数据。
  - 直接邮寄和反熵修复是实现最终一致性的不同方式，直接邮寄属于“写时修复”，即在写入数据，检测数据的不一致时，进行修复；反熵修复属于“异步修复”，即定时对账检测副本数据的一致性，进行修复。实际工程中两者是配合使用的

- 如何使用 Anti-entropy 实现最终一致

  - 在自研 InfluxDB 中，一份数据副本是由多个分片组成的，也就是实现了数据分片

    - 反熵的目标是确保每个 DATA 节点拥有元信息指定的分片，而且不同节点上，同一分片组中的分片都没有差异。比如说，节点 A 要拥有分片 Shard1 和 Shard2，而且，节点 A 的 Shard1 和 Shard2，与节点 B、C 中的 Shard1 和 Shard2，是一样的。

  - 在 DATA 节点上，存在哪些数据缺失的情况呢？也就说，我们需要解决哪些问题呢？

    - 我们将数据缺失，分为这样 2 种情况。

      - 缺失分片：也就是说，在某个节点上整个分片都丢失了。

        - 第一种情况修复起来不复杂，我们只需要将分片数据，通过 RPC 通讯，从其他节点上拷贝过来就可以了。

      - 节点之间的分片不一致：也就是说，节点上分片都存在，但里面的数据不一样，有数据丢失的情况发生

        - 第二种情况修复起来要复杂一些。我们需要设计一个闭环的流程，按照一个顺序修复，执行完流程后，也就是实现了一致性了。具体是怎么设计的呢？
        - **它是按照一定顺序来修复节点的数据差异**，先随机选择一个节点，然后循环修复，每个节点生成自己节点有、下一个节点没有的差异数据，发送给下一个节点，进行修复
        - 为什么这么设计呢？因为我们希望能在一个确定的时间范围内实现数据副本的最终一致性，而不是基于随机性的概率，在一个不确定的时间范围内实现数据副本的最终一致性。

        





## **Quorum NWR算法**

- 要实现强一致性，数据更改后，要保证用户能立即查询到。

  - 过 Quorum NWR，你可以自定义一致性级别，通过临时调整写入或者查询的方式，当 W + R > N 时，就可以实现强一致性了。
  - 掌握 Quorum NWR，不仅是掌握一种常用的实现一致性的方法，更重要的是，后续用户可以根 据业务的特点，灵活地指定一致性级别。

- **Quorum NWR** **的三要素**

  - **N 表示副本数，又叫做复制因子(Replication Factor)**。也就是说，N 表示集群中同一份 数据有多少个副本。
    - 副本数可以不等于节点数，不同的数据可以有不同 的副本数。
    - 需要你注意的是，在实现 Quorum NWR 的时候，你需要实现自定义副本的功能。也就是 说，用户可以自定义指定数据的副本数，比如，用户可以指定 DATA-1 具有 2 个副本， DATA-2 具有 3 个副本
  - W，又称写一致性级别(Write Consistency Level)，表示成功完成 W 个副本更新，才完 成写操作
    - DATA-2 的写副本数为 2，也就说，对 DATA-2 执行写操作时，完成 了 2 个副本的更新(比如节点 A、C)，才完成写操作。
  - **R，又称读一致性级别(Read Consistency Level)**，表示读取一个数据对象时需要读 R 个副本。你可以这么理解，读取指定数据时，要读 R 副本，然后返回 R 个副本中最新的那 份数据
    - 这里需要你注意的是，无论客户端如何执行读操作，哪怕它访问的是写操作未强制更新副本 数据的节点，**但因为 W(2) + R(2) > N(3)**，也就是说，访问节点 B，执行 读操作时，因为要读 2 份数据副本，所以除了节点 B 上的 DATA-2，还会读取节点 A 或节 点 C 上的 DATA-2，就像上图的样子，而节点 A 和节点 C 的 DATA-2 数据副本是强制更新成功的。这个时候，返回给客户端肯定是最新的那份数 据。

- 除此之外，关于 NWR 需要你注意的是，N、W、R 值的不同组合，会产生不同的一致性效 果，具体来说，有这么两种效果:

  - 当 W + R > N 的时候，对于客户端来讲，整个系统能保证强一致性，一定能返回更新后 的那份数据。
  - 当 W + R < N 的时候，对于客户端来讲，整个系统只能保证最终一致性，可能会返回旧 数据。

- **如何实现** **Quorum NWR**

  - 在 InfluxDB 企业版中，可以在创建保留策略时，设置指定数据库(Database)对应的副本数
    - 需要你注意的，在 InfluxDB 企业版中，副本数不能超过节点数据。你可以这么理解，多副 本的意义在于冗余备份，如果副本数超过节点数，就意味着在一个节点上会存在多个副本， 那么这时冗余备份的意义就不大了。比如机器故障时，节点上的多个副本是同时被影响的
  - InfluxDB 企业版，支持“any、one、quorum、all”4 种写一致性级别，具体的含义是这 样的。
    - any:任何一个节点写入成功后，或者接收节点已将数据写入 Hinted-handoff 缓存(也 就是写其他节点失败后，本地节点上缓存写失败数据的队列)后，就会返回成功给客户 端。
    - one:任何一个节点写入成功后，立即返回成功给客户端，不包括成功写入到 Hinted- handoff 缓存。
    - quorum:当大多数节点写入成功后，就会返回成功给客户端。此选项仅在副本数大于 2 时才有意义，否则等效于 all。
    - all:仅在所有节点都写入成功后，返回成功。
  - 对时序数据库而言，读操作常会拉取大量数据，查询性能是挑战，是必须要 考虑优化的，因此，在 InfluxDB 企业版中，不支持读一致性级别，只支持写一致性级别。 另外，我们可以通过设置写一致性级别为 all，来实现强一致性。

- 当 W + R > N 时，可以实现强一致性。另外，如何设置 N、W、R 值，取决于我们想优 化哪方面的性能。比如，N 决定了副本的冗余备份能力;如果设置 W = N，读性能比较 好;如果设置 R = N，写性能比较好;如果设置 W = (N + 1) / 2、R = (N + 1) / 2，容 错能力比较好，能容忍少数节点(也就是 (N - 1) / 2)的故障。

- Quorum NWR 是非常实用的一个算法，能有效弥补 AP 型系统缺乏强 一致性的痛点，给业务提供了按需选择一致性级别的灵活度，建议你的开发实现 AP 型系统 时，也实现 Quorum NWR。






## 分布式 ID 的设计方案

- 首先，我们需要明确通常的分布式 ID 定义，基本的要求包括：

  - 全局唯一，区别于单点系统的唯一，全局是要求分布式系统内唯一。
  - 有序性，通常都需要保证生成的 ID 是有序递增的。例如，在数据库存储等场景中，有序ID 便于确定数据位置，往往更加高效。

- 目前业界的方案很多，典型方案包括：

  - > 整体长度通常是 64 （1 + 41 + 10+ 12 = 64）位，适合使用 Java 语言中的 long 类型来存储。
    > 头部是 1 位的正负标识位。
    > 紧跟着的高位部分包含 41 位时间戳，通常使用 System.currentTimeMillis()。
    > 后面是 10 位的 WorkerID，标准定义是 5 位数据中心 + 5 位机器 ID，组成了机器编号，以区分不同的集群节点。
    > 最后的 12 位就是单位毫秒内可生成的序列号数目的理论极限。

- Snowflake 是否受冬令时切换影响

  - 没有影响，你可以从 Snowflake 的具体算法实现寻找答案。我们知道 Snowflake 算法的 Java 实现，大都是依赖于 System.currentTimeMillis()，这个数值代表什么呢？从Javadoc 可以看出，它是返回当前时间和 1970 年 1 月 1 号 UTC 时间相差的毫秒数，这个数值与夏 / 冬令时并没有关系，所以并不受其影响。

- 我们到底需要一个什么样的分布式 ID

  - 除了唯一和有序，考虑到分布式系统的功能需要，通常还会额外希望分布式 ID 保证：
  - 有意义，或者说包含更多信息，例如时间、业务等信息。这一点和有序性要求存在一定关联，如果 ID 中包含时间，本身就能保证一定程度的有序，虽然并不能绝对保证。ID 中包含额外信息，在分布式数据存储等场合中，有助于进一步优化数据访问的效率。
  - 高可用性，这是分布式系统的必然要求。前面谈到的方案中，有的是真正意义上的分布式，有得还是传统主从的思路，这一点没有绝对的对错，取决于我们业务对扩展性、性能等方面的要求。
  - 紧凑性，ID 的大小可能受到实际应用的制约，例如数据库存储往往对长 ID 不友好，太长的 ID 会降低 MySQL 等数据库索引的性能；编程语言在处理时也可能受数据类型长度限制

- Snowflake 方案的好处是算法简单，依赖也非常少，生成的序列可预测，性能
  也非常好，比如 Twitter 的峰值超过 10 万 /s。

  - 但是，它也存在一定的不足，例如时钟偏斜问题（Clock Skew）。我们知道普通的计算机系统时钟并不能保证长久的一致性，可能发生时钟回拨等问题，这就会导致时间戳不准确，进而产生重复 ID

- Snowflake 这种基于时间的算法，从形式上天然地限制了 ID 的并发生成数量，如果在极端情况下，短时间需要更多 ID，有什么办法解决呢？

  - 因为snowflake的可预测性，可以提前生成好放到队列里，获取的时候直接获取。相当于做了一层缓存；理论上可以解决短时间大量获取id的需求；

- 69年的极限问题不难解决，timestamp减个常量就可以了，对于已生成的历史id，可以导表刷id，当然，这里涉及到个数据库设计原则，系统之间传递数据不应使用物理主键，这样刷id 就容易了





















