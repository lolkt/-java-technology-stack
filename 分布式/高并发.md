# 分布式

### 集群、分布式

- 集群
  - 集群，是指同一种组件的多个实例，形成的逻辑上的整体。
  - 单机处理到达瓶颈的时候，你就把单机复制几份，这样就构成了一个“集群”。集群中每台服务器就叫做这个集群的一个“节点”，所有节点构成了一个集群。每个节点都提供相同的服务，那么这样系统的处理能力就相当于提升了好几倍
  - 但问题是用户的请求究竟由哪个节点来处理呢
    - 负载均衡服务器
  - 当你的业务发展到一定程度的时候，你会发现一个问题——无论怎么增加节点，貌似整个集群性能的提升效果并不明显了。这时候，你就需要使用微服务结构了。
  - 分布式结构就是将一个完整的系统，按照业务功能，拆分成一个个独立的子系统，在分布式结构中，每个子系统就被称为“服务”。这些子系统能够独立运行在web容器中，它们之间通过RPC方式通信。
- 分布式
  - 分布式不一定就是不同的组件，同一个组件也可以，关键在于是否通过交换信息的方式进行协作。比如说Zookeeper的节点都是对等的，但它自己就构成一个分布式系统。
  - 也就是说，分布式是指通过网络连接的多个组件，通过交换信息协作而形成的系统。
  - 好处
    - 系统之间的耦合度大大降低，可以独立开发、独立部署、独立测试
    - 系统之间的耦合度降低，从而系统更易于扩展
    - 服务的复用性更高。比如，当我们将用户系统作为单独的服务后，该公司所有的产品都可以使用该系统作为用户系统，无需重复开发。
- 可以看出这两个概念并不完全冲突，分布式系统也可以是一个集群，例子就是前面说的zookeeper等，它的特征是服务之间会互相通信协作。
- 情况
  - 是分布式系统不是集群的情况，就是多个不同组件构成的系统
  - 是集群不是分布式系统的情况，比如多个经过负载均衡的HTTP服务器，它们之间不会互相通信，如果不带上负载均衡的部分的话，一般不叫做分布式系统。

### 数据库高并发解决方法

- 概述
  - 关键是如何解决慢和等，核心一个是短，一个是少，一个是分流,最后一个是集群/横向扩张/读写分离/建立主从。
    - 短是指路径（请求）要短
      - 页面静态化
      - 缓存
      - 储存过程
      - 批量读取
      - 延迟修改
      - 索引
    - 少是指查询的数据要少
      - 分表
      - 分离活跃数据
      - 分块
    - 分流
      - 集群
        - 并发请求分配到不同的服务器上
      - 分布式
        - 把单次请求的多项业务逻辑分配到多个服务器上
      - CDN
        - 例如将华南地区的用户请求分配到华南的服务器，华中地区的用户请求分配到华中的服务器
- 解决数据库高并发访问瓶颈问题
  - 缓存式的Web应用程序架构
    - 在Web层和db层之间加一层cache层
  - 业务拆分
    - 每一个模块都使用单独的数据库来进行存储，不同的业务访问不同的数据库
  - MySQL主从复制，读写分离
    - 主从复制技术（master-slave模式）来达到读写分离，以提高读写性能和读库的可扩展性
    - 主从复制
      - 数据复制的实际就是Slave从Master获取Binary log文件，然后在本地镜像的执行日志中记录的操作
    - 读写分离
      - 只在主服务器上写，只在从服务器上读
      - 让主数据库处理事务性查询，而从数据库处理select查询
      - 数据库复制被用于把事务性查询（增删改）导致的改变更新同步到集群中的从数据库
    - 实现主从分离可以使用MySQL中间件如：Atlas
  - 分表分库
    - 采用Master-Slave复制模式的MySQL架构，只能对数据库的读进行扩展，而对数据的写操作还是集中在Master上
    - 分表
      - 对于访问极为频繁且数据量巨大的单表来说，首先要做的是减少单表的记录条数
    - 分表能够解决单表数据量过大带来的查询效率下降的问题，但是却无法给数据库的并发处理能力带来质的提升
      - 分表的实质还是在一个数据库上进行的操作，很容易受数据库IO性能的限制
    - 分库
      - 当数据库master服务器无法承载写操作压力时，不管如何扩展Slave服务器都是没有意义的
    - 数据库分表可以解决单表海量数据的查询性能问题，分库可以解决单台数据库的并发访问压力问题
    - 经过业务拆分及分库分表，虽然查询性能和并发处理能力提高了。但是原本跨表的事务上升为分布式事务
    - 分库分表后需要进一步对系统进行扩容（路由策略变更）将变得非常不方便，需要重新进行数据迁移
    - 分库分表的策略
      - １、中间变量　＝ user_id%（库数量*每个库的表数量）; 　　

　　２、库序号　＝　取整（中间变量／每个库的表数量）; 　　

　　３、表序号　＝　中间变量％每个库的表数量;

### 全局唯一序列号（Snowflake 算法）

- 概述
  - 分布式架构下，唯一序列号生成是我们在设计一个系统，尤其是数据库使用分库分表的时候常常会遇见的问题
  - 当分成若干个sharding表后，如何能够快速拿到一个唯一序列号，是经常遇到的问题
- 需求
  - 全局唯一
    支持高并发
    能够体现一定属性
    高可靠，容错单点故障
    高性能
    - 通常分布式系统采用主从模式，一个主机连接多个处理节点，主节点负责分发任务，而子节点负责处理业务，当主节点发生故障时，会导致整个系统发故障，我们把这种故障叫做单点故障
- 业内方案
  - Snowflake 算法
    - 全局唯一ID生成服务
    - 41位的时间序列
    - 10位的机器标识
    - 12位的计数顺序号
    - snowflake的结构如下(每部分用-分开)： 
      0 - 0000000000 0000000000 0000000000 0000000000 0 - 00000 - 00000 - 000000000000
    - 一共加起来刚好64位，为一个Long型
    - 生成的ID整体上按照时间自增排序，并且整个分布式系统内不会产生ID碰撞
    - 缺点：需要独立的开发和部署
  - Redis生成ID
    - Redis是单线程的，所以也可以用生成全局唯一的ID。可以用Redis的原子操作INCR和INCRBY来实现
    - 可以使用Redis集群来获取更高的吞吐量
    - 可以初始化每台Redis的值分别是1,2,3,4,5，然后步长都是5
    - 比较适合使用Redis来生成每天从0开始的流水号。比如订单号=日期+当日自增长号
    - 需要编码和配置的工作量比较大，多环境运维很麻烦
  - Flicker的解决方案
    - MySQL本身支持auto_increment操作
    - Flicker在解决全局ID生成方案里就采用了MySQL自增长ID的机制（auto_increment + replace into + MyISAM）
  - 其他一些方案
    - 订单号尽可能要多些冗余的业务信息
      - 滴滴：时间+起点编号+车牌号
      - 淘宝订单：时间戳+用户ID
    - 携程方案
      - 以flicker方案为基础进行优化改进。具体实现是，单表递增，内存缓存号段的方式
        - replace to来更新记录来获得唯一id
        - replace into 首先尝试插入数据到表中， 如果发现表中已经有此行数据（根据主键或者唯一索引判断）则先删除此行数据，然后插入新的数据
        - 插入数据的表必须有主键或者是唯一索引！否则的话，replace into 会直接插入数据，这将导致表中出现重复的数据
        - 再用 SELECT id FROM sequenceid WHERE ip = “192.168.1.1”  把它拿回来
      - 但是追根溯源，在原理上，方案还是依靠数据库的特性，每次生成id都要请求db，开销很大
      - 把这个id作为一个号段，而并不是要发出去的序列号
      - 现在的问题就是要解决同一台服务器在高并发场景，让大家顺序拿号，别拿重复，也别漏拿
        - 保持这个号段对象隔离性的问题
          - 当第一次拿回号段id后，扩大1000倍，然后赋值给这个变量atomic，这就是这个号段的第一个号码。

atomic.set(n * 1000);

并且内存里保存一下最大id，也就是这个号段的最后一个号码

currentMaxId = (n + 1) * 1000;

一个号段就形成了。
					- 此时每次有请求来取号时候，判断一下有没有到最后一个号码，没有到，就拿个号，走人

```
					- Long uid = atomic.incrementAndGet();

				- 如果到达了最后一个号码，那么阻塞住其他请求线程，最早的那个线程去db取个号段，再更新一下号段的两个值，就可以了。

	- 美团

		- 用户通过Round-robin的方式调用Leaf Server的各个服务，所以某一个Client获取到的ID序列可能是：1，1001，2001，2，1002，2002……也可能是：1，2，1001，2001，2002，2003，3，4……当某个Leaf Server号段用完之后，下一次请求就会从DB中加载新的号段，这样保证了每次加载的号段是递增的。
		- DB压力
		- 采用了异步更新的策略，同时通过双Buffer的方式，保证无论何时DB出现问题，都能有一个Buffer的号段可以正常对外提供服务
		- 动态调整Step

			- 服务QPS为Q，号段长度为L，号段更新周期为T，那么Q * T = L
			- Leaf本质的需求是希望T是固定的。那么如果L可以和Q正相关的话，T就可以趋近一个定值了
			- Leaf每次更新号段的时候，根据上一次更新号段的周期T和号段长度step，来决定下一次的号段长度nextStep

				- - T < 15min，nextStep = step * 2
```

- 15min < T < 30min，nextStep = step
- T > 30min，nextStep = step / 2

### 高并发系统的设计

- 设置http连接池
  - 如果不采用连接池，每次连接发起Http请求的时候 都会重新建立TCP连接(经历3次握手)，用完就会关闭连接(4次挥手)，如果采用连接池则减少了这部分时间损耗
  - 采用连接池，连接的复用，可以提高并发访问量
  - 降低延迟，支持更大的并发
- 把一些静态资源先加载到浏览器缓存里面
- 可以对服务器端的数据进行压缩
- 反向代理服务器可以保护服务器的安全（服务器的负载均衡）
- NIO模型
  - 解决线程资源受限的方案，实际开发过程中，我们会开多个线程，每个线程都管理着一批连接
  - 处理器访问任何寄存器和 Cache 等封装以外的数据资源都可以当成 I/O 操作，包括内存，磁盘，显卡等外部设备。
- 线程池
  - 设置一个最大线程数量和最小线程数量
  - 阻塞队列的大小要有界
    - 阻塞队列
      - 线程把请求放到阻塞队列里面
  - 线程池的失败策略
- 数据库连接池
- 数据存储部分
  - 数据库的优化，包括合理的事务隔离级别、SQL语句优化、索引的优化
- 缓存
  - 使用缓存，尽量减少数据库 IO
  - 分布式数据库、分布式缓存
- 数据库考虑集群、分库分表。
- 一致性哈希算法实现分布式缓存数据库
- 若有重复数据，布隆过滤器去重



# 如何应对大流量、高并发思路

所谓高并发指的是：在同时或极短时间内，有大量的请求到达服务端，每个请求都需要服务端耗费资源进行处理，并做出相应的反馈。

**常用的高并发处理的思路与手段**

从服务端视角看高并发
服务端处理请求需要耗费服务端的资源，比如能同时开启的进程数、能同时运行的线程数、网络连接数、cpu、I/O、内存等等，由于服务端资源是有限的，那么服务端能同时处理的请求也是有限的。高并发问题的本质就是：资源的有限性

高并发带来的问题
服务端的处理和响应会越来越慢，甚至会丢弃部分请求不予处理，更严重的会导致服务端崩溃。



**高并发处理的基本思路**
1）从客户端看
尽量减少请求数量，比如：依靠客户端自身的缓存或处理能力

尽量减少对服务端资源的不必要耗费，比如：重复使用某些资源，如连接池客户端处理的基本原则就是：能不访问服务端就不要访问

2）从服务端看
增加资源供给，比如：更大的网络带宽，使用更高配置的服务器，使用高性能的Web服务器，使用高性能的数据库

请求分流，比如：使用集群,分布式的系统架构

应用优化，比如：使用更高效的编程语言,优化处理业务逻辑的算法,优化访问数据库的SQL

基本原则：分而治之，并提高单个请求的处理速度



**高并发处理的基本手段**
1）客户端发出请求层面，常见的手段有：
尽量利用浏览器的缓存功能，减少访问服务端，比如：js、css、图片等

可以考虑使用压缩传输的功能，减少网络流量，也会提高传输速度

考虑使用异步请求，分批获取数据

**2）前端接收客户端请求层面，常见的手段有：**
动静分离，部分静态资源可以直接从Nginx返回

按请求的不同，分发到不同的后端进行处理，比如：负载均衡、业务拆分访问等

前面再加上一层来做多个Nginx的负载均衡，比如：LVS、F5等

还可以在更前面使用CDN服务

还可以对动态内容进行缓存，尽量减少访问后端服务

**3）Web服务器层面，常见的手段有：**
使用最新的JVM，并进行配置优化

对Web服务器进行配置优化，比如：调整内存数量、线程数量等

提供多个能提供相同服务的Web服务器，以实现负载均衡

仔细规划Web服务器上部署的应用规模

对Web服务器进行集群

**4）Web应用层面，常见的手段有：**
动态内容静态化

Java开发优化

优化处理业务逻辑的算法

合理高效的利用缓存

优化访问数据库的Sql，可以考虑利用存储过程等数据库的能力

合理使用多线程，加快业务处理

部分业务可以考虑内存数据库，或者是进行纯内存处理

尽量避免远程调用、大量I/O等耗时的操作

合理规划事务等较为耗资源的操作

合理使用异步处理

对部分业务考虑采用预处理或者预计算的方式，减少实时计算量

内部系统间的业务尽量直接调用、直接处理，减少WebService、工作流等

**5）数据库层面，常见的手段有：**
合理选择数据库的引擎，比如Mysql的InnoDB与MyISAM引擎

进行配置优化

可以考虑使用存储过程来处理复杂的数据逻辑

数据库集群，进行读写分离

合理设计数据库的表结构、索引等

分库、分表，降低单库、单表的数据量

# 请求量突增（限流降级）

一般的业务服务系统大体上就是通过**网络远程对DB进行读写**。如果流量突然飙大，总有一个资源会遇到瓶颈。按照经验大概出问题地方是DB磁盘io、CPU、带宽、连接数、内存其中的一个或几个。不同的业务，不同的系统设计，出问题的地方会有所不同。如果流量增大数倍，势必某个资源会在瞬间被榨干，然后所有的服务都会“开小差”，引起用户的抱怨。而解决问题的关键，是在问题发生时，尽量减少出问题的资源被访问。

对于这个问题，我这里给出两个回答，一个是应付面试的，一个面向实际的。大家各取所需。

## 面试中怎么回答

面试官其实就想听到几个术语的解释而已——**缓存、服务降级、限流**。

**缓存**，就是用内存来顶替一部分DB的查询+数据的处理。这应该是所有业务开发人员的必修课。业务上大致可以把缓存分为三类：浏览器缓存（HTTP Cache-Control Header)，[CDN](https://cloud.tencent.com/product/cdn?from=10680)和服务器业务缓存。而业务缓存根据实现的结构可以分多个层级，可以用in-memory cache (如Guava Cache），或者是分布式共享Cache（如[Redis](https://cloud.tencent.com/product/crs?from=10680)）。在设计缓存一致性更新模式时，无非就是*Cache Aside*、*Read/Write Through*和*Write Behind*这三大种模式。有些超级NB的缓存系统自带Cluster加持（比如Ehcache即可单机用，也可以组集群）。限于本文主题，具体的缓存设计不赘述。

留意下这里说的缓存仅仅是利用了内存访问比磁盘访问快得多的特性（大概可以理解为2～3个数量级），并不会让用户感知到数据一致性哪里不对劲（与下面的降级不同）。

**服务降级**，是指通过降低服务质量的方法，达到节省资源的目的。简单来说就是**弃车保帅**。比如你的服务有ABC，平时消耗差不多的资源。突发事件时，A的请求量极大的增高了，B和C没有变化。那么可以比如减少或者暂停B和C的服务，省出资源给A用。

再比如，一个热点新闻的业务，有新闻内容，有评论，有点赞数等等。一旦新闻热点了，就可以把所有这些内容“静态化”，不必再查DB。这时虽然评论，点赞的数暂时就不准了，但是主要的服务——内容，还是让用户可以看到。这就足够了。

可以看到，降级的具体的方案，要结合业务和系统实现来综合设计，并没有定法。

降级很多时候也会用到缓存。只不过这时候使用缓存的方法就可能会以牺牲数据一致性为代价——内存里的数据和DB不一样，就不一样吧，业务上可接受，并且这段热点时间段过去了，能够恢复为一致就可以。

**限流**，即限制用户的请求流量。具体的做法有*计数器*、*滑动窗口*、*滴漏*、*服务token*、*请求队列化*等办法。这些方法的详细解释，在[这里](https://www.jianshu.com/p/d9504fc0af4d)都说得比较清楚，所以我就不重复了。只是值得注意的是，现在很多生产级别的服务都是多节点分布式架构。很多单机上容易做的算法和控制逻辑到了分布式下就会带来一些实现上的麻烦。这又涉及到了分布式一致性、CAP的权衡等等问题。

怎么样，这些足够你在10分钟内和面试官白话一番了吧。下面我们说说真的。

**总是预先准备**

当设计一个业务时，产品设计和研发团队应该找个时间，除了讨论产品本身怎么实现之外，还应该关心一下如下几点的实施：

**流量估算**。到底大概有多少人可能会用呢？对于大公司，都有长时间运营的经验，可以参照之前的产品/活动给出一个量化的估算结果。但是小公司往往就只能拍脑袋。但即便是拍脑袋也比没有强，可以作为设计容量的依据。这下你明白为啥有些公司面试时会出“你觉得本城市有多少个辆汽车”这样的题目了吧。

作为一个经验，可以把设计流量*3作为系统压力的下限——即，实现完了要压测，压测得到的结果要达到设计流量 * 3。当然如果是你的话，要 * 4， * 5都可以，关键是要给系统留些缓冲。一旦发生了什么，不至于挂的太惨。此时，一般会得到一个带缓存的业务服务系统。考虑到缓存高于后台服务2～3个数量级的性能优势，多撑几倍流量一般不成问题。

**降级方案**。降级总得是用户可以买账的方式才行，不能瞎降。能降级成什么样，显示成什么样子，都得预先设计好。UI上有的要配图，有的要出警告语提示。而作为后台服务器，需要有对应的实时开关，一旦设置，立刻进入降级方案。

但是，如果核心服务就是热点本身，就没得降级，那就尴尬了…… 比如下单就是下单，不能下一半，不能砍掉支付，不能随机性有的能买有的不能买。这时就得靠限流。

**限流方案**。上面提到了种种限流算法——*计数器*、*滑动窗口*、*滴漏*、*服务token*、*请求队列化*，等办法技术在更加传统的模版式页面的网站更容易做——整个界面是由一个GET请求在后台通过模版产生的。所以只要在这个请求处理过程中做限流控制即可。

但是在SPA或者移动端App时，一个界面可能是由数个或者数十个Ajax接口分批获得。每个请求都做限流可能会得到随机的半残的界面——有几个接口没限制，有几个被限制了。此时做限流还得考虑前后端架构设计。一般来讲，每个主要界面都应该有个主控接口来实现限流（比如产品详情接口）——即，一旦该接口说限流了，后续的前端代码就得配合按照预先的设计显示限流后的界面。同时会影响关键资源的接口在后端要再做一道限流——毕竟你不知道有没有人绕开前端直接压接口使坏不是。嗯，抢票就是这么来的。

**提前安排开发和演练排期**。如果一切安排妥当，就可以做作演习了。你可以找个没人用你服务的时间点（大半夜？）使用流量replay压一下你的真实生产环境，看看真的发生了流量增高的问题，系统是否足够健壮能够应对，之前设计的种种方案是不是可以达到设计的需要。根据**墨菲定律**，可能会发生的事情一定会发生，不经演练的系统上线到了出问题的时候100%会让你大开眼界。

**相关问题**

这里稍微说一下高流量问题带来的一些相关的问题。这里仅仅是简单列举，具体内容之后找时间细细说。

-  **雪崩效应**——如果用户看到“服务开小差”，他的第一反应一定是再刷一次；如果是微服务架构，服务与服务之间可能会有自动重试机制。而这些，会让已经半死的系统死的更透彻。对此类问题，一般使用**断路器**的方案，简单来说就是，如果一个服务已经证明快挂了，就别再调用了，直接fallback。等一会再试。nginx里的upstream控制有`max_fails`和`fail_timeout`处理这个问题。Hystrix也实现了该机制。但断路了不等于让用户看到404页面骂娘，一定要结合业务+产品设计来实现断路方案。
-  **无效的服务响应**——在高压下，可以简单将等待处理的服务看作是在排队，队首的请求被处理。但被最终“见”到处理逻辑的请求从队尾排到队首时可能已经过了比较长的时间，而客户端那边可能早就超时了。所以业务服务处理了也是白处理。这时如果队列系统做得好，比如要处理前先猜一次是不是处理完了会超时，如果是就忽略扔掉。可以减少这种问题的发生几率。这也算是一种服务降级。
-  **大量的TIME_WAIT**——如果业务服务器的压力造成服务端大量主动关闭连接，就会产生大量的TIME_WAIT状态的TCP链接。这些链接会在数分钟内像僵尸一样堆在那里，榨干所有的连接数。这种问题尤其以自研业务服务框架容易出现。
-  **一致性**——为了服务降级，可能会把用户请求放内存里缓一缓，再批量进DB。那么一旦系统出现故障，就意味着比如下单数据不一致，支付状态不一致等问题。有时，这些问题在业务上极大的影响用户的使用体验。当系统降级时，尽量保证，要不就告诉用户现在不能给你服务，要服务了结果就明确。对于交易这种业务，事前打脸还是比事后扯皮要好一些。两害取其轻。
-  **系统可能会临时stop the world**——对于java这样的系统，会因为GC而暂时卡那么一下；对于mongoDB，可能因为要底层flush数据到磁盘，也会卡那么一下；平时写的什么正则表达式处理一类的逻辑，在高峰期也可能会卡那么一下…… 平时一般没事，但是赶上高峰时，这些问题一旦出现就有可能成为压垮骆驼的最后一根稻草。因此平时还是多多压测和演习，心里踏实。



# 双十一抢购性能瓶颈调优

首先，由于没有限流，超过预期的请求量导致了系统卡顿；其次，基于 Redis 实现的分布式锁分发抢购名额的功能抛出了大量异常；再次，就是我们误判了横向扩容服务可以起到的作用，其实第一波抢购的性能瓶颈是在数据库，横向扩容服务反而又增加了数据库的压力，起到了反作用；最后，就是在服务挂掉的情况下，丢失了异步处理的业务请求。

## 抢购业务流程

在进行具体的性能问题讨论之前，我们不妨先来了解下一个常规的抢购业务流程，这样方便我们更好地理解一个抢购系统的性能瓶颈以及调优过程。

- 用户登录后会进入到商品详情页面，此时商品购买处于倒计时状态，购买按钮处于置灰状态。
- 当购买倒计时间结束后，用户点击购买商品，此时用户需要排队等待获取购买资格，如果没有获取到购买资格，抢购活动结束，反之，则进入提交页面。
- 用户完善订单信息，点击提交订单，此时校验库存，并创建订单，进入锁定库存状态，之后，用户支付订单款。
- 当用户支付成功后，第三方支付平台将产生支付回调，系统通过回调更新订单状态，并扣除数据库的实际库存，通知用户购买成功。

## 抢购系统中的性能瓶颈

熟悉了一个常规的抢购业务流程之后，我们再来看看抢购中都有哪些业务会出现性能瓶颈。

1. **商品详情页面**

如果你有过抢购商品的经验，相信你遇到过这样一种情况，在抢购马上到来的时候，商品详情页面几乎是无法打开的。

这是因为大部分用户在抢购开始之前，会一直疯狂刷新抢购商品页面，尤其是倒计时一分钟内，查看商品详情页面的请求量会猛增。此时如果商品详情页面没有做好，就很容易成为整个抢购系统中的第一个性能瓶颈。

类似这种问题，我们通常的做法是提前将整个抢购商品页面生成为一个静态页面，并 push 到 CDN 节点，并且在浏览器端缓存该页面的静态资源文件，通过 CDN 和浏览器本地缓存这两种缓存静态页面的方式来实现商品详情页面的优化。

2. **抢购倒计时**

在商品详情页面中，存在一个抢购倒计时，这个倒计时是服务端时间的，初始化时间需要从服务端获取，并且在用户点击购买时，还需要服务端判断抢购时间是否已经到了。

如果商品详情每次刷新都去后端请求最新的时间，这无疑将会把整个后端服务拖垮。我们可以改成初始化时间从客户端获取，每隔一段时间主动去服务端刷新同步一次倒计时，这个时间段是随机时间，避免集中请求服务端。这种方式可以避免用户主动刷新服务端的同步时间接口。

**3. 获取购买资格**

可能你会好奇，在抢购中我们已经通过库存数量限制用户了，那为什么会出现一个获取购买资格的环节呢？

我们知道，进入订单详情页面后，需要填写相关的订单信息，例如收货地址、联系方式等，在这样一个过程中，很多用户可能还会犹豫，甚至放弃购买。如果把这个环节设定为一定能购买成功，那我们就只能让同等库存的用户进来，一旦用户放弃购买，这些商品可能无法再次被其他用户抢购，会大大降低商品的抢购销量。

增加购买资格的环节，选择让超过库存的用户量进来提交订单页面，这样就可以保证有足够提交订单的用户量，确保抢购活动中商品的销量最大化。

获取购买资格这步的并发量会非常大，还是基于分布式的，通常我们可以通过 Redis 分布式锁来控制购买资格的发放。

**4. 提交订单**

由于抢购入口的请求量会非常大，可能会占用大量带宽，为了不影响提交订单的请求，我建议将提交订单的子域名与抢购子域名区分开，分别绑定不同网络的服务器。

用户点击提交订单，需要先校验库存，库存足够时，用户先扣除缓存中的库存，再生成订单。如果校验库存和扣除库存都是基于数据库实现的，那么每次都去操作数据库，瞬时的并发量就会非常大，对数据库来说会存在一定的压力，从而会产生性能瓶颈。与获取购买资格一样，我们同样可以通过分布式锁来优化扣除消耗库存的设计。

由于我们已经缓存了库存，所以在提交订单时，库存的查询和冻结并不会给数据库带来性能瓶颈。但在这之后，还有一个订单的幂等校验，为了提高系统性能，我们同样可以使用分布式锁来优化。

而保存订单信息一般都是基于数据库表来实现的，在单表单库的情况下，碰到大量请求，特别是在瞬时高并发的情况下，磁盘 I/O、数据库请求连接数以及带宽等资源都可能会出现性能瓶颈。此时我们可以考虑对订单表进行分库分表，通常我们可以基于 userid 字段来进行 hash 取模，实现分库分表，从而提高系统的并发能力。

**5. 支付回调业务操作**

在用户支付订单完成之后，一般会有第三方支付平台回调我们的接口，更新订单状态。

除此之外，还可能存在扣减数据库库存的需求。如果我们的库存是基于缓存来实现查询和扣减，那提交订单时的扣除库存就只是扣除缓存中的库存，为了减少数据库的并发量，我们会在用户付款之后，在支付回调的时候去选择扣除数据库中的库存。

此外，还有订单购买成功的短信通知服务，一些商城还提供了累计积分的服务。

在支付回调之后，我们可以通过异步提交的方式，实现订单更新之外的其它业务处理，例如库存扣减、积分累计以及短信通知等。通常我们可以基于 MQ 实现业务的异步提交。

## 性能瓶颈调优

了解了各个业务流程中可能存在的性能瓶颈，我们再来讨论下商城基于常规优化设计之后，还可能出现的一些性能问题，我们又该如何做进一步调优。

**1. 限流实现优化**

限流是我们常用的兜底策略，无论是倒计时请求接口，还是抢购入口，系统都应该对它们设置最大并发访问数量，防止超出预期的请求集中进入系统，导致系统异常。

通常我们是在网关层实现高并发请求接口的限流，如果我们使用了 Nginx 做反向代理的话，就可以在 Nginx 配置限流算法。Nginx 是基于漏桶算法实现的限流，这样做的好处是能够保证请求的实时处理速度。

Nginx 中包含了两个限流模块：[ngx_http_limit_conn_module](http://nginx.org/en/docs/http/ngx_http_limit_conn_module.html) 和 [ngx_http_limit_req_module](http://nginx.org/en/docs/http/ngx_http_limit_req_module.html)，前者是用于限制单个 IP 单位时间内的请求数量，后者是用来限制单位时间内所有 IP 的请求数量。以下分别是两个限流的配置：

```
limit_conn_zone $binary_remote_addr zone=addr:10m;
 
server {
    location / {
        limit_conn addr 1;
    }
http {
    limit_req_zone $binary_remote_addr zone=one:10m rate=1r/s;
    server {
        location / {
            limit_req zone=one burst=5 nodelay;
        }
} 
```

在网关层，我们还可以通过 lua 编写 OpenResty 来实现一套限流功能，也可以通过现成的 Kong 安装插件来实现。除了网关层的限流之外，我们还可以基于服务层实现接口的限流，通过 Zuul RateLimit 或 Guava RateLimiter 实现。

**2. 流量削峰**

瞬间有大量请求进入到系统后台服务之后，首先是要通过 Redis 分布式锁获取购买资格，这个时候我们看到了大量的“JedisConnectionException Could not get connection from pool”异常。

这个异常是一个 Redis 连接异常，由于我们当时的 Redis 集群是基于哨兵模式部署的，哨兵模式部署的 Redis 也是一种主从模式，我们在写 Redis 的时候都是基于主库来实现的，在高并发操作一个 Redis 实例就很容易出现性能瓶颈。

你可能会想到使用集群分片的方式来实现，但对于分布式锁来说，集群分片的实现只会增加性能消耗，这是因为我们需要基于 Redission 的红锁算法实现，需要对集群的每个实例进行加锁。

后来我们使用 Redission 插件替换 Jedis 插件，由于 Jedis 的读写 I/O 操作还是阻塞式的，方法调用都是基于同步实现，而 Redission 底层是基于 Netty 框架实现的，读写 I/O 是非阻塞 I/O 操作，且方法调用是基于异步实现。

但在瞬时并发非常大的情况下，依然会出现类似问题，此时，我们可以考虑在分布式锁前面新增一个等待队列，减缓抢购出现的集中式请求，相当于一个流量削峰。当请求的 key 值放入到队列中，请求线程进入阻塞状态，当线程从队列中获取到请求线程的 key 值时，就会唤醒请求线程获取购买资格。

**3. 数据丢失问题**

无论是服务宕机，还是异步发送给 MQ，都存在请求数据丢失的可能。例如，当第三方支付回调系统时，写入订单成功了，此时通过异步来扣减库存和累计积分，如果应用服务刚好挂掉了，MQ 还没有存储到该消息，那即使我们重启服务，这条请求数据也将无法还原。

重试机制是还原丢失消息的一种解决方案。在以上的回调案例中，我们可以在写入订单时，同时在数据库写入一条异步消息状态，之后再返回第三方支付操作成功结果。在异步业务处理请求成功之后，更新该数据库表中的异步消息状态。

假设我们重启服务，那么系统就会在重启时去数据库中查询是否有未更新的异步消息，如果有，则重新生成 MQ 业务处理消息，供各个业务方消费处理丢失的请求数据。

**总结**

减少抢购中操作数据库的次数，缩短抢购流程，是抢购系统设计和优化的核心点。

抢购系统的性能瓶颈主要是在数据库，即使我们对服务进行了横向扩容，当流量瞬间进来，数据库依然无法同时响应处理这么多的请求操作。我们可以对抢购业务表进行分库分表，通过提高数据库的处理能力，来提升系统的并发处理能力。

除此之外，我们还可以分散瞬时的高并发请求，流量削峰是最常用的方式，用一个队列，让请求排队等待，然后有序且有限地进入到后端服务，最终进行数据库操作。当我们的队列满了之后，可以将溢出的请求放弃，这就是限流了。通过限流和削峰，可以有效地保证系统不宕机，确保系统的稳定性。

# 1秒1000并发 高并发需要什么样的服务器

- 我这边一分钟产生40万条数据，大概是400MB，期间要有其它程序处理这些数据。最初采用了Redis和MySQL，因为有读有写，发现写库根本来不及。最后采用的方式是：先缓存数据在内存，将每10万条数据进行序列化，写文件（7200转的硬盘，每秒写100MB），另外一程序解析文件，处理数据（处理完数据没那么多了），之后存库。

  - 所有数据先存队列里（比如beanstalkd），然后异步写入数据库
  - 网络环境是局域网

- 不需要有特殊服务器，一般云上主流主机都可以，主要还是软件架构要符合业务场景。

  - 阿里云10台2核4g

- 宽带肯定是要万兆的，硬件这块其实还好，现在可以用很廉价的pc来做分布式的架构，至于内存和硬盘的大小主要是根据数据量的大小和存储多少来决定的。

- 先普及一下基础知识：

  一、硬件条件。确认服务器硬件是否足够支持当前的流量，一台普通的P4服务器一般最多能支持每天10万独立IP，如果访问量比这个还要大， 那么必须首先配置一台更高性能的专用服务器才能解决问题 ，另外就是增加服务器数量，否则怎么优化都不可能彻底解决性能问题。

  

  二、数据库。优化数据库访问前台实现完全的静态化当然最好，可以完全不用访问数据库，不过对于频繁更新的网站， 静态化往往不能满足某些功能。缓存技术就是另一个解决方案，就是将动态数据存储到缓存文件中，动态网页直接调用 这些文件，而不必再访问数据库，WordPress和Z-Blog都大量使用这种缓存技术。如果确实无法避免对数据库的访问，那么可以尝试优化数据库的查询SQL，避免使用 Select * from这样的语句，每次查询只返回自己需要的结果，避免短时间内的大,尽量做到"所查即所得" ,遵循以小表为主,附表为辅,查询条件先索引,先小后大的原则,提高查询效率.量SQL查询。

  三、禁止盗链。外部网站的图片或者文件盗链往往会带来大量的负载压力，因此应该严格限制外部对于自身的图片或者文件盗链，好在目前可以简单地通过refer来控制盗链，Apache自 己就可以通过配置来禁止盗链，IIS也有一些第三方的ISAPI可以实现同样的功能。当然，伪造refer也可以通过代码来实现盗链，不过目前蓄意伪造refer盗链的还不多， 可以先不去考虑，或者使用非技术手段来解决，比如在图片上增加水印。

  

  四、控制大文件的下载。大文件的下载会占用很大的流量，并且对于非SCSI硬盘来说，大量文件下载会消耗 CPU，使得网站响应能力下降。因此，尽量不要提供超过2M的大文件下载，如果需要提供，建议将大文件放在另外一台服务器上。

  

  五、镜像分流。将文件放在不同的主机上，提供不同的镜像供用户下载。比如如果觉得RSS文件占用流量大，那么使用FeedBurner或者FeedSky等服务将RSS输出放在其他主机上，这样别人访问的流量压力就大多集中在FeedBurner的主机上，RSS就不占用太多资源了。

  

  六、做好流量监控。在网站上安装一个流量分析统计软件，可以即时知道哪些地方耗费了大量流量，哪些页面需要再进行优化，因此，解决流量问题还需要进行精确的统计分析才可以。推荐使用的流量分析统计软件是Google Analytics（Google分析）。

  

  接下来我们来说说服务器架构，前段时间我们请到了国内服务器顶级攻城狮，他把服务器那点事讲得非常通透简单。对于一个刚起步的创业公司，不需要考虑太多复杂的服务器架构，能把业务跑起来就行了。但是在早期业务逻辑设计时，懂一些稍微复杂的服务器架构的逻辑，后面可以少走很多弯路。

  

  下面这个图估计大家都明白，这就是最基础的服务器架构。傻瓜式的方法是把应用服务器、文件服务器、数据库服务器全部混合在一起，呵呵呵！但这并不是最科学的。

  ![img](https://pic3.zhimg.com/80/v2-8e496f310fc643f9c12934395fa7c445_1440w.png)

  

  

  当业务量持续增加到一定量以后，执行应用程序、读写文件、访问数据库应该有所区分，保证各自的需求都能得到满足，这时候你需要考虑把应用服务器、文件服务器、数据库服务器分离，这个时候的服务器架构应该是下面这样的，它是由三个独立的服务器组成，各司其职。

  ![img](https://pic1.zhimg.com/80/v2-3c7d638f12854bfff9defe7448a2648a_1440w.png)

  

  

  随着业务量持续增加，应用程序访问缓存数据会成为瓶颈，这个时候需要增加本地缓存，有的也需要分布式缓存。分布式缓存是指缓存部署在多个服务器组成的服务器集群中，以集群的方式提供缓存服务，其架构方式主要有两种，一种是以JBoss Cache为代表的需要同步更新的分布式缓存，一种是以Memchached为代表的互不通信的分布式缓存。如下图：

  ![img](https://pic1.zhimg.com/80/v2-2fe94a063184df2ba0ef279145e728c6_1440w.png)

  

  

  接下来，需要更多台应用服务器以应对复杂的业务逻辑，同时需要负载均衡调度服务器来调度和分配应用服务器的工作任务。

  ![img](https://pic1.zhimg.com/80/v2-073ccab95e5175268865591de2353804_1440w.png)

  

  

  再往后，需要考虑数据库服务器的承压能力，通常可以采用主从式数据库服务器架构，把读、写两部分分开，既可以提高数据访问的安全性，也能提高数据读写的效率。

  ![img](https://pic4.zhimg.com/80/v2-44bee1d539b9b6e8ce96a15a231c7c5c_1440w.png)

  

  

  随着业务量暴增，单一区域的服务器带宽将不能承载全国的业务需求，这时候需要增加反向代理和CDN服务器。CDN系统能够实时地根据网络流量和各节点的连接、负载状况以及到用户的距离和响应时间等综合信息将用户的请求重新导向离用户最近的服务节点上。其目的是使用户可就近取得所需内容，解决 Internet网络拥挤的状况，提高用户访问网站的响应速度。

  ![img](https://picb.zhimg.com/80/v2-cfbd61431a0b08b140184deb0c77d063_1440w.png)

  

  

  同样，服务器架构师应该分析文件服务器和数据库服务器的网络读写速度，进一步部署分布式文件和分布式数据库的架构。

  ![img](https://pic3.zhimg.com/80/v2-019478359e8a916b405b724000922e4f_1440w.png)

  

  

  对于有搜索和大量查询的网络业务，还需要增加独立的搜索引擎和NoSQL服务器。

  ![img](https://picb.zhimg.com/80/v2-967cc701131edeffd524857d84660d0a_1440w.png)

  

  

  对于更复杂的系统，还需要进一步拆分应用服务器，增加消息队列服务器。增加消息队列服务器有以下几点好处：

  1，由于消息队列服务器的速度远远高于数据库服务器，所以能够快递处理并返回数据；

  2，消息队列服务器具有更好的扩展性；

  3，在高并发的情况下，延迟写入数据库，可以有效降低数据库的压力。

  ![img](https://picb.zhimg.com/80/v2-1ebdedfc6cb79c4cbeab8203f5926a15_1440w.png)

  

  

  对于一些超大型综合互联网业务，应用服务器也需要分布式的架构，这个时候在不同业务的应用服务器之间做好消息协同会有较大的挑战。

  

  

  ![img](https://pic2.zhimg.com/80/v2-ea826176a298de845a07acb7318f35e9_1440w.png)

  

  读完后，是不是感觉很简单，**基本上就是围绕着应用服务器、文件服务器、数据库服务器，以及不断提升其性能需要增加的服务器**。好在如今腾讯云、阿里云、金山云都提供了完整的解决方案

# 设计一个高并发、高可用秒杀系统



## **秒杀系统的难点**

- 友好的用户体验

  - 用户不能接受破窗的体验，例如：系统超时、系统错误的提示，或者直接 404 页面

- 瞬时高并发流量的挑战

  - 木桶短板理论，整个系统的瓶颈往往都在 DB，如何设计出高并发、高可用系统？

## **如何设计**

![img](https://pic1.zhimg.com/v2-e3a128f561bb4739c156ed6e392cb426_b.jpg)

上图是一个典型的互联网业务，用户完成一个写操作，一般会通过接入层和逻辑层，这里的服务都是无状态，可以通过平行拓展去解决高并发的问题；到了 db 层，必须要落到介质中，可以是磁盘/ssd/内存，如果出现 key 的冲突，会有一些并发控制技术，例如 cas/加锁/串行排队等。

**直筒型**

直筒型业务，指的是用户请求 1:1 的洞穿到 db 层，如下图所示。在比较简单的业务中，才会采用这个模型。随着业务规模复杂度上来，一定会有 db 和逻辑层分离、逻辑层和接入层分离。

![img](https://pic2.zhimg.com/v2-15f4981679385991797c2c435160d2dd_b.jpg)

**漏斗型**

漏斗型业务，指的是，用户的请求，从客户端到 db 层，层层递减，递减的程度视业务而定。例如当 10w 人去抢 1 个物品时，db 层的请求在个位数量级，这就是比较理想的模型。如下图所示

![img](https://picb.zhimg.com/v2-243232ef478e347dd4d594af21a03d8a_b.jpg)

这个模型，是高并发的基础，翻译一下就是下面这些：

- 及早发现，及早拒绝
- Fast Fail
- 前端保护后端
  

**如何实现漏斗型系统**

漏斗型系统需要从产品策略/客户端/接入层/逻辑层/DB 层全方位立体的设计。

![img](https://pic4.zhimg.com/v2-e0a777b9cacabc2e32df83f4e7d6d829_b.jpg)

**产品策略**

- 轻重逻辑分离，以秒杀为例，将抢到和到账分开；

- - 抢到，是比较轻的操作，库存扣成功后，就可以成功了
  - 到账，是比较重的操作，需要涉及到到事务操作
    

- 用户分流，以整点秒杀活动为例，在 1 分钟内，陆续对用户放开入口，将所有用户请求打散在 60s 内，请求就可以降一个数量级
  
- 页面简化，在秒杀开始的时候，需要简化页面展示，该时刻只保留和秒杀相关的功能。例如，秒杀开始的时候，页面可以不展示推荐的商品。
  

**客户端**

- 重试策略非常关键，如果用户秒杀失败了，频繁重试，会加剧后端的雪崩。如何重试呢？根据后端返回码的约定，有两种方法：
  
- - 不允许重试错误，此时 ui 和文案都需要有一个提示。同时不允许重试
  - 可重试错误，需要策略重试，例如二进制退避法。同时文案和 ui 需要提示。
    

- ui 和文案，秒杀开始前后，用户的所有异常都需要有精心设计的 ui 和文案提示。例如：【当前活动太火爆，请稍后再重试】【你的货物堵在路上，请稍后查看】等
  
- 前端随机丢弃请求可以作为降级方案，当用户流量远远大于系统容量时，人工下发随机丢弃标记，用户本地客户端开始随机丢弃请求。
  

**接入层**

- 所有请求需要鉴权，校验合法身份
  
- - 如果是长链接的服务，鉴权粒度可以在 session 级别；如果是短链接业务，需要应对这种高并发流量，例如 cache 等

- 根据后端系统容量，需要一个全局的限流功能，通常有两种做法：
  
- - 设置好 N 后，动态获取机器部署情况 M，然后下发单机限流值 N/M。要求请求均匀访问，部署机器统一。
  - 维护全局 key，以时间戳建 key。有热 key 问题，可以通过增加更细粒度的 key 或者定时更新 key 的方法。
    

- 对于单用户/单 ip 需要频控，主要是防黑产和恶意用户。如果秒杀是有条件的，例如需要完成 xxx 任务，解锁资格，对于获得资格的步骤，可以进行安全扫描，识别出黑产和恶意用户。
  

**逻辑层**

- 逻辑层首先应该进入校验逻辑，例如参数的合法性，是否有资格，如果失败的用户，快速返回，避免请求洞穿到 db。
  
- 异步补单，对于已经扣除秒杀资格的用户，如果发货失败后，通常的两种做法是：
  
- - 事务回滚，回滚本次行为，提示用户重试。这个代价特别大，而且用户重试和前面的重试策略结合的话，用户体验也不大流畅。
  - 异步重做，记录本次用户的 log，提示用户【稍后查看，正在发货中】，后台在峰值过后，启动异步补单。需要服务支持幂等
    

- 对于发货的库存，需要处理热 key。通常的做法是，维护多个 key，每个用户固定去某个查询库存。对于大量人抢红包的场景，可以提前分配。
  

**存储层**

对于业务模型而言，对于 db 的要求需要保证几个原则：

- 可靠性
  
- - 主备：主备能互相切换，一般要求在同城跨机房
    
  - 异地容灾：当一地异常，数据能恢复，异地能选主
    
  - 数据需要持久化到磁盘，或者更冷的设备
    

- 一致性
  
- - 对于秒杀而言，需要严格的一致性，一般要求主备严格的一致。

## **实践——微视集卡瓜分系统**

微视集卡瓜分项目属于微视春节项目之一。用户的体验流程如下：

![img](https://pic2.zhimg.com/v2-9bde162bee150dc33d61df71ffb71724_b.jpg)

**架构图**

![img](https://pic2.zhimg.com/v2-60b7f118b41855263cd1e4d434285b4d_b.jpg)

- 客户端主要是微视主 app 和 h5 页面，主 app 是入口，h5 页面是集卡活动页面和瓜分页面。
  
- 逻辑部分为分：发卡来源、集卡模块、奖品模块，发卡来源主要是任务模块；集卡模块主要由活动模块和集卡模块组成。瓜分部分主要在活动控制层。
  
- 奖品模块主要是发钱和其他奖品。
  

**瓜分降级预案**

为了做好瓜分时刻的高并发，对整个系统需要保证两个重要的事情：

- 全链路梳理，包括调用链的合理性和时延设置
  
- 降级服务预案分析，提升系统的鲁棒性
  

如下图所示，是针对瓜分全链路调用分析如下图，需要特别说明的几点：

![img](https://picb.zhimg.com/v2-2357aac83e60101d1b143f634725b5b2_b.jpg)

- 时延很重要，需要全链路分析。不但可以提高吞吐量，而且可以快速暴露系统的瓶颈。
  
- 峰值时刻，补单逻辑需要关闭，避免加剧雪崩。
  

我们的降级预案大概如下：

- 一级预案，瓜分时刻前后 5 分钟自动进入：
  
- - 入口处 1 分钟内陆续放开入口倒计时，未登录用户不弹入口
    
  - 主会场排队，以进入主会场 100wqps 为例，超过了进入排队，由接入层频控控制
    
  - 拉取资格接口排队，拉取资格接口 100wqps，超过了进入排队，由接入层频控控制
    
  - 抢红包排队，抢红包 100wqps，超过了进入排队，由接入层频控控制
    
  - 红包到账排队，如果资格扣除成功，现金发放失败，进入排队，24 小时内到账。异步补单
    
  - 入口处调用后端非关键 rpc:ParticipateStatus，手动关闭
    
  - 异步补单逻辑关闭。
    

- 二级预案，后端随机丢请求，接入层频控失效或者下游服务过载，手动开启进入
  
- 三级预案，前端随机丢请求，后端服务过载或者宕机进入。手动开启
  

综上，整个瓜分时刻体验如下所示：

![img](https://pic4.zhimg.com/v2-4551c7d39e776fe0d494c4cc6005fa17_b.jpg)

回顾下漏斗模型，总结下整个实践：

![img](https://picb.zhimg.com/v2-db80d36c4a81173ec75db7fe88b25a75_b.jpg)





降级开关设计，过载感知，分流策略设计





# 架构设计的本质

## 思维分析

### 1. 系统总览

归结起来设计一个系统，或者朴素的说，做一件事情，我们需要解决以下问题：

![1.jpg](https://ucc.alicdn.com/pic/developer-ecology/ebbdc96a533e49dbaeb8a16252fd3910.jpg)

在解决以上提出的问题前，首先声明我们要实现的是一个系统，而不是随意混搭的一件物品，毕竟现在讨论的不是行为艺术。那么就需要先来了解系统的定义:

> 系统是由一组实体和实体之间关系构成的集合，其功能大于各个实体功能之和。

### 2. 系统演化

#### 1）业务描述

假设现在我们想登上火星，言下之意是需要借助一套设备要把人类送到火星上

根据系统总览提到的问题，先一一作答。

![2.jpg](https://ucc.alicdn.com/pic/developer-ecology/046062bc1a27443caf42034b6afdc2fb.jpg)

业务流程画图元素：火箭，机舱，地球，火星，来回，基础功能（安全，舒适，成本）

![3.png](https://ucc.alicdn.com/pic/developer-ecology/f60f08f0b6f14c0596a3a7488b1086a5.png)

#### 2）概念抽象

怎样把这件事的主线说明清楚？那么就需要抓重点的来说，这个时候就需要一个叫做“概念”的工具。

> 概念是抽象的、普遍的想法，是充当指明实体、事件或关系的范畴或类的实体。

**简单来说，概念就是用简单的一个词汇，就可以让在坐的大家都能准确无误的理解这个词汇所表达的含义。**

在业务建模后就是概念建模，作为架构设计的输入，**这个阶段就需要对核心业务的充分理解，同时在基础性和通用性方面的功能也需要同时考虑**

经过一系列的概念抽象和组合，最终输出登陆火星工程的架构图，这里只是用于说明登陆火星项目同样遵循这**业务-概念-架构-设计**的流程，不要在意架构图本身合不合理。

![6.png](https://ucc.alicdn.com/pic/developer-ecology/4a427eec0dd5448bb83ea0239f60b6ce.png)

#### 3）系统落地

当然这还远远不够，系统之所以复杂，就是我们对系统总有无数更多的要求，更多的功能，更好的性能，那么接下来就是对各个模块进行分析，细化，设计和实施。

### 3. 架构思维

#### 1）架构目标

打开 idea 才开始考虑业务，写代码才开始思考领域模型，这是非常不好的习惯，好像如果没有在 coding 状态下是无法进行建模思考，这个很难，需要持久的训练才能达成设计阶段进行思考。

**架构师眼里第一件事不是多流行的技术，多高性能的框架，或者多完善的业务模型，而应该聚焦在利益之上**。

#### 2）架构过程

在面对复杂业务的场景下，我们需要做出如下的思考：

- 确定系统**实体对象和预期功能**
- 抽象系统**实体之间的关系，功能与实体的关系**
- 划定系统的**边界和外部环境的关系（边界：DDD，完美拆分不可能，业务影响最低就可以）**
- 预测系统**带来的效果**

在架构过程中，很重要的一项任务就是**识别系统的实体关系和功能关系**，进而对系统效果进行预测

#### 3）系统思维

系统思维不等于系统化思考，与系统思维并列的可以是批判思维，分析思维，**创新思维**，而我们追求的是元认知，也即是认识到自己处于何种思维模式。系统思维目标：

- 理解系统**是什么**
- 预测系统的**走向**
- **为决策提供知识支持**

## 系统分析

本节将提供一套基础的方法，来对现有系统进行分析，得出一些系统架构相关的推论。按照惯例需要先搞清楚系统分析的概念：

> 系统分析，旨在研究**特定系统结构中各部分的相互作用，系统的对外接口与界面，以及该系统整体的行为、功能和局限**，从而为系统未来的变迁与有关决策提供参考和依据。**系统分析的经常目标之一，在于改善决策过程及系统性能，以期达到系统的整体最优。**

对系统的分解也是讲究方法的，可以参考以下总结的一些方向：

- 体系归纳
- 层级分解
- 逻辑关系
- 自顶向下
- 自底向上
- 由外向内
- 由内而外

### 1. 实体分析

在分析实体之前，我们可以带着下面的问题进行分析：

- 系统是什么？
- 构成系统的元素有哪些？
- 系统元素之间的结构是什么？
- 系统的边界在哪里？
- 系统的使用场景是什么？

实体之间的关系就是结构，分析结构时需要对实体进行分解，实体可以建模为对象及其之间的结构，进一步可以分解为小的实体，又可以聚合起来称为系统本身，对实体之间的各种结构分析则可以得出系统架构，即是把功能元素组合成物理块时所用的编排方式。

#### 1）分析实体

- 对实体的载体进行**抽象聚类**，形成对象，**体现出边界**
- 用**适当的层次**来分解架构的实体

### 2. 功能分析

功能 = 主体 + 操作 + 操作对象

总结起来，系统分析就是建立一套方法论，去分析复杂的系统，令系统不再那么难懂。

## 系统设计

**TOGAF**：框架开放组体系结构框架（The Open Group Architecture Framework，缩写：TOGAF）是一个企业架构框架，它提供了一种设计，规划，实施和管理企业信息技术架构的方法。TOGAF 是一种高层设计方法。它通常被建模为四个级别：业务，应用程序，数据，和技术。

在 TOGAF 中，任何一种企业能力的建设都需要对如下四种领域进行设计，包括针对这一可持续性架构实践建设：

- 业务架构：突出了**架构治理、架构流程、架构组织结构、架构信息需求以及架构产品等方面；**
- 数据架构：定义了组织中架构连续体和架构**资源库**的结构；
- 应用架构：描述了用于支持此可持续架构实践的**功能和服务**；
- 技术架构：描述了架构实践中用于支持各架构应用和企业连续体的**基础设施需求和部署方式。**

### 1. 设计工具

列出 UML 的核心语言元素，视图，模型和过程。

### 2. 需求分析

#### 1）架构角色

互联网时代架构师有如下职责：

- 了解**问题领域**，消除歧义
- 树立**业务目标**，抽象业务用例
- 完成**涉众分析**，发现系统主要受益者
- **划清系统边界**，确立对外交互方式
- **划分优先级别，聚焦系统核心诉求**
- **分析业务需求**，输出业务模型
- **抽象**业务概念，输出概念模型
- 推导**系统架构**，输出架构模型
- 负责**技术选型**，完成系统**落地**

#### 2）利益分析

#### 3）资源评估

资源评估不仅仅是项目经理的事，而且还是对团队资源的评估和编排，比如某项业务技术团队中研发和数据人员的配比，决定了数据平台投入的资源范围，这要求架构师在做设计分析的时候需要充分考虑到资源的利用效率，包括人力资源和机器等资源。

#### 4）需求规范

用户的诉求体现为需求的输出过程，不同层次分解需求得出了不同的复杂度，我们知道架构师的一项重要职责就是消除歧义，精准的把握需求来匹配用户的诉求。那么就需要一系列规范来保障需求采集的过程中不失真。

下面列出了需求采集过程的一些指导原则：

- 每个软件需求是否都有唯一的标识符？
- 每个软件需求都可以验证吗？（如果可能，是否可正规化，量化）
- 是否对每个软件要求进行了优先排序？
- 所有不稳定的软件要求是否都已标明？
- 软件需求是否完整？（涵盖了所有用户要求，考虑了所有相关的输入情况）
- 软件要求是否一致？
- 是否明确指出了软件需求之间的重叠交叉？
- 是否明确规定了初始系统状态？
- 软件需求是否表达了逻辑模型， 而不是实现形式？
- 软件需求是否以结构化的方式表示为抽象层次？
- 是否足够清楚，逻辑模型的结构
- 软件要求是否已正式形式化？
- 是否已证明软件需求的关键属性？
- 所有形式化的图表材料是否都随附了充足解释性文字？
- 是否针对项目团队缺乏经验的领域描述了探索性原型？

#### 5）需求描述▶️▶️▶️▶️▶️

在进行需求描述时，我们可以从多个角度来审视需求是否合理地表达出来：

- 满足需求，能带来什么价值，**符合什么利益诉求**？
- 需求无法满足时，会带来什么危害，有何**潜在风险**？
- **需求是否紧迫**，必须在什么时间段内完成？
- **多个需求直接是否产生耦合**，完成一个需求后是否带来了新的问题？
- 是否能多个**备选方案**来完成需求？

#### 6）需求采集

回到我们平台设计的案例上，经过用户访谈粗略的采集的需求：

• 单项业务壁垒是个困局，**本质上是功能缺陷，打通数据壁垒**
• 业务各个阶段的数据管理，**要对数据底层进行管理**
• 需要对由相同特性的**报表**实现快捷地生成

需求背后：

• 可以一次性看更多的数据
• 可以方便的切换数据
• 可以更快的看到**数据**

**so，这个真的是客户想要的吗？整体上，用户想看什么数据？同时我也在思考下面的问题**：

- 深入分析用户需求
- 搞清楚我们的客户是谁？
- 定义好问题，搞清楚问题的本质，分析问题矛盾之处，我们要解决什么问题？
- 我们要在多大范围内去解决问题，要解决跨度多长时间可预计的问题？
- 我们的边界在哪里？
- 我们的使命是什么？有了使命后再谈我们自己，愿景是什么？

### 3. 模型建立

在系统设计这个阶段，我们已经介绍了如何运用工具，还有用户需求的管理，**接下来就是要把需求“消化”成我们需要的架构**。但是架构不是平白无故就产生的，前文我们用登陆火星的案例也大概描述了系统的建设过程，那么在推导出架构之前，把用户不那么清晰的诉求转化成严谨的业务概念模型就很有必要了。

**建立模型不是最终目的，而是把复杂的业务诉求构建成简单的业务概念，在软件开发团队沟通过程中能形成共识，消除歧义，而且信息传递不失真，为输出架构奠定基础**。

#### 1）建模方法

建模有很多种方法，对于同样的问题域使用不同的建模手段，得到的模型可能也不尽相同。建模是一种对现实事件的抽象，不同的心智会产生不同的模型，比如宗教，不同宗教就是对人生观世界观产生不同的模型，我们先介绍常用的建模方法：

- 领域驱动（DDD）
- 用例驱动（UDD）
- 四色建模
- CRC建模
- CQRS建模

#### 2）用例驱动

用例驱动是一种由外而内，先招式后内功的思想。我们先从涉众对系统的期望开始，定义出系统如何满足他们的愿望。这一过程是感性的、外在的、符合当前需求的。

用例驱动的结果是我们的软件是**以实现一个个场景为目的的，认为当一个系统的行为满足了所有涉众的期望之后，即满足了涉众使用系统的场景之后，该系统就是一个成功的系统**。

###### 【建立用例】

用例定义：工具—>过程—>操作数 （主 谓 宾）

- 参与者：某些具有行为的事物，可以是人，计算机系统或组织；
- 场景：参与者和系统之间的一系列特定的活动和交互；
- 用例：一组相关的成功和失败的场景集合，用来描述参与者如何使用系统来实现目标。

###### 【用例规范】

用例其实就是对一件独立事情的描述，这非常符合我们人类语言的表达过程，我们日常沟通很大部分是陈述一个观点，那就是以主谓宾的方式来表达，同样的编写用例也可以遵循这个结构。

###### 【建模过程】

- 用例模型

> 用例：每个用例提供了一个或多个场景，该场景说明了系统是如何和最终用户或其它系统互动，也就是谁可以用系统做什么，从而获得一个明确的业务目标。编写用例时要避免使用技术术语，而应该用最终用户或者领域专家的语言。

用例有严格的规范，回顾上文系统分析里面，我们对系统功能分析给出一个公式：功能 = 主体 + 操作 + 操作对象。用例也是需要这样的结构，比如“我爱你”是完整的用例，能完整的描述一件事情，而“爱你”则不能称为一个用例。所以用例模型建立阶段就要力求把用户诉求都完整的以用例表达出来。

- 业务模型

> 业务模型：业务模型采用业务用例来绘制，表达业务的观点。

我们在数据平台对用例模型进行抽象。

**分析师** 为客户 **制作** 业务 **报表**：

- 主语：分析师
- 状语：客户
- 谓语：制作
- 定语：某一项业务
- 宾语：报表

经过我们对语言的分析，已经很清晰地呈现出我们的业务模型，就是分析师制作报表，加上状语和定语的修饰，我们知道是为客户这个主体创建的报表，而且是特定领域的报表，状语就是跟分析师强相关的。

现在我们明确了业务模型后，接着就是细化用户，补充更多的细节：

- **分析师** 为 不同的客户  **制作** 不同业务的 **报表**
- **分析师** 为 不同的客户  **制作** 几款业务的 **报表**
- **分析师** 为 不同的客户  **制作** 一项业务不同区域的 **报表**
- **两个分析师** 为 某个群体用户  **制作** 业务线的 **报表**
- **客户** **授权** 某个群体  查看  不同业务的 **报表**

结合我们对业务的了解，可以丰富领域的属性，还有一个隐性的“权限”名词，我们需要独立出来，因为权限不属于任何一个领域。

通常我们需要角色概念来管理用户访问报表的权限。

- **管理员** 为 不同的客户  **创建** 一项业务不同区域的 **角色**
- **管理员** 为 不同的客户  **分配** 一项业务不同区域的 **角色**
- **小二** 为不同报表  **创建** 不同  **权限**
- 系统模型
  - 系统模型：系统模型是一个系统某一方面本质属性的描述，它以某种确定的形式（如文字、符号、图表、实物、数学公式等）提供关于该系统的知识。

丰富业务场景后，整体的用例如下图：

- **分析师** 为 不同的客户  **制作** 不同业务的 **报表**
- **工程师** 为制作 个性  报表
- 小二 给不同业务创建报表模板 来生成报表
- 小二创建权限 来 匹配报表
- 客户创建角色
- 客户分配角色
- 客户筛选人群进行营销活动
- 前置条件：业务告警？

![12.png](https://ucc.alicdn.com/pic/developer-ecology/00acab37479e4ac0946221402ef46747.png)



#### 3）领域驱动

领域驱动则是一开始就站在上帝视角来着手业务，领域驱动要求化整为零，它是一种由内而外，先内功后招式的思想。它要求团队里有资深的业务领域专家，该专家对业务领域极其了解，不但要了解其然，还要理解其所以然，或者是能够跟领域专业人员学习到足够的领域知识。

在此条件下，**团队将从业务领域里找出反映业务本质的那些事物、规则和结构，把它抽象化，描述业务运行的基本原理和业务交互的机制，识别出用户的首要利益。**

领域模型是采用业务对象建立起来的一种模型，我们把领域模型当中使用到的业务对象称为**领域类**。

**领域模型的核心思想是对象，而领域驱动的核心是分层**，需要对实现架构进行分层，不同的团队，不同业务可能会有相应不同的分层，但是整体上分层的思想就是解耦，把复杂的事情分解开来简单化处理。

###### 【建模过程】

有了领域建模的基础知识后，下面我们介绍下领域建模的过程。

- 用户访谈：充分贴合业务，基于现有人员资源能力；
- 领域知识：首先我们分析项目在领域分层后的概念项目涉及到：
  - 名词：分析师，工程师，客户，小二
  - 报表，报表模板，权限，角色，告警，人群，活动，决策
  - 动词：登陆，创建权限，匹配权限，授权，建表，圈人，营销
  - 实体：报表，报表模板，权限，角色，人群，活动，决策
  - 值对象：告警
  - 服务：登陆，创建权限，报表匹配权限，授权给用户，创建报表，圈人，营销
  - 模块：报表域，权限域，洞察域，营销域
  - 聚合根：报表（报表模板，报表数据），权限（权限，角色），活动（人群，营销规则）
  - 工厂：报表模板工厂，人群工厂，决策工厂，权限工厂
  - 资源库：数据库，消息，外部接口
- 领域模型：经过对领域知识的消化，就可以输出领域模型图。

### 4. 架构推导

> 架构：对系统中实体与实体之间的关系进行抽象的描述，用于指导软件系统各个方面的设计。

#### 1）推导架构

**先问题，后定位**，即：先使命后愿景，解决什么问题？先定义问题，何为问题，有矛盾即存在问题，专业的抽象和架构知识，以及背后的归纳和演绎的逻辑思考方法，加上丰富的业务用例，通过逻辑排列，形成业务架构，首先我们会用以下的表格来描述问题。

![15.jpg](https://ucc.alicdn.com/pic/developer-ecology/c40882b798e442558d317f3c0f96612f.jpg)

- **演绎**
  - 将用例进行抽象分类成为业务模型
  - 将业务模型进行 IT 层面的思考，增加非功能性的组件形成系统模型
- **归纳**
  - 将用例以及问题进行分类聚合
  - 业务用例形成系统架构过程需要进行归纳
  - 对行为稳定性，性能考虑的总结，归纳为通用组件

#### 2）架构输出

- 方案概述：对设计方案的概括性描述；
- 设计约束：包括要遵循的标准或规范，技术上依赖的假设条件等；
- 技术选型：包括系统运行的软硬件环境，研发、测试的软硬件环境，编程语言，现有或开源框架、平台、模块、基础库的重用策略；
- 系统结构：包括系统的网络部署结构，子系统划分，推荐用 UML 部署图、包图描述；
- 关键技术设计：每个系统关键点不一样，但一般都会有安全设计，一些算法的设计；
- 接口设计：包括协议栈，子系统间的接口数据结构，子系统间的业务流程描述。业务流程推荐用 UML 序列图描述；
- 数据设计：流动的数据已通过接口设计，这里描述要存储的数据，数据的组织形式不一样，比如 NoSQL，NewSQL，SQL 等不同类型，描述方式也会不一样，关系数据库推荐用 ER 模型描述顶层逻辑结构，字段表描述物理结构；
- 质量预测：对遗留缺陷率、平均无故障运行时间等质量指标进行预测，提出可能出现的缺陷和问题。

#### 3）架构总结

- **自底向上**：由点及面，步步为营，通过用例堆积，分类，归纳，划分，内聚，逐步扩大范围，再通过剥离，复用，从业务架构到技术架构；
- **自顶向下**：洞察客户背后的本质需求，定义问题，分析问题，问题分类，优先级，升层思考，一上来自带上帝视角。

实际应用，两者结合。


















