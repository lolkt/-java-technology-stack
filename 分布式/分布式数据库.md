## **什么是分布式数据库**

- **分布式数据库是服务于写多读少、低延时、海量并发 OLTP 场景的，具备海量数据存储能力和高可靠性的关系型数据库**。
- 外部视角
  - 定义1.0 OLTP关系型数据库
    - 写多读少、低延时、高并发
      - 之所以强调写多读少，因为写操作的负载只能是单体数据库的主节点上，是无法 转移的;而读操作，如果对一致性要求不高可以转移到备节点，甚至在某些条件下还能保证一致 性。就是说单体数据库可以通过一主多备解决读负载大的问题，而无需引入分布式数据库
  - 定义 2.0 + 海量并发
    - 传统关系型数据库往往是单机模式，也就是主要负载运行在一台机器上。这样，数据库的 并发处理能力与单机的资源配置是线性相关的，所以并发处理能力的上限也就受限于单机 配置的上限。这种依靠提升单机资源配置来扩展性能的方式，被称为**垂直扩展**(Scale Up)。
    - 分布式数据库就不同了，在维持关系型数据库特性不变的基础上，它可以通过**水平扩展**，也就是增加机器数量的方式，提供远高于单体数据库的并发量。这个并发量几乎不受 单机性能限制，我将这个级别的并发量称为“海量并发”。
  - 定义 3.0 + 高可靠
    - 我猜你会建议用 RAID(独立冗余磁盘阵列)来提高磁盘的可靠性。这确实是一个办法，但 也会带来性能上的损耗和存储空间上的损失。分布式数据库的副本机制可以比 RAID 更好地平衡可靠性、性能和空间利用率三者的关系。**副本机制就是将一份数据同时存储在多个 机器上，形成多个物理副本。**
    - 回到数据库的话题上，可靠性还要更复杂一点，包括两个度量指标，恢复时间目标 (Recovery Time Objective, RTO)和恢复点目标(Recovery Point Objective, RPO)。**RTO 是指故障恢复所花费的时间，可以等同于可靠性;RPO 则是指恢复服务后丢 失数据的数量。**
      - 数据库存储着重要数据，而金融行业的数据库更是关系到客户资产安全，不能容忍任何数 据丢失。所以，数据库高可靠意味着 RPO 等于 0，RTO 小于 5 分钟。
      - 分布式数据 库则是一个很好的备选方案，它**凭借节点之间的互为备份、自动切换的机制**，降低了 x86 服务器的单点故障对系统整体的影响，提供了高可靠性保障。
  - 定义 4.0 + 海量存储
    - 虽然单体数据库依靠外置存储设备可以扩展存储能力，但这种方式本质上不是数据库的能 力。现在，借助分布式的**横向扩展架构**，通过物理机的本地磁盘就可以获得强大的存储能 力，这让海量存储成为分布式数据库的标配。

- 内部构成
  - **客户端组件 + 单体数据库**
    - 通过独立的逻辑层建立数据分片和路由规则，实现单体数据库的初步管理，使应用能够对 接多个单体数据库，实现并发、存储能力的扩展。其作为应用系统的一部分，对业务侵入 较为深。
    - 这种客户端组件的典型产品是 Sharding-JDBC。
  - **代理中间件 + 单体数据库**
    - 以独立中间件的方式，管理数据规则和路由规则，以独立进程存在，与业务应用层和单体 数据库相隔离，减少了对应用的影响。随着代理中间件的发展，还会衍生出部分分布式事 务处理能力。
    - 这种中间件的典型产品是 MyCat。
  - **单元化架构 + 单体数据库**
    - 单元化架构是对业务应用系统的彻底重构，**应用系统被拆分成若干实例，配置独立的单体 数据库，让每个实例管理一定范围的数据。**例如对于银行贷款系统，可以为每个支行搭建 独立的应用实例，管理支行各自的用户，当出现跨支行业务时，由应用层代码通过分布式 事务组件保证事务的 ACID 特性。
  - 看过这三种方案，我相信你能够明白，它们共同的特点是单体数据库仍然能够被应用系统 感知到。相反，分布式数据库则是**将技术细节收敛到产品内部，以一个整体面对业务应用。**



## **强一致性**

- 强一致性意味着什么
  - 有人说，只要使用了 Paxos 或者 Raft算法，就可以实现强一致性;也有人说，根据 CAP 原理只能三选二，分区容忍性和高可用 性又是必不可少的，所以分布式数据库是做不到强一致性的。可是，**这些观点或多或少都 是有问题的**。
  - 对于分布式系统而言，一致性是在探讨当系统内的一份逻辑数据存在多个物理的数据副本 时，对其执行读写操作会产生什么样的结果，这也符合 CAP 理论对一致性的表述。
  - 而在数据库领域，“一致性”与事务密切相关，又进一步细化到 ACID 四个方面。其中，I 所代表的隔离性(Isolation)，是“一致性”的核心内容，研究的就是如何协调事务之间 的冲突。
  - 因此，当我们谈论分布式数据库的一致性时，实质上是在谈论**数据一致性**和**事务一致性**两个方面。
- **数据一致性**
  - 包括分布式数据库在内的分布式存储系统，为了避免设备与网络的不可靠带来的影响，通 常会存储多个数据副本。逻辑上的一份数据同时存储在多个物理副本上，自然带来了数据 一致性问题。
  - 强一致性:MySQL全同步复制
    - 显然，用户获得响应时，主库和备库的数据副本已经达成一致，所以后续的读操作肯定是 没有问题的，但这种模式的副作用非常大，体现在性能差、可用性问题。
    - 集群规模越大，这些问题就越严重，所以全同步复制模式在生产系统中也很少使用。更进 一步说，在工程实践中，实现状态视角的强一致性需要付出的代价太大，尤其是与可用性 有无法回避的冲突，所以很多产品选择了状态视角的弱一致性。
  - 弱一致性:NoSQL 最终一致性
    - NoSQL 产品是应用弱一致性的典型代表，但对弱一致性的接受仍然是有限度的，这就是 BASE 理论中的 E 所代表的最终一致性(Eventually Consistency)，弱于最终一致性的产 品就几乎没有了。
    - 对于最终一致性，你可以这样理解:在主副本执行写操作并反馈成功时，不要求其他副本 与主副本保持一致，但在经过一段时间后这些副本最终会追上主副本的进度，重新达到数 据状态的一致。
    - 你再仔细推敲一下，是不是觉得这个定义还有点含糊?“经过一段时间”到底是多久呢? 几秒还是几分钟?如果是一个不确定的数值，怎么在工程中使用呢?
- **操作视角**
  - **写后读一致性**
    - 它也称为“读写一致 性”，或“读自己写一致性”(Read My Writes Consistency)。你可能觉得最后一个名 字听上去有些奇怪，但它却最准确地描述了这种一致性模型的使用效果。
    - 自己写入成功的任何数据，下一刻一定能读取到，其内容保证与自己最后一次写入完全一 致，这就是“读自己写一致性”名字的由来。
  - **单调读一致性**
    - 关于单调读一致性的定义，常见的解释是这样的:一个用户一旦读到某个值，不会读到比这 个值更旧的值。
    - 实现单调读一致性的方式，可以是将用户与副本建立固定的映射关系，比如使用哈希算法 将用户 ID 映射到固定副本上，这样避免了在多个副本中切换。
  - **前缀一致性**
    - 小明和小红的评论分别写入了节点 N1 和 N2，但是它们与 N3 同步数据时，由于网络传输 的问题，N3 节点接收数据的顺序与数据写入的顺序并不一致，所以小刚是**先看到答案后看 到问题**。
    - 显然，问题与答案之间是有因果关系的，但这种关系在复制的过程中被忽略了，于是出现 了异常。
    - 保持这种因果关系的一致性，被称为**前缀读**或**前缀一致性**(Consistent Prefix)。要实现 这种一致性，可以考虑在原有的评论数据上增加一种显式的因果关系，这样系统可以据此 控制在其他进程的读取顺序。
  - **线性一致性**
    - 在“前缀一致性”的案例中，问题与答案之间存在一种显式声明，但在现实中，多数场景 的因果关系更加复杂，也不可能要求全部做显式声明。
    - 线性一致性(Linearizability)就是建立在事件的先后顺序之上的。在线性一致性下，整个 系统表现得好像只有一个副本，所有操作被记录在一条时间线上，并且被原子化，这样任 意两个事件都可以比较先后顺序。
    - 但是，集群中的各个节点不能做到真正的时钟同步，这样节点有各自的时间线。那么，如 何将操作记录在一条时间线上呢?这就需要一个绝对时间，也就是**全局时钟**。
  - **因果一致性**
    - 既然线性一致性不够完美，那么有没有不依赖绝对时间的方法呢?
    - 因果一致性的基础是**偏序关系**，也就是说，部分事件顺序是可以比较的。至少一个节点内 部的事件是可以排序的，依靠节点的本地时钟就行了;节点间如果发生通讯，则参与通讯 的两个事件也是可以排序的，接收方的事件一定晚于调用方的事件。
    - 借助**逻辑时钟**仍然可以建立全序关系，当然这个全序关系是不够精确的。因为如果两个事 件并不相关，那么逻辑时钟给出的大小关系是没有意义的。
    - 多数观点认为，**因果一致性弱于线性一致性，但在并发性能上具有优势**，也足以处理多数 的异常现象，所以因果一致性也在工业界得到了应用。
    - 因果一致性是靠逻辑时钟确定偏序关系，**不需要应用介入**;而前缀一 致性靠事件之间显式声明的依赖关系，**可以在应用层处理**
- 有些时候 大家又会将 Paxos 称为一致性协议。你觉得这个“一致性协议”和数据一致性又是什么关 系呢?
  - 数据一致性是从数据的用户视角出发对数据属性的描述，而paxos协议是达成共识 的过程的一种实现方式，是从数据的生产者或者维护者角度出发的
    - 从状态视角看，是不是只有全同步这种方式实现了强一致性，即使像paxos、raft这些实现了操作上线性一致性的算法，从状态视角看也不是强一致的。
    - 然而全同步降低了系统的可用性，paxos、raft不保证所有节点状态的一致，而是通过额外 的算法来保证操作视角的一致性，同时提高了系统的可用性。
  - **一致性模型里有两个要点，读写策略和多副本状态**
    - Raft是多数派协议，从写入成功那一刻的数据状态来说，肯定不是一致的。不过，通过操作方面 的封装，约定由主副本对外提供服务，所以不会体现出副本间的差异。一致性模型，除了副本的 状态，还要看读写操作。**最终一致性的定义，其实只是描述了副本的状态而已。**我认为，一致性 模型，主要还是从读写操作的效果来分析，也数据副本的一致性有关但不是强依赖。比如，如果 不使用Raft，用半同步，也可以做到线性一致性。
  - CAP的C也是Consistency，是多副本、单操作的数据一致性;而ACID里的 C是指单副本、多操作的事务一致性。
    - Paxos这类共识算法，可以看作是复制协议的一种，虽然有 时也叫做一致性协议，但这个一致性是指Consensus。**Consensus是实现数据一致性目标下的具 体技术，但并不是唯一的选择。**采用主从复制也可以达到同样效果，比如PGXC风格的分布式数据库就是采用主从复制的方式。
    - CAP中的C就是Consistency，是数据一致性，也是我们所说的操作视角的一致 性，这里包含的多副本和读写策略两层含义。共识算法是复制协议层面的内容，并不一定对操作 做严格定义。比如，就算我们使用Raft算法，但是如果开放了Follower读，也有可能达不到线性 一致性或因果一致性的。事实上，CockroachDB的Follower读就是这样的。
- **事务一致性**
  - 虽然 ACID 名义上并列为事务的四大特性，但它们对于数据库的重要程度并不相同。
    - 第一个是**一致性，它无疑是其中存在感最低的特性，可以看作是对 “事务”整体目标的阐 述。**它并没有提出任何具体的功能需求，所以在数据库中也很难找到针对性的设计。
    - 第二个是持久性，它不仅是对数据库的基本要求。如果你仔细琢磨下持久性的定义，就会 发现**它的核心思想就是要应对系统故障**。怎么理解系统故障呢?我们可以把故障分为两 种。
      - **存储硬件无损、可恢复的故障**。这种情况下，主要依托于预写日志(Write Ahead Log, WAL)保证第一时间存储数据。WAL 采用顺序写入的方式，可以保证数据库的低延时 响应。WAL 是单体数据库的成熟技术，NoSQL 和分布式数据库都借鉴了过去。
      - **存储硬件损坏、不可恢复的故障**。这种情况下，需要用到日志复制技术，将本地日志及 时同步到其他节点。实现方式大体有三种:第一种是单体数据库自带的同步或半同步的 方式，其中半同步方式具有一定的容错能力，实践中被更多采用;第二种是将日志存储 到共享存储系统上，后者会通过冗余存储保证日志的安全性，亚马逊的 Aurora 采用了 这种方式，也被称为 Share Storage;第三种是基于 Paxos/Raft 的共识算法同步日志 数据，在分布式数据库中被广泛使用。无论采用哪种方式，目的都是保证在本地节点之 外，至少有一份完整的日志可用于数据恢复。
    - 第三个是原子性，是数据库区别于其他存储系统的重要标志。在单体数据库时代，原子性 问题已经得到妥善解决，但**随着向分布式架构的转型，在引入不可靠的网络因素后，原子 性又成为一个新的挑战。**
      - **分布式数据库是在分布式架构上实现的关系型数 据库，那么就必须支持事务，首先就要支持原子性。**原子性，在实现机制上较为复杂，目标却很简单，和分成多个级别的隔离性不同，原子性就只有支持和不支持的区别。
    - 最后一个是隔离性，它是事务中最复杂的特性。隔离性分为多个隔离级别，较低的隔离级 别就是在正确性上做妥协，将一些异常现象交给应用系统的开发人员去解决，从而获得更 好的性能。
  - 1995 年，Jim Gray 等人 发表了论文“A Critique of ANSI SQL Isolation Levels”(以下简称 **Critique**)，对于 事务隔离性进行了更加深入的分析
    - **幻读和写倾斜**无疑则是通往最高隔离级别的两座大山
      - Critique 对幻读的描述大致是这样的，事务 T1 使用特定的查询条件获得一个结果集，事 务 T2 插入新的数据，并且这些数据符合 T1 刚刚执行的查询条件。T2 提交成功后，T1 再 次执行同样的查询，此时得到的结果集会增大。这种异常现象就是幻读。
      - 不少人会将幻读与不可重复读混淆，这是因为它们在自然语义上非常接近，都是在一个事 务内用相同的条件查询两次，但两次的结果不一样。差异在于，**对不可重复读来说，第二 次的结果集相对第一次，有些记录被修改(Update)或删除(Delete)了;而幻读是第二 次结果集里出现了第一次结果集没有的记录 (Insert)。一个更加形象的说法，幻读是在第一 次结果集的记录“间隙”中增加了新的记录**。所以，MySQL 将防止出现幻读的锁命名为间 隙锁(Gap Lock)。
    - 写倾斜要稍微复杂一点，我用一个黑白球的例子来说明
      - 首先，箱子里有三个白球和三个黑球，两个事务(T1,T2)并发修改，不知道对方的存在。 T1 要让 6 个球都变成白色;T2 则希望 6 个球都变成黑色。
      - 根据可串行化的定义，“多事务并行执行所得到的结果，与串行执行(一个接一个)完全 相同”。比照两张图，很容易发现事务并行执行没有达到串行的同等效果，所以这是一种 异常现象。也可以说，写倾斜是一种更不易察觉的更新丢失。
  - 隔离性的产品实现
    - 第一个方向是，用真正的串行化实现“可串行化”隔离。我们往往认为多线程并发在性能 上更优，但 **Redis 和 VoltDB** 确实通过串行化执行事务的方式获得了不错的性能。考虑到 VoltDB 作为一款分布式数据库的复杂度，其成功就更为难得了。我想，其中部分原因可能 在于内存的大量使用，加速了数据计算的过程。另外，VoltDB 以存储过程为逻辑载体的方 式，也使得事务有了更多的优化机会。
    - 如果说第一个方向有点剑走偏锋，那第二个方向就是硬桥硬马了。没错，还是在并发技术 上继续做文章。PostgreSQL 在 2008 年提出了 **Serializable Snapshot Isolation (SSI)， 这实际就是可串行化**。而后，兼容 PostgreSQL 生态的 CockroachDB，也同样选择支持 SSI，而且是唯一支持的隔离级别。
- **分布式数据库的强一致性**
  
  - | 隔离级别         | 线性一致性     | 因果一致性  |
    | ---------------- | -------------- | ----------- |
    | 可串行化（SSL）  | Spanner        | CockroachDB |
    | 快照隔离（SL）   | TiDb           | YugabyteDB  |
    | 可重复度读（RR） | GolenDB        |             |
    | 已提交读（RC）   | OceanBase 2.0+ |             |





## NewSQL和PGXC

- **总的来说，分布式数据库大多可以分为两种架构风格**
  - 一种是 NewSQL，它的代表系统是 Google Spanner;
  - 另一种是从单体数据库中间件基础上演进出来的，被称为 Prxoy 风 格，没有公认的代表系统，便于理解，所以选了一个出现较早的产品来指代这种风 格，这就是 PostgreSQL-XC(下文简称 PGXC)。
- 数据库的基本架构
  - 数据库从逻辑上拆分为 5 个部分，分别是客户端 通讯管理器 (Client Communications Manager)、查询处理器(Relational Query Processor)、事务存储管理器(Transactional Storage Manager)、进程管理器 (Process Manager)和共享组件与工具 (Shared Components and Utilities)，每个部 分下面又可以拆分成一些组件。
  - **客户端通讯管理器。**这是应用开发者能够直观感受到的模块，通常我们使用 JDBC 或者 ODBC 协议访问数据库时，连接的就是这个部分。
  - **进程管理器。**连接建好了，数据库会为客户端分配一个进程，客户端后续发送的所有操 作都会通过对应的进程来执行。
  - **查询处理器。**它包括四个部分，功能上是顺序执行的。首先是**解析器**，它将接收到的 SQL 解析为内部的语法树。然后是**查询重写(Query Rewrite)**，它也被称为逻辑优 化，主要是依据关系代数的等价变换，达到简化和标准化的目的，比如会消除重复条件 或去掉一些无意义谓词 ，还有将视图替换为表等操作。再往后就是**查询算法优化 (Query Optimizer)**，它也被称为物理优化，主要是根据表连接方式、连接顺序和排 序等技术进行优化，我们常说的基于规则优化(RBO)和基于代价优化(CBO)就在这 部分。最后就是**计划执行器(Plan Executor)**，最终执行查询计划，访问存储系统。
  - **事务存储管理器。**它包括四个部分，其中访问方式(Access Methods)是指数据在磁 盘的具体存储形式。锁管理(Lock Manager)是指并发控制。日志管理(Log Manager)是确保数据的持久性。缓存管理(Buffer Manager)则是指 I/O 操作相关 的缓存控制。
  - **共享组件和工具。**在整个过程中还会涉及到的一些辅助操作，当然它们对于数据库的运 行也是非常重要的。例如编目数据管理器(Catalog Manager)会记录数据库的表、字 段、视图等元数据信息，并根据这些信息来操作具体数据内容。**复制机制 (Replication)也很重要，它是实现系统高可靠性的基础**，在单体数据库中，通过主备 节点复制的方式来实现数据的复制。
- **PGXC**:单体数据库的自然演进
  - 单体数据库的功能看似已经很完善了，但在面临高并发场景的时候，还是会碰到写入性能 不足的问题，很难解决。因此，也就有了向分布式数据库演进的动力。要解决**写入性能不足的问题**，大家首先想到的，最简单直接的办法就是**分库分表**。
    - 分库分表方案就是在多个单体数据库之前增加代理节点，**本质上是增加了 SQL 路由功能**。 这样，**代理节点首先解析客户端请求，再根据数据的分布情况，将请求转发到对应的单体 数据库。**
    - **代理节点需要实现三个主要功能，它们分别是客户端接入、简单的查询处理器和进程管理 中的访问控制。**
    - 另外，分库分表方案还有一个重要的功能，那就是**分片信息管理**，分片信息就是数据分布 情况，是区别于编目数据的一种元数据。不过考虑到分片信息也存在多副本的一致性的问 题，大多数情况下它会独立出来
    - 显然，如果把每一次的事务写入都限制在一个单体数据库内，业务场景就会很受局限。因 此，跨库事务成为必不可少的功能，但是单体数据库是不感知这个事情的，所以我们就要**在代理节点增加分布式事务组件**。
    - 同时，简单的分库分表不能满足全局性的查询需求，因为每个数据节点只能看到一部分数 据，有些查询运算是无法处理的，比如排序、多表关联等。所以，**代理节点要增强查询计算能力，支持跨多个单体数据库的查询**。
    - 随着分布式事务和跨节点查询等功能的加入，**代理节点已经不再只是简单的路由功能，更 多时候会被称为协调节点。**
  - **很多分库分表方案会演进到这个阶段，比如 MyCat。这时离分布式数据库还差重要的一 步，就是全局时钟**
    - 加上这最后一块拼图，PGXC 区别于单体数据库的功能也就介绍完整了，它们是分片、分 布式事务、跨节点查询和全局时钟。
  - **协调节点与数据节点，实现了一定程度上的计算与存储分离，这也是所有分布式数据库的 一个架构基调。**但是，因为 PGXC 的数据节点本身就是完整的单体数据库，所以也具备很 强的计算能力。
- **NewSQL**:革命性的新架构
  - NewSQL 也叫原生分布式数据库，我 觉得这个名字能更准确地体现这类架构风格的特点，就是说**它的每个组件在设计之初都是基于分布式架构的，不像 PGXC 那样带有明显的单体架构痕迹。**
    - NewSQL 的基础是 NoSQL，更具体地说，是类似 BigTable 的**分布式键值(K/V)系统**。 分布式键值系统选择做了一个减法，**完全放弃了数据库事务处理能力，然后将重点放在对 存储和写入能力的扩展上**，这个能力扩展的基础就是**分片**。引入分片的另一个好处是，系 统能够以更小的粒度调度数据，实现各节点上的存储平衡和访问负载平衡。
    - **分布式键值系统由于具备这些鲜明的特点，所以在不少细分场景获得了成功(比如电商网 站对于商品信息的存储)，但在面对大量的事务处理场景时就无能为力了(比如支付系 统)。**这种状况直到 **Google Spanner** 横空出世才被改变，因为 Spanner 基于 BigTable 构建了新的事务能力。
  - 除了上述内容，NewSQL 还有两个重要的革新，分别出现在**高可靠机制和存储引擎**的设计上。
    - 高可靠机制的变化在于，放弃了粒度更大的主从复制，转而**以分片为单位采用 Paxos 或 Raft 等共识算法**。这样，NewSQL 就实现了更小粒度的高可靠单元，获得了更高的系统整 体可靠性。存储引擎层面，则是使用 **LSM-Tree 模型**替换 B+ Tree 模型，大幅提升了写入 性能。
    - Spanner 是 NewSQL 的开山鼻祖，这个不用说了;其他知名度比较高的产品有 CockroachDB、TiDB 和 YugabyteDB，这三款数据库都宣称设计灵感来自 Spanner
  - 当然，**NewSQL 的架构设计也不是完美无缺**。比如，**作为一个计算与存储分离得更加彻底 的架构，NewSQL 的计算节点需要借助网络才能与存储节点通讯**，这意味着要花费更大的 代价来传输数据。随着 NewSQL 分布式数据库的应用实践越来越多，很多产品为了获得更 好的计算性能，会尽量将更多计算下压到存储节点执行。这种架构上的修正，似乎也可以 理解为，NewSQL 朝 PGXC 的方向做了一点回拨。



## 全局时钟

- TrueTime 作为全局时钟的一种实现形式，它是 Google通过 GPS 和原子钟两种方式混合提供的授时机制，误差可以控制在 7 毫秒以内。正是在这 7 毫秒内，时光是可能倒流的。

- 线性一致性，它的基础就是全局时钟，还有后面会讲到的多版本并发控制(MVCC)、快照、乐观 协议与悲观协议，都和时间有关。

- **常见授时方案**

  - 三个要素

    - 时间源:单个还是多个
    - 使用的时钟类型:物理时钟还是混合逻辑时钟
    - 授时点:一个还是多个

  - 常见的方案主要只有 4 类

    - NTP(Network Time Protocol)误差大， 也不能保证单调递增，所以就没有单独使用 NTP 的产品

    - |          | 物理时钟            | 物理时钟 | 混合逻辑时钟       | 混合逻辑时钟 |
      | -------- | ------------------- | -------- | ------------------ | ------------ |
      |          | 多时间源            | 单时间源 | 多时间源           | 单时间源     |
      | 单点授时 | N/A                 | N/A      | N/A                | TSO（TiDB）  |
      | 多点授时 | TrueTime（Spanner） | NTP      | HLC（CockroachDB） | STP（巨杉）  |

  - **TrueTime**

    - 采用了多点授时机制，就是说集群内有多个时间服务器都可以提供授时 服务。
    - 例如，A、B 两个进程先后调用 TrueTime 服务，各自拿到一个时间区间，如果在其中随机选择，则可能出现 B 的时间早 于 A 的时间。不只是 TrueTime，**任何物理时钟都会存在时钟偏移甚至回拨。**
    - **单个物理时钟会产生误差，而多点授时又会带来整体性的误差**，那 TrueTime 为什么还要 这么设计呢?
      - 因为**它也有两个显著的优势**:首先是**高可靠高性能**，多时间源和多授时点实现了完全的去 中心化设计，不存在单点;其次是**支持全球化部署**，客户端与时间服务器的距离也是可控 的，不会因为两者通讯延迟过长导致时钟失效。

  - **HLC**

    - CockroachDB 和 YugabyteDB 也是以高性能高可靠和全球化部署为目标，不过 Truetime 是 Google 的独门绝技，它依赖于特定硬件设备的思路，不适用于开源软件。所以，它们 使用了混合逻辑时钟(Hybrid Logical Clock，HLC)
    - 同样是多时间源、多点授时，但时钟采用了物理时钟与逻辑时钟混合的方式。HLC 在实现机制上也是蛮复杂的，而且和 TrueTime 同样有整体性的时间误差。

  - **TSO**

    - 其他的分布式数据库大多选择了单时间源、单点授时的方式，承担这个功能的组件在 NewSQL 风格架构中往往被称为 TSO(Timestamp Oracle)，而**在 PGXC 风格架构中被 称为全局事务管理器**(Golobal Transcation Manager，GTM)。**这就是说一个单点递增的时间戳和全局事务号基本是等效的。**
    - 这种授时机制的最大优点就是实现简便，如果能够 保证时钟单调递增，还可以简化事务冲突时的设计。但缺点也很明显，集群不能大范围部 署，同时性能也有上限。TiDB、OceanBase、GoldenDB 和 TBase 等选择了这个方向。

  - **STP**

    - 最后，还有一些小众的方案，比如巨杉的 STP(SequoiaDB Time Protoco)。它采用了单时 间源、多点授时的方式，优缺点介于 HLC 和 TSO 之间。

- **中心化授时:TSO（TiDB）**

  - **TiDB 的全局时钟是一个数值，它由两部分构成，其中高位是物理时间**，也就是操作系统的 毫秒时间;**低位是逻辑时间**，是一个 18 位的数值。那么从存储空间看，1 毫秒最多可以产 生 262,144 个时间戳(2^18)，这已经是一个很大的数字了，一般来说足够使用了。
  - **单点授时首先要解决的肯定是单点故障问题**。TiDB 中提供授时服务的节点被称为 Placement Driver，简称 PD。多**个 PD 节点构成一个 Raft 组**，这样通过共识算法可以保 证在主节点宕机后马上选出新主，在短时间内恢复授时服务。
  - 如何保证新主产生的时间戳一定大于旧主呢?那就必须将旧主的时间戳存储 起来，存储也必须是高可靠的，所以 **TiDB 使用了 etcd**。但是，每产生一个时间戳都要保 存吗?显然不行，那样时间戳的产生速度直接与磁盘 I/O 能力相关，会存在瓶颈的。
  - TiDB的PD虽然是高可靠的，但工作的只是主节点，所以还是**单点授时**;**多时间源**，是说多个独立提供时间的实例，比如部分原子钟和GPS坏掉了， 其他的原子钟可以照常提供时间不受影响。
  - TiDB 采用**预申请时间窗口**的方式
    - 当前 PD(主节点)的系统时间是 103 毫秒，PD 向 etcd 申请了一个“可分配的时间窗 口”。要知道时间窗口的跨度是可以通过参数指定的，系统的默认配置是 3 毫秒，示例采 用了默认配置，所以这个窗口的起点是 PD 当前时间 103，时间窗口的终点就在 106 毫秒 处。写入 etcd 成功后，PD 将得到一个从 103 到 106 的“可分配时间窗口”，在这个 时间窗口内 PD 可以使用系统的物理时间作为高位，拼接自己在内存中累加的逻辑时间， 对外分配时间戳。
    - 上述设计意味着，**所有 PD 已分配时间戳的高位，也就是物理时间，永远小于 etcd 存储的 最大值。**那么，如果 PD 主节点宕机，新主就可以读取 etcd 中存储的最大值，在此基础上 申请新的“可分配时间窗口”，这样新主分配的时间戳肯定会大于旧主了。
    - 此外，为了降低通讯开销，**每个客户端一次可以申请多个时间戳**，时间戳数量作为参数， 由客户端传给 PD。**但要注意的是，一旦在客户端缓存，多个客户端之间时钟就不再是严格 单调递增的**，这也是追求性能需要付出的代价。

- **分布式授时:HLC(CockroachDB)**

  - 假如我们有 ABCD 四个节点，方框是节点上发生的事件，方框内的**三个数字依次是节点的 本地物理时间(简称本地时间，Pt)、HLC 的高位(简称 L 值)和 HLC 的低位(简称 C 值)。**
  - A 节点的本地时间初始值为 10，其他节点的本地时间初始值都是 0。四个节点的第一个事 件都是在节点刚启动的一刻发生的。首先看 A1，它的 HLC 应该是 (10,0)，其中高位直接 取本地时间，低位从 0 开始。同理，其他事件的 HLC 都是 (0,0)。
  - **事件 D2 发生时，首先取上一个事件 D1 的 L 值和本地时间比较**。
    - L 值等于 0，本地时间已 经递增变为 1，取最大值，那么用本地时间作为 D2 的 L 值。高位变更了，低位要归零， 所以 D2 的 HLC 就是 (1,0)。
  - 如果节点间有调用关系，计时逻辑会更复杂一点。
    - **我们看事件 B2，要先判断 B2 的 L 值， 就有三个备选**:
      - 本节点上前一个事件 B1 的 L 值
      - 当前本地时间
      - 调用事件 A1 的 L 值，A1 的 HLC 是随着函数调用传给 B 节点的
    - 这三个值分别是 0、1 和 10。按照规则取最大值，所以 B2 的 L 值是 10，也就是 A1 的 L 值，而 C 值就在 A1 的 C 值上加 1，最终 B2 的 HLC 就是 (10,1)。
  - B3 事件发生时，发现当前本地时间比 B2 的 L 值还要小，所以沿用了 B2 的 L 值，而 C 值 是在 B2 的 C 值上加一，最终 B3 的 HLC 就是 (10,2)。

  



## 分片机制

- 大规模的业务应用下，单体数据库遇 到的主要问题是什么?首先就是写入性能不足，另外还 有存储方面的限制。而分片就是解决性能和存储这两个问题的关键设计，甚至不仅是分布 式数据库，在所有分布式存储系统中，分片这种设计都是广泛存在的。

  - 分布式数据库的分片与单体数据库的分区非常相似，区别在于:分区虽然可以将数据表按 照策略切分成多个数据文件，但这些文件仍然存储在单节点上;而**分片则可以进一步根据 特定规则将切分好的文件分布到多个节点上，从而实现更强大的存储和计算能力。**

- 分片机制通常有两点值得关注

  - 分片策略

    - 主要有 Hash(哈希)和 Range(范围)两种

  - 分片的调度机制

    - 分为静态与动态两种。静态意味着分片在节点上的分布基本是固定的，即使移动也需要人 工的介入;动态则是指通过调度管理器基于算法在各节点之间自动地移动分片。

    - |       | 静态        | 动态   |
      | ----- | ----------- | ------ |
      | Hash  | PGXC/NewSQL | N/A    |
      | Range | PGXC        | NewSQL |

- **PGXC**

  - **Hash** **分片**
    - Hash 分片，就是按照数据记录中指定关键字的 Hash 值将数据记录映射到不同的分片中。
      - **因为 Hash 计算会过滤掉数据原有的业务特性，所以可以保证数据非常均匀地分布到多个 分片上，这是 Hash 分片最大的优势**，而且它的实现也很简洁。但示例中采用的分片方法 直接用节点数作为模，**如果系统节点数量变动，模也随之改变，数据就要重新 Hash 计 算，从而带来大规模的数据迁移。显然，这种方式对于扩展性是非常不友好的。**
    - 那接下来的问题就是，我们需要找一个方法提升系统的扩展性。你可能猜到了，这就是**一致性 Hash**
      - **要在工业实践中应用一致性 Hash 算法，首先会引入虚拟节点，每个虚拟节点就是一个分 片。**为了便于说明，我们在这个案例中将分片数量设定为 16。但实际上，因为分片数量决 定了集群的最大规模，所以它通常会远大于初始集群节点数。
      - 16 个分片构成了整个 Hash 空间，数据记录的主键和节点都要通过 Hash 函数映射到这个 空间。这个 Hash 空间是一个 Hash 环。**节点和数据都通过 Hash 函数映射到 Hash 环上，数据按照顺时针找到最近的节点。**
    - **Hash 函数的优点是数据可以较为均匀地分配到各节点，并发写入性能更好。**
      - **本质上，Hash 分片是一种静态分片方式，必须在设计之初约定分片的最大规模。同时，因 为 Hash 函数已经过滤掉了业务属性，也很难解决访问业务热点问题**。所谓业务热点，就 是由于局部的业务活跃度较高，形成系统访问上的热点。这种情况普遍存在于各类应用 中，比如电商网站的某个商品卖得比较好，或者外卖网站的某个饭店接单比较多，或者某 个银行网点的客户业务量比较大等等。
  - **Range** **静态分片**
    - **与 Hash 分片不同，Range 分片的特点恰恰是能够加入对于业务的预估**。例如，我们 用“Location”作为关键字进行分片时，不是以统一的行政级别为标准。因为注册地在北 京、上海的用户更多，所以这两个区域可以按照区县设置分片，而海外用户较少，可以按 国家设置为分片。这样，分片间的数据更加平衡。
    - 相对 Hash 分片，Range 分片的适用范围更加广泛。其中一个非常重要的原因是，**Range 分片可以更高效地扫描数据记录，而 Hash 分片由于数据被打散，扫描操作的 I/O 开销更 大。**但是，**PGXC 的 Range 分片受限于单体数据库的实现机制，很难随数据变动和负载变化而调整。**
  - 虽然有些 PGXC 同时支持两种分片方式，但 Hash 分片仍是主流，比如 GoldenDB 默认使 用 Hash 分片，而 TBase 仅支持 Hash 分片。

- **NewSQL**

  - 总体上，NewSQL 也是支持 Hash 和 Range 两种分片方式的。具体就产品来说， CockroachDB 和 YugabyteDB 同时支持两种方式，TiDB 仅支持 Range 分片
  - **Range** **动态分片**
    - **NewSQL 的 Range 分片，多数是用主键作为关键字来分片的**，当然主键可以是系统自动 生成的，也可以是用户指定的。既然提供了用户指定主键的方式，那么理论上可以通过设 定主键的产生规则，控制数据流向哪个分片。**但是，主键必须保证唯一性，甚至是单调递 增的，导致这种控制就会比较复杂，使用成本较高**。所以，我们基本可以认为，分片是一 个系统自动处理的过程，用户是感知不到的。这样做的好处显然是提升了系统的易用性。
    - 我们将 NewSQL 的 Range 分片称为动态分片，主要有两个原因:
      - **分片可以自动完成分裂与合并**
        - 当单个分片的数据量超过设定值时，分片可以一分为二，这样就可以保证每个分片的数据 量较为均衡。多个数据量较少的分片，会在一定的周期内被合并为一个分片。
      - **可以根据访问压力调度分片**
        - 我们看到系统之所以尽量维持分片之间，以及节点间的数据量均衡，存储的原因外，还可 以更大概率地将访问压力分散到各个节点上。但是，有少量的数据可能会成为访问热点， 就是上面提到的**业务热点**，从而打破这种均衡。
        - 这时候，系统会根据负载情况，将分片分别调度到不同的节点，来均衡访问压力。
  - **存储均衡**和**访问压力均衡**，是 NewSQL 分片调度机制普遍具备的两项能力。此外，还有两 项能力在Spanner 论文中被提及，但在其他产品中没有看到工程化实现。
    - **减少分布式事务**
      - 对分布式数据库来说，有一个不争的事实，那就是分布式事务的开销永远不会小于单节点 本地事务的开销。因此，所有分布式数据库都试图通过减少分布式事务来提升性能。
      - Spanner 在 Tablet，也就是 Range 分片，之下增加了目录(Directory)，作为数据调度 的最小单位，它的调度范围是可以跨 Tablet 的。**通过调度 Directory 可以将频繁参与同样 事务的数据，转移到同一个 Tablet 下，从而将分布式事务转换为本地事务。**
    - **缩短服务延时**
      - 对于全球化部署的分布式数据库，数据可能存储在相距很远的多个数据中心，**如果用户需 要访问远端机房的数据，操作延时就比较长，这受制于数据传输速度。而 Spanner 可以将 Directory 调度到靠近用户的数据中心，缩短数据传输时间。**当然，这里的调度对象都是数据的主副本，跨中心的数据副本仍然存在，负责保证系统整体的高可靠性。
      - Directory 虽然带来新的特性，但显然也削弱了分片的原有功能，分片内的记录不再连续， 扫描要付出更大成本。而**减少分布式事务和靠近客户端位置这本身就是不能兼顾的，再加 上存储和访问压力，分片调度机制要在四个目标间进行更复杂的权衡。**

- **分片与高可靠的关系**

  - NewSQL 与 PGXC 的区别在于，**对于 NewSQL 来说，分片是高可靠的最小单元;而对于 PGXC，分片的高可靠要依附于节点的高可靠。**
  - **NewSQL 的实现方式是复制组(Group)。在产品层面，通常由一个主副本和若干个副本 组成，通过 Raft 或 Paxos 等共识算法完成数据同步**，称为 Raft Group 或 Paxos Group，所以我们简称这种方式为 Group。因为不相关的数据记录会被并发操作，所以同 一时刻有多个 Group 在工作。因此，NewSQL 通常支持 Multi Raft Group 或者 Multi Paxos Group。
    - **每个 Group 是独立运行的，只是共享相同的网络和节点资源，所以不同复制组的主副本是 可以分布在不同节点的。**
  - **PGXC 的最小高可靠单元由一个主节点和多个备节点组成**，我们借用 TDSQL 中的术语，将 其称为 Set。一个 PGXC 是由多个 Set 组成。S**et 的主备节点间复制，多数采用半同步复 制，平衡可靠性和性能。这意味着，所有分片的主副本必须运行在 Set 的主节点上。**
  - **从架构设计角度看，Group 比 Set 更具优势**，原因主要有两个方面。首先，Group 的**高可靠单元更小**，出现故障时影响的范围就更小，系统整体的可靠性就更高。其次，**在主机房 范围内，Group 的主副本可以在所有节点上运行，资源可以得到最大化使用，而 Set 模式 下，占大多数的备节点是不提供有效服务的，资源白白浪费掉。**



## 数据复制

- **分片元数据的存储**
  - 我们知道，**在任何一个分布式存储系统中，收到客户端请求后，承担路由功能的节点首先 要访问分片元数据(简称元数据)，确定分片对应的节点，然后才能访问真正的数据。**这 里说的元数据，一般会包括分片的数据范围、数据量、读写流量和分片副本处于哪些物理 节点，以及副本状态等信息。
  - 从存储的角度看，元数据也是数据，但特别之处在于每一个请求都要访问它，所以元数据 的存储很容易成为整个系统的性能瓶颈和高可靠性的短板。如果系统支持动态分片，那么 分片要自动地分拆、合并，还会在节点间来回移动。这样，元数据就处在不断变化中，又 带来了**多副本一致性(Consensus)的问题**。
- **静态分片**
  - 最简单的情况是静态分片。我们可以忽略元数据变动的问题，只要把元数据复制多份放在 对应的工作节点上就可以了，这样同时兼顾了性能和高可靠
  - **但如果要更新分片信息，这种方式显然不适合，因为副本数量过多，数据同步的代价太大了**。所以**对于动态分片，通常是不会在有工作负载的节点上存放元数据的**。
    - 那要怎么设计呢?有一个凭直觉就能想到的答案，那就是**专门给元数据搞一个小规模的集群，用 Paxos 协议复制数据**。这样保证了高可靠，数据同步的成本也比较低。
    - TiDB 大致就是这个思路，但具体的实现方式会更巧妙一些。
- **TiDB:无服务状态**
  - 在 TiDB 架构中，**TiKV** 节点是实际存储分片数据的节点，而元数据则由 Placement Driver节点管理。Placement Driver 这个名称来自 Spanner 中对应节点角色，简称为 **PD**。
  - 在 PD 与 TiKV 的通讯过程中，PD 完全是被动的一方。TiKV 节点定期主动向 PD 报送**心跳**，分片的元数据信息也就随着心跳一起报送，而 PD 会将分片调度指令放在心跳的返回信息中。等到 TiKV 下次报送心跳时，PD 就能了解到调度的执行情况。
  - **由于每次 TiKV 的心跳中包含了全量的分片元数据，PD 甚至可以不落盘任何分片元数据， 完全做成一个无状态服务。**这样的好处是，PD 宕机后选举出的新主根本不用处理与旧主的 状态衔接，在一个心跳周期后就可以工作了。当然，在具体实现上，PD 仍然会做部分信息 的持久化，这可以认为是一种缓存。
  - **虽然无状态服务有很大的优势，但 PD 仍然是一个单点，也就是说这个方案还是一个中心 化的设计思路，可能存在性能方面的问题**。
- **CockroachDB:去中心化**
  - CockroachDB 的解决方案是使用 Gossip 协议
  - 为什么不用 Paxos 协议呢?
    - 这是因为 Paxos 协议本质上是一种广播机制，也就是由一个中心节点向其他节点发送消 息。当节点数量较多时，通讯成本就很高。
    - **CockroachDB 采用了 P2P 架构，每个节点都要保存完整的元数据**，这样节点规模就非常 大，当然也就不适用广播机制。**而 Gossip 协议的原理是谣言传播机制**，每一次谣言都在几 个人的小范围内传播，但最终会成为众人皆知的谣言。**这种方式达成的数据一致性是 “最终一致性”**，即执行数据更新操作后，经过一定的时间，集群内各个节点所存储的数据最 终会达成一致。
  - **CockroachDB 真的是基于“最终一致性”的元数据实现了强一致性的分布式数据库**。
    - 节点 A 接到客户端的 SQL 请求，要查询数据表 T1 的记录，根据主键范围确定记录可能 在分片 R1 上，而本地元数据显示 R1 存储在节点 B 上。
    - 节点 A 向节点 B 发送请求。很不幸，节点 A 的元数据已经过时，R1 已经重新分配到节 点 C。
    - 此时节点 B 会回复给节点 A 一个非常重要的信息，R1 存储在节点 C。
    - 节点 A 得到该信息后，向节点 C 再次发起查询请求，这次运气很好 R1 确实在节点 C。
    - 节点 A 收到节点 C 返回的 R1。
    - 节点 A 向客户端返回 R1 上的记录，同时会更新本地元数据。
    - **可以看到，CockroachDB 在寻址过程中会不断地更新分片元数据，促成各节点元数据达成 一致。**
  - 看完 TiDB 和 CockroachDB 的设计，我们可以做个小结了。**复制协议的选择和数据副本 数量有很大关系**
    - 如果副本少，参与节点少，可以采用广播方式，也就是 Paxos、Raft 等 协议;
    - 如果副本多，节点多，那就更适合采用 Gossip 协议。
- **Raft** **的性能缺陷**
  - 在**复制效率**上 Raft 会 差一些，主要原因就是 Raft 必须“**顺序投票**”，不允许日志中出现空洞。在我看来，顺序 投票确实是影响 Raft 算法复制效率的一个关键因素。
  - 单个事务的运行情况中一个完整的 Raft 日志复制过程
    - Leader 收到客户端的请求。
    - Leader 将请求内容(即 Log Entry)追加(Append)到本地的 Log。
    - Leader 将 Log Entry 发送给其他的 Follower。
    - Leader 等待 Follower 的结果，如果大多数节点提交了这个 Log，那么这个 Log Entry 就是 Committed Entry，Leader 就可以将它应用(Apply)到本地的状态机。
    - Leader 返回客户端提交成功。
    - Leader 继续处理下一次请求。
  - 多事务并行操作时
    - 我们设定这个 Raft 组由 5 个节点组成，T1 到 T5 是先后发生的 5 个事务操作，被发送到 这个 Raft 组。
    - **事务 T1 的操作是将 X 置为 1，5 个节点都 Append 成功，Leader 节点 Apply 到本地状 态机，并返回客户端提交成功。**事务 T2 执行时，虽然有一个 Follower 没有响应，但仍然 得到了大多数节点的成功响应，所以也返回客户端提交成功。
    - **现在，轮到 T3 事务执行，没有得到超过半数的响应，这时 Leader 必须等待一个明确的失 败信号，比如通讯超时，才能结束这次操作。因为有顺序投票的规则，T3 会阻塞后续事务 的进行。**T4 事务被阻塞是合理的，因为它和 T3 操作的是同一个数据项，但是 T5 要操作 的数据项与 T3 无关，也被阻塞，显然这不是最优的并发控制策略。
    - 同样的情况也会发生在 Follower 节点上，**第一个 Follower 节点可能由于网络原因没有收 到 T2 事务的日志，即使它先收到 T3 的日志，也不会执行 Append 操作**，因为这样会使 日志出现空洞。
  - **Raft 的顺序投票是一种设计上的权衡，虽然性能有些影响，但是节点间日志比对会非常简单。在两个节点上，只要找到一条日志是一致的，那么在这条日志之前的所有日志就都是 一致的。**这使得选举出的 Leader 与 Follower 同步数据非常便捷，开放 Follower 读操作 也更加容易。
- **Raft** **的性能优化方法(TiDB*)**
  - 四个优化点
    - **批操作(Batch)。**Leader 缓存多个客户端请求，然后将这一批日志批量发送给 Follower。Batch 的好处是减少的通讯成本。
    - **流水线(Pipeline)。**Leader 本地增加一个变量(称为 NextIndex)，每次发送一个 Batch 后，更新 NextIndex 记录下一个 Batch 的位置，然后不等待 Follower 返回，马 上发送下一个 Batch。如果网络出现问题，Leader 重新调整 NextIndex，再次发送 Batch。当然，**这个优化策略的前提是网络基本稳定**。
    - **并行追加日志(Append Log Parallelly)。**Leader 将 Batch 发送给 Follower 的同 时，并发执行本地的 Append 操作。因为 Append 是磁盘操作，开销相对较大，而标准流程中 Follower 与 Leader 的 Append 是先后执行的，当然耗时更长。改为并行就 可以减少部分开销。当然，这时 Committed Entry 的判断规则也要调整。在并行操作 下，即使 Leader 没有 Append 成功，只要有半数以上的 Follower 节点 Append 成 功，那就依然可以视为一个 Committed Entry，Entry 可以被 Apply。
    - **异步应用日志(Asynchronous Apply)。**Apply 并不是提交成功的必要条件，任何处 于 Committed 状态的 Log Entry 都确保是不会丢失的。Apply 仅仅是为了保证状态能 够在下次被正确地读取到，但多数情况下，提交的数据不会马上就被读取。因此，Apply 是可以转为异步执行的，同时读操作配合改造。
- etcd，它是最早的、生产级的 Raft 协议开源实现，TiDB 和 CockroachDB 都借鉴了它的设计。甚至可以说，它们选择 Raft 就是因为 etcd 提供了可 靠的工程实现。






