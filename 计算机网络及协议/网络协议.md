## 协议三要素

- 语法
  - 这一段内容要符合一定的规则和格式
- 语义
  - 这一段内容要代表某种意义
- 顺序
  - 先干啥，后干啥





## UDP协议

- TCP和UDP有哪些区别？

  - TCP是面向连接的，UDP是面向无连接的
    - 所谓的建立连接，是为了在客户端和服务端维护连接，而建立一定的数据结构来维护双方交互的状态，用这样的数据结构来保证所谓的面向连接的特性。
  - TCP提供可靠交付,通过TCP连接传输的数据，无差错、不丢失、不重复、并且按序到达。
    - UDP继承了IP的特性，不保证不丢失，不保证按顺序到达。
  - TCP是面向字节流的。
    - 而UDP继承了IP的特性，基于数据报的，一个一个地发，一个一个地收。
  - TCP是可以有拥塞控制的
- MAC层定义了本地局域网的传输行为，IP层定义了整个网络端到端的传输行为
- 当我们看到UDP包头的时候，有源端口号和目标端口号,UDP除了端口号，再没有其他的了
- UDP的三大使用场景

  - 需要资源少，在网络情况比较好的内网，或者对于丢包不敏感的应用
    - DHCP就是基于UDP协议的
  - 不需要一对一沟通，建立连接，而是可以广播的应用
    - UDP的不面向连接的功能，可以使得可以承载广播或者多播的协议。DHCP就是一种广播的形式，就是基于UDP协议的。
  - 需要处理速度快，时延低，可以容忍少数丢包，但是要求即便网络拥塞，也毫不退缩，一往无前的时候。（当前很多应用都是要求低时延的）
- 基于UDP的“城会玩”的例子

  - 网页或者APP的访问

    - 目前的HTTP协议，往往采取多个数据通道共享一个连接的情况，这样本来为了加快传输速度，但是TCP的严格顺序策略使得哪怕共享通道，前一个不来，后一个和前一个即便没关系，也要等着，时延也会加大。
    - QUIC（全称Quick UDP Internet Connections，快速UDP互联网连接）

  - 流媒体的协议

    - 直播

  - 实时游戏

    - 游戏对实时要求较为严格的情况下，采用自定义的可靠UDP协议，自定义重传策略，能够把丢包产生的延迟降到最低，尽量减少网络问题对游戏性造成的影响。

  - IoT物联网

    - 物联网领域终端资源少
    - 物联网对实时性要求也很高
- UDP 常用于以下几个方面： 1.包总量较少的通信（DNS、SNMP等）； 2.视频、音频等多媒体通信（即时通信）； 3.限定于 LAN 等特定网络中的应用通信； 4.广播通信（广播、多播）。
  - 而且以现在的应用场景，UDP协议一般用作短消息的传输，或者对结果的完整度没有太高要求的情况，比如，音频、视频等普通数据，即使丢几个包，对结果的影响也不会太大，但是UDP对网络质量要求较高，尤其是处理大文件数据时，大面积的丢包会使文件直接损坏，根本无法使用 
- **TCP和UDP可以同时监听相同的端口吗**
  - 使用netstat -an自己看看就知道了，IP数据包首部有个叫做协议的字段，指出了上层协议是TCP还是UDP还是其他P。
    协议字段（报头检验和前面那个），其值为6，则为TCP；
    其值为17，则为UDP。
  - Stack overflow:许多协议已经做到了这一点，例如DNS可在udp / 53和tcp / 53上运行。

## TCP协议

- TCP包头格式

  - 源端口号和目标端口号

  - 序号。解决乱序的问题

  - 确认序号。发出去的包应该有确认

  - 状态位。SYN是发起一个连接，ACK是回复，RST是重新连接，FIN是结束连接等

  - 窗口大小。TCP要做流量控制

  - > 顺序问题 ，稳重不乱；
    > 丢包问题，承诺靠谱；
    > 连接维护，有始有终；
    > 流量控制，把握分寸；
    > 拥塞控制，知进知退。

- TCP的三次握手

  - 进行三次握手的主要作用就是为了确认双方的接收能力和发送能力是否正常、指定自己的初始化序列号为后面的可靠性传送做准备。实质上其实就是连接服务器指定端口，建立TCP连接，并同步连接双方的序列号和确认号，交换TCP窗口大小信息。
  - TCP初始序列号为什么是随机的
    - 在TCP的三次握手中，采用随机产生的初始化序列号进行请求，这样做主要是出于网络安全的因素着想。如果不是随机产生初始序列号，黑客将会以很容易的方式获取到你与其他主机之间通信的初始化序列号，并且伪造序列号进行攻击，这已经成为一种很常见的网络攻击手段。
  - 第三次握手的原因
    - 避免已经失效的连接请求报文段占用服务器的连接资源。
    - 假如A发送的连接请求报文段并没有丢失，而是因为在某些网络节点长时间滞留，在收到A发送的这个已失效的请求报文段之后，没有第三次握手的确认
    - 造成一种假象：B认为和A的连接已经建立，一直等待A发送数据，而A认为自己没有发送请求连接，不理睬B的确认。
  - 过程
    - A：SYN=1 seq=x
    - B：SYN=1 ACK=1 seq=y ack=x+1
    - A：ACK=1 seq=x+1 ack=y+1
  - 过程
  - 一开始，客户端和服务端都处于CLOSED状态。先是服务端主动监听某个端口，处于LISTEN状态。
    - 客户端主动发起连接SYN，之后处于SYN-SENT状态。
    - 服务端收到发起的连接，返回SYN,ACK客户端的SYN，之后处于SYN-RCVD状态
    - 客户端收到服务端发送的SYN和ACK之后，发送ACK的ACK，之后处于ESTABLISHED状态，因为它一发一收成功了
    - 服务端收到ACK的ACK之后，处于ESTABLISHED状态，因为它也一发一收了。

- 状态

  > LISTEN - 侦听来自远方TCP端口的连接请求； 
  > SYN-SENT -在发送连接请求后等待匹配的连接请求； 
  > SYN-RECEIVED - 在收到和发送一个连接请求后等待对连接请求的确认； 
  > ESTABLISHED- 代表一个打开的连接，数据可以传送给用户； 
  > FIN-WAIT-1 - 等待远程TCP的连接中断请求，或先前的连接中断请求的确认；
  > FIN-WAIT-2 - 从远程TCP等待连接中断请求；
  > CLOSE-WAIT - 等待从本地用户发来的连接中断请求； 
  > CLOSING -等待远程TCP对连接中断的确认； 
  > LAST-ACK - 等待原来发向远程TCP的连接中断请求的确认； 
  > TIME-WAIT -等待足够的时间以确保远程TCP接收到连接中断请求的确认； 
  > CLOSED - 没有任何连接状态；

  

- TCP四次挥手

  - 因为当服务端收到客户端的SYN连接请求报文后，可以直接发送SYN+ACK报文。其中ACK报文是用来应答的，SYN报文是用来同步的。但是关闭连接时，当服务端收到FIN报文时，很可能并不会立即关闭SOCKET，所以只能先回复一个ACK报文，告诉客户端，“你发的FIN报文我收到了”。只有等到我服务端所有的报文都发送完了，我才能发送FIN报文，因此不能一起发送。故需要四次挥手。

  - 过程

    - A：FIN=1 seq=u
    - B：ACK=1 seq=v ack=u+1
    - B：FIN=1 ACK=1 seq=w ack=u+1
    - A：ACK=1 seq=u+1 ack=w+1

  - 过程

    - 刚开始双方都处于 establised 状态，假如是客户端先发起关闭请求，则：

      1、第一次挥手：客户端发送一个 FIN 报文，报文中会指定一个序列号。此时客户端处于**CLOSED_WAIT1**状态。

      2、第二次握手：服务端收到 FIN 之后，会发送 ACK 报文，且把客户端的序列号值 + 1 作为 ACK 报文的序列号值，表明已经收到客户端的报文了，此时服务端处于 **CLOSE_WAIT2**状态。

      3、第三次挥手：如果服务端也想断开连接了，和客户端的第一次挥手一样，发给 FIN 报文，且指定一个序列号。此时服务端处于 **LAST_ACK** 的状态。

      4、第四次挥手：客户端收到 FIN 之后，一样发送一个 ACK 报文作为应答，且把服务端的序列号值 + 1 作为自己 ACK 报文的序列号值，此时客户端处于 **TIME_WAIT** 状态。需要过一阵子以确保服务端收到自己的 ACK 报文之后才会进入 CLOSED 状态

      5、服务端收到 ACK 报文之后，就处于关闭连接了，处于 CLOSED 状态。

    - 按说A可以跑路了，但是最后的这个ACK万一B收不到呢？

      - 则B会重新发一个“B不玩了”，这个时候A已经跑路了的话，B就再也收不到ACK了，因而TCP协议要求A最后等待一段时间TIME_WAIT
      - 这个时间要足够长，长到如果B没收到ACK的话，“B说不玩了”会重发的，A会重新发一个ACK并且足够时间到达B。
      - A直接跑路还有一个问题是，A的端口就直接空出来了，但是B不知道，B原来发过的很多包很可能还在路上，如果A的端口被一个新的应用占用了，这个新的应用会收到上个连接中B发过来的包
      - B超过了2MSL的时间，依然没有收到它发的FIN的ACK，怎么办呢？按照TCP的原理，B当然还会重发FIN，这个时候A再收到这个包之后，A就表示，我已经在这里等了这么长时间了，已经仁至义尽了，之后的我就都不认了，于是就直接发送RST，B就知道A早就跑了。
        - RFC 793 中虽然指出了 TCP 连接需要在 `TIME_WAIT` 中**等待 2 倍的 MSL**，但是并没有解释清楚这里的两倍是从何而来，比较合理的解释是 — 网络中可能存在来自发起方的数据段，当这些发起方的数据段被服务端处理后又会向客户端发送响应，所以一来一回需要等待 2 倍的时间[5](https://draveness.me/whys-the-design-tcp-time-wait/#fn:5)。
        - **RFC 793 文档将 MSL 的时间设置为 120 秒，即两分钟**，然而这并不是一个经过严密推断的数值，而是工程上的选择，如果根据服务历史上的经验要求我们改变操作系统的设置，也是没有任何问题的；实际上，较早版本的 Linux 就开始将 `TIME_WAIT` 的等待时间 [`TCP_TIMEWAIT_LEN`](https://github.com/torvalds/linux/blob/bd2463ac7d7ec51d432f23bf0e893fb371a908cd/include/net/tcp.h#L121) 设置成 60 秒，以便更快地复用 TCP 连接资源

  - **TCP 的 `TIME_WAIT` 状态有着非常重要的作用，它是保证 TCP 协议可靠性不可缺失的设计**

  - **`TIME_WAIT` 只在主动断开连接的一方出现**，被动断开连接的一方会直接进入 `CLOSED` 状态，进入 `TIME_WAIT` 的客户端需要等待 2 MSL 才可以真正关闭连接。**TCP 协议需要 `TIME_WAIT` 状态的原因和客户端需要等待两个 MSL 不能直接进入 `CLOSED` 状态的原因是一样的**：

    - 防止延迟的数据段被其他使用相同源地址、源端口、目的地址以及目的端口的 TCP 连接收到；

    - 保证 TCP 连接的远程被正确关闭，即等待被动关闭连接的一方收到 `FIN` 对应的 `ACK` 消息；

      - 从 RFC 793 对 `TIME_WAIT` 状态的定义中，我们可以发现该状态的另一个重要作用，等待足够长的时间以确定远程的 TCP 连接接收到了其发出的终止连接消息 `FIN` 对应的 `ACK`
      - 如果客户端等待的时间不够长，当服务端还没有收到 `ACK` 消息时，客户端就重新与服务端建立 TCP 连接就会造成以下问题 — 服务端因为没有收到 `ACK` 消息，所以仍然认为当前连接是合法的，客户端重新发送 `SYN` 消息请求握手时会收到服务端的 `RST` 消息，连接建立的过程就会被终止

    - **TIME-WAIT 较短导致的握手终止**

      - 在默认情况下，**如果客户端等待足够长的时间就会遇到以下两种情况**：

      1. 服务端正常收到了 `ACK` 消息并关闭当前 TCP 连接；
      2. 服务端没有收到 `ACK` 消息，重新发送 `FIN` 关闭连接并等待新的 `ACK` 消息；

      - **只要客户端等待 2 MSL 的时间，客户端和服务端之间的连接就会正常关闭，新创建的 TCP 连接收到影响的概率也微乎其微，保证了数据传输的可靠性。**
      - **TCP 的 `TIME_WAIT` 状态有着非常重要的作用，它是保证 TCP 协议可靠性不可缺失的设计**，如果能通过加机器解决的话就尽量加机器，如果不能解决的话，我们就需要理解其背后的设计原理并尽可能避免修改默认的配置，就像 Linux 手册中说的一样，在修改这些配置时应该咨询技术专家的建议；在这里，我们再重新回顾一下 TCP 协议中 `TIME_WAIT` 状态存在的原因，**如果客户端等待的时间不够长，那么使用相同端口号重新与远程建立连接时会造成以下问题**：
        - 因为数据段的网络传输时间不确定，所以可能会收到上一次 TCP 连接中未被收到的数据段；
        - 因为客户端发出的 `ACK` 可能还没有被服务端接收，服务端可能还处于 `LAST_ACK` 状态，所以它会回复 `RST` 消息终止新连接的建立；

- 顺序问题和丢包问题都有可能发生，所以我们先来看确认与重发的机制。

  - 时间必须大于往返时间RTT，否则会引起不必要的重传
    - 超时间隔加倍。每当遇到一次超时重传的时候，都会将下一次超时时间间隔设为先前值的两倍。两次超时，就说明网络环境差，不宜频繁反复发送
  - 超时触发重传存在的问题是，超时周期可能相对较长。那是不是可以有更快的方式呢？
    - 有一个可以快速重传的机制，当接收方收到一个序号大于下一个所期望的报文段时，就检测到了数据流中的一个间格，于是发送三个冗余的ACK，客户端收到后，就在定时器过期之前，重传丢失的报文段。
    - 例如，接收方发现6、8、9都已经接收了，就是7没来，那肯定是丢了，于是发送三个6的ACK，要求下一个是7。客户端收到3个，就会发现7的确又丢了，不等超时，马上重发。
  - 还有一种方式称为Selective Acknowledgment （SACK）。这种方式需要在TCP头里加一个SACK的东西，可以将缓存的地图发送给发送方。例如可以发送ACK6、SACK8、SACK9，有了地图，发送方一下子就能看出来是7丢了

- tcp如何实现可靠性传输

  - 确认机制、重传机制、滑动窗口

- 流量控制问题

  - 滑动窗口
  - 发送方会定时发送窗口探测数据包，看是否有机会调整窗口的大小。当接收方比较慢的时候，要防止低能窗口综合征，别空出一个字节来就赶快告诉发送方，然后马上又填满了，可以当窗口太小的时候，不更新窗口，直到达到一定大小，或者缓冲区一半为空，才更新窗口。

- 拥塞控制问题

  - 拥塞控制的问题，也是通过窗口的大小来控制的
  - 滑动窗口rwnd是怕发送方把接收方缓存塞满，而拥塞窗口cwnd，是怕把网络塞满。
    - TCP发送包常被比喻为往一个水管里面灌水，而TCP的拥塞控制就是在不堵塞，不丢包的情况下，尽量发挥带宽。
  - TCP的拥塞控制主要来避免两种现象，包丢失和超时重传
    - 一旦出现了这些现象就说明，发送速度太快了
  - 慢启动、指数增长、超过sshresh后线性增长、拥塞后这个时候，将sshresh设为cwnd/2，将cwnd设为1，重新开始慢启动。
    - 快速重传算法：cwnd减半为cwnd/2，然后sshthresh = cwnd，线性增长
  - TCP的拥塞控制主要来避免的两个现象都是有问题的。
    - 第一个问题是丢包并不代表着通道满了，也可能是管子本来就漏水
    - 第二个问题是TCP的拥塞控制要等到将中间设备都填充满了，才发生丢包，从而降低速度，这时候已经晚了。其实TCP只要填满管道就可以了，不应该接着填，直到连缓存也填满。

- TCP BBR拥塞算法

  - 企图找到一个平衡点，就是通过不断的加快发送速度，将管道填满，但是不要填满中间设备的缓存，因为这样时延会增加，在这个平衡点可以很好的达到高带宽和低时延的平衡。
  - 设备缓存会导致延时
    - 假如经过设备的包都不需要进入缓存，那么得到的速度是最快的。进入缓存且等待，等待的时间就是额外的延时。BBR就是为了避免这些问题。
    - 充分利用带宽；降低buffer占用率。
  - 快速下降后重新慢启动，整个过程对于带宽来说是浪费的。
  - BBR如何解决延时
    - S1：慢启动开始时，以前期的延迟时间为延迟最小值Tmin。然后监控延迟值是否达到Tmin的n倍
      - 达到这个阀值后，判断带宽已经消耗尽并且使用了一定的缓存，进入排空阶段。
    - S2：指数降低发送速率，直至延迟不再降低
    - S3：协议进入稳定运行状态。交替探测带宽和延迟，且大多数时间下都处于带宽探测阶段。
  - BBR是如何探测最大带宽和最小延时呢？首先有一点就是最大带宽和最小延时是无法同时得到的。
    - 探测最大带宽的方法就是尽量多发数据，把网络中的buffer占满，带宽在一段时间内不会增加，这样可以得到此时的最大带宽。
    - 探测最小RTT的方法就是尽量把buffer腾空，让数据交付延时尽量低。
    - 由此，BBR就引入了基于不同探测阶段的状态机。

- **顺序问题、丢包问题、流量控制都是通过滑动窗口来解决的**

- **拥塞控制是通过拥塞窗口来解决的，**相当于往管道里面倒水，快了容易溢出，慢了浪费带宽，要摸着石头过河，找到最优值。

- 应用场景

  - ![img](https://pic4.zhimg.com/v2-06bdd52997add27938607b33edea4068_b.jpg)

- TCP 适合金融等大多数领域，UDP适合游戏和娱乐场景

  当对网络通讯质量有要求的时候，比如：整个数据要准确无误的传递给对方，这往往用于一些要求可靠的应用，比如HTTP、HTTPS、FTP等传输文件的协议，POP、SMTP等邮件传输的协议。当对网络通讯质量要求不高的时候，要求网络通讯速度能尽量的快，这时就可以使用UDP（如视频传输、实时通信等）。


- **如果服务端的机器挂了，而客户端去连接会发生什么？如果是服务端的进程挂了，客户端去连接会发生什么？**

  - 如果是机器挂了，说明操作系统也没了，那么客户端在建立连接的时候，只会发送第一次握手的包，当然收不到，然后重试几次，就不试了。
  - 如果是进程挂了，那么操作系统还在，操作系统会代替进程向客户端发送一个RESET的包，此时客户端就知道通信的那个进程没了，直接关闭连接。

- 长连接与短连接

  **TCP 本身并没有长短连接的区别** ，长短与否，完全取决于我们怎么用它。

  - 短连接：每次通信时，创建 Socket；一次通信结束，调用 socket.close()。这就是一般意义上的短连接，短连接的好处是管理起来比较简单，存在的连接都是可用的连接，不需要额外的控制手段。
  - 长连接：每次通信完毕后，不会关闭连接，这样可以做到连接的复用。 **长连接的好处是省去了创建连接的耗时。**

  短连接和长连接的优势，分别是对方的劣势。想要图简单，不追求高性能，使用短连接合适，这样我们就不需要操心连接状态的管理；想要追求性能，使用长连接，我们就需要担心各种问题：比如 **端对端连接的维护，连接的保活** 。

  长连接还常常被用来做数据的推送，我们大多数时候对通信的认知还是 request/response 模型，但 TCP 双工通信的性质决定了它还可以被用来做双向通信。在长连接之下，可以很方便的实现 push 模型。

- 连接的保活

  这个话题就有的聊了，会牵扯到比较多的知识点。首先需要明确一点，为什么需要连接的保活？当双方已经建立了连接，但因为网络问题，链路不通，这样长连接就不能使用了。需要明确的一点是，通过 netstat，lsof 等指令查看到连接的状态处于 `ESTABLISHED` 状态并不是一件非常靠谱的事，因为连接可能已死，但没有被系统感知到，更不用提假死这种疑难杂症了。如果保证长连接可用是一件技术活。


   - 连接的保活：KeepAlive

     首先想到的是 TCP 中的 KeepAlive 机制。KeepAlive 并不是 TCP 协议的一部分，但是大多数操作系统都实现了这个机制（所以需要在操作系统层面设置 KeepAlive 的相关参数）。KeepAlive 机制开启后，在一定时间内（一般时间为 7200s，参数 `tcp_keepalive_time`）在链路上没有数据传送的情况下，TCP 层将发送相应的 KeepAlive 探针以确定连接可用性，探测失败后重试 10（参数 `tcp_keepalive_probes`）次，每次间隔时间 75s（参数 `tcp_keepalive_intvl`），所有探测失败后，才认为当前连接已经不可用。

     在 Netty 中开启 KeepAlive：

     ```
     bootstrap.option(ChannelOption.SO_KEEPALIVE, true)
     ```

     Linux 操作系统中设置 KeepAlive 相关参数，修改 `/etc/sysctl.conf` 文件：

     ```
     net.ipv4.tcp_keepalive_time=90
     net.ipv4.tcp_keepalive_intvl=15
     net.ipv4.tcp_keepalive_probes=2
     ```

     **KeepAlive 机制是在网络层面保证了连接的可用性** ，但站在应用框架层面我们认为这还不够。主要体现在三个方面：

     - KeepAlive 的开关是在应用层开启的，但是具体参数（如重试测试，重试间隔时间）的设置却是操作系统级别的，位于操作系统的 `/etc/sysctl.conf` 配置中，这对于应用来说不够灵活。
     - KeepAlive 的保活机制只在链路空闲的情况下才会起到作用，假如此时有数据发送，且物理链路已经不通，操作系统这边的链路状态还是 `ESTABLISHED`，这时会发生什么？自然会走 TCP 重传机制，要知道默认的 TCP 超时重传，指数退避算法也是一个相当长的过程。
     - KeepAlive 本身是面向网络的，并不面向于应用，当连接不可用，可能是由于应用本身的 GC 频繁，系统 load 高等情况，但网络仍然是通的，此时，应用已经失去了活性，连接应该被认为是不可用的。

     我们已经为应用层面的连接保活做了足够的铺垫，下面就来一起看看，怎么在应用层做连接保活。

   - 连接的保活：应用层心跳

     终于点题了，文题中提到的 **心跳** 便是一个本文想要重点强调的另一个重要的知识点。上一节我们已经解释过了，网络层面的 KeepAlive 不足以支撑应用级别的连接可用性，本节就来聊聊应用层的心跳机制是实现连接保活的。

     如何理解应用层的心跳？简单来说，就是客户端会开启一个定时任务，定时对已经建立连接的对端应用发送请求（这里的请求是特殊的心跳请求），服务端则需要特殊处理该请求，返回响应。如果心跳持续多次没有收到响应，客户端会认为连接不可用，主动断开连接。不同的服务治理框架对心跳，建连，断连，拉黑的机制有不同的策略，但大多数的服务治理框架都会在应用层做心跳，Dubbo/HSF 也不例外。

   - 应用层心跳的设计细节

     以 Dubbo 为例，支持应用层的心跳，客户端和服务端都会开启一个 `HeartBeatTask`，客户端在 `HeaderExchangeClient` 中开启，服务端将在 `HeaderExchangeServer` 开启。文章开头埋了一个坑：Dubbo 为什么在服务端同时维护 `Map<String,Channel>` 呢？主要就是为了给心跳做贡献，心跳定时任务在发现连接不可用时，会根据当前是客户端还是服务端走不同的分支，客户端发现不可用，是重连；服务端发现不可用，是直接 close。

   - 注意和 HTTP 的 KeepAlive 区别对待

     - HTTP 协议的 KeepAlive 意图在于连接复用，同一个连接上串行方式传递请求 - 响应数据
     - TCP 的 KeepAlive 机制意图在于保活、心跳，检测连接错误。

   


## 套接字 Socket

- 基于TCP协议的Socket程序函数调用过程
  - TCP的服务端要先监听一个端口，一般是先调用bind函数
  - 在内核中，为每个Socket维护两个队列。一个是已经建立了连接的队列，这时候连接三次握手已经完毕，处于established状态；一个是还没有完全建立连接的队列，这个时候三次握手还没完成，处于syn_rcvd的状态。
  - 当服务端有了IP和端口号，就可以调用listen函数进行监听
  - 接下来，服务端调用accept函数，拿出一个已经完成的连接进行处理。如果还没有完成，就要等着。
  - 在服务端等待的时候，客户端可以通过connect函数发起连接，内核会给客户端分配一个临时的端口。一旦握手成功，服务端的accept就会返回另一个Socket。
    - 监听的 Socket 和真正用来传数据的 Socket 是两个，一个叫作监听 Socket，一个叫作已连接 Socket。
  - 连接建立成功之后，双方开始通过read和write函数来读写数据
  - ![img](https://static001.geekbang.org/resource/image/77/92/77d5eeb659d5347874bda5e8f711f692.jpg)
  - TCP 的 Socket 就是一个文件流，是非常准确的。因为，Socket 在 Linux 中就是以文件的形式存在的。除此之外，还存在文件描述符。写入和读出，也是通过文件描述符。
    - 在内核中，Socket 是一个文件，那对应就有文件描述符。每一个进程都有一个数据结构 task_struct，里面指向一个文件描述符数组，来列出这个进程打开的所有文件的文件描述符。文件描述符是一个整数，是这个数组的下标。
    - 这个数组中的内容是一个指针，指向内核中所有打开的文件的列表。既然是一个文件，就会有一个 inode，只不过 Socket 对应的 inode 不像真正的文件系统一样，保存在硬盘上的，而是在内存中的。在这个 inode 中，指向了 Socket 在内核中的 Socket 结构。
    - 在这个结构里面，主要的是两个队列，一个是发送队列，一个是接收队列。在这两个队列里面保存的是一个缓存 sk_buff。这个缓存里面能够看到完整的包的结构
- 基于UDP协议的Socket程序函数调用过程
  - UDP是没有连接的，所以不需要三次握手，也就不需要调用listen和connect，但是，UDP的的交互仍然需要IP和端口号，因而也需要bind。
  - UDP是没有维护连接状态的，因而不需要每对连接建立一组Socket，而是只要有一个Socket，就能够和多个客户端通信
  - 每次通信的时候，都调用sendto和recvfrom，都可以传入IP地址和端口。
  - ![img](https://static001.geekbang.org/resource/image/77/ef/778687d1a02ffc0c24078c33be2ac1ef.jpg)
- 手写socket
  - http://www.52im.net/thread-1722-1-1.html
- 服务端最大并发 TCP 连接数远不能达到理论上限。首先主要是文件描述符限制，按照上面的原理，Socket 都是文件，所以首先要通过 ulimit 配置文件描述符的数目；另一个限制是内存，按上面的数据结构，每个 TCP 连接都要占用一定内存，操作系统是有限的。
  - 多进程方式
    - 这就相当于你是一个代理，在那里监听来的请求。一旦建立了一个连接，就会有一个已连接 Socket，这时候你可以创建一个子进程，然后将基于已连接 Socket 的交互交给这个新的子进程来做。
    - 在 Linux 下，创建子进程使用 fork 函数。通过名字可以看出，这是在父进程的基础上完全拷贝一个子进程。在 Linux 内核中，会复制文件描述符的列表，也会复制内存空间，还会复制一条记录当前执行到了哪一行程序的进程。显然，复制的时候在调用 fork，复制完毕之后，父进程和子进程都会记录当前刚刚执行完 fork。这两个进程刚复制完的时候，几乎一模一样，只是根据 fork 的返回值来区分到底是父进程，还是子进程。如果返回值是 0，则是子进程；如果返回值是其他的整数，就是父进程。
    - 因为复制了文件描述符列表，而文件描述符都是指向整个内核统一的打开文件列表的，因而父进程刚才因为 accept 创建的已连接 Socket 也是一个文件描述符，同样也会被子进程获得。
    - 接下来，子进程就可以通过这个已连接 Socket 和客户端进行互通了，当通信完毕之后，就可以退出进程，那父进程如何知道子进程干完了项目，要退出呢？还记得 fork 返回的时候，如果是整数就是父进程吗？这个整数就是子进程的 ID，父进程可以通过这个 ID 查看子进程是否完成项目，是否需要退出。
  - 多线程方式
    - 在 Linux 下，通过 pthread_create 创建一个线程，也是调用 do_fork。不同的是，虽然新的线程在 task 列表会新创建一项，但是很多资源，例如文件描述符列表、进程空间，还是共享的，只不过多了一个引用而已。
    - 新的线程也可以通过已连接 Socket 处理请求，从而达到并发处理的目的。上面基于进程或者线程模型的，其实还是有问题的。新到来一个 TCP 连接，就需要分配一个进程或者线程。一台机器无法创建很多进程或者线程。
    - 有个 C10K，它的意思是一台机器要维护 1 万个连接，就要创建 1 万个进程或者线程，那么操作系统是无法承受的。如果维持 1 亿用户在线需要 10 万台服务器，成本也太高了。
  - IO 多路复用：一个线程维护多个 Socket
    - 由于 Socket 是文件描述符，因而某个线程盯的所有的 Socket，都放在一个文件描述符集合 fd_set 中，这就是项目进度墙，然后调用 select 函数来监听文件描述符集合是否有变化。一旦有变化，就会依次查看每个文件描述符。那些发生变化的文件描述符在 fd_set 对应的位都设为 1，表示 Socket 可读或者可写，从而可以进行读写操作，然后再调用 select，接着盯着下一轮的变化。
  - IO 多路复用：从“派人盯着”到“有事通知“
    - 上面 select 函数还是有问题的，因为每次 Socket 所在的文件描述符集合中有 Socket 发生变化的时候，都需要通过轮询的方式，也就是需要将全部项目都过一遍的方式来查看进度，这大大影响了一个项目组能够支撑的最大的项目数量。因而使用 select，能够同时盯的项目数量由 FD_SETSIZE 限制。
    - 如果改成事件通知的方式，情况就会好很多，项目组不需要通过轮询挨个盯着这些项目，而是当项目进度发生变化的时候，主动通知项目组，然后项目组再根据项目进展情况做相应的操作。能完成这件事情的函数叫 epoll，它在内核中的实现不是通过轮询的方式，而是通过注册 callback 函数的方式，当某个文件描述符发送变化的时候，就会主动通知。
    - 假设进程打开了 Socket m, n, x 等多个文件描述符，现在需要通过 epoll 来监听是否这些 Socket 都有事件发生。其中 epoll_create 创建一个 epoll 对象，也是一个文件，也对应一个文件描述符，同样也对应着打开文件列表中的一项。在这项里面有一个红黑树，在红黑树里，要保存这个 epoll 要监听的所有 Socket。
    - 当 epoll_ctl 添加一个 Socket 的时候，其实是加入这个红黑树，同时红黑树里面的节点指向一个结构，将这个结构挂在被监听的 Socket 的事件列表中。当一个 Socket 来了一个事件的时候，可以从这个列表中得到 epoll 对象，并调用 call back 通知它。
    - 这种通知方式使得监听的 Socket 数据增加的时候，效率不会大幅度降低，能够同时监听的 Socket 的数目也非常的多了。上限就为系统定义的、进程打开的最大文件描述符个数。因而，epoll 被称为解决 C10K 问题的利器。









## MTU1500

- 最大传输单元（ Maximum Transmission Unit）

- MTU是二层MAC层的概念。MAC层有MAC的头，以太网规定MAC头带正文合起来，不允许超过1500个字节。MTU指的是IP头+IP数据部分长度

- MTU 大小是不包含二层头部和尾部的，MTU 1500表示二层MAC帧大小不超过1518. MAC 头14 字节，尾4字节。可以抓包验证

- 为什么标准以太网帧长度上限为1518字节

  - IP头total length为两个byte，理论上IP packet可以有65535 byte，加上Ethernet Frame头和尾，可以有65535 +14 + 4 = 65553 byte。如果在10Mbps以太网上，将会占用共享链路长达50ms，这将严重影响其它主机的通信，特别是对延迟敏感的应用是无法接受的。
  - 由于线路质量差而引起的丢包，发生在大包的概率也比小包概率大得多，所以大包在丢包率较高的线路上不是一个好的选择。
  - 但是如果选择一个比较小的长度，传输效率又不高，拿TCP应用来说，如果选择以太网长度为218byte，TCP payload = 218 - Ethernet Header - IP Header - TCP Header = 218 - 18 - 20 - 20 = 160 byte
  - 那有效传输效率= 160 / 218 = 73%
  - 而如果以太网长度为1518，那有效传输效率= 1460 / 1518 = 96%
  - 通过比较，选择较大的帧长度，有效传输效率更高，而更大的帧长度同时也会造成上述的问题，于是最终选择一个折衷的长度：1518 byte ! 对应的IP packet 就是 1500 byte，这就是最大传输单元MTU的由来。

  



## 无类型域间选路（CIDR）

- 32位的IP地址一分为二，前面是网络号，后面是主机号

  - > 10.100.122.2/24
    > 	后面24的意思是，32位中，前24位是网络号，后8位是主机号。





## 动态主机配置协议（DHCP）

- 他只需要配置一段共享的IP地址。每一台新接入的机器都通DHCP协议，来这个共享的IP地址里申请，然后自动配置好就可以了。
  - 如果是数据中心里面的服务器，IP一旦配置好，基本不会变，这就相当于买房自己装修。DHCP的方式就相当于租房。你不用装修，都是帮你配置好的。你暂时用一下，用完退租就可以了。





## 预启动执行环境（PXE）

- 网络管理员不仅能自动分配IP地址，还能帮你自动安装操作系统！
- 所以管理员希望的不仅仅是自动分配IP地址，还要自动安装系统。装好系统之后自动分配IP地址，直接启动就能用了，这样当然最好了！
  - 首先，启动BIOS。这是一个特别小的小系统，只能干特别小的一件事情。其实就是读取硬盘的MBR启动扇区，将GRUB启动起来；然后将权力交给GRUB，GRUB加载内核、加载作为根文件系统的initramfs文件；然后将权力交给内核；最后内核启动，初始化整个操作系统。





## 数据链路层

- ARP协议，也就是已知IP地址，求MAC地址的协议。
- 交换机怎么知道每个口的电脑的MAC地址呢？这需要交换机会学习。
  - 一台MAC1电脑将一个包发送给另一台MAC2电脑，当这个包到达交换机的时候，一开始交换机也不知道MAC2的电脑在哪个口，所以没办法，它只能将包转发给除了来的那个口之外的其他所有的口。
  - 交换机会记住，MAC1是来自一个明确的口。以后有包的目的地址是MAC1的，直接发送到这个口就可以了。
  - 当交换机作为一个关卡一样，过了一段时间之后，就有了整个网络的一个结构了，这个时候，基本上不用广播了，全部可以准确转发。当然，每个机器的IP地址会变，所在的口也会变，因而交换机上的学习的结果，我们称为转发表，是有一个过期时间的。
- 广播风暴
  - ARP广播时，交换机会将一个端口收到的包转发到其它所有的端口上
  - 比如数据包经过交换机A到达交换机B，交换机B又将包复制为多份广播出去。如果整个局域网存在一个环路，使得数据包又重新回到了最开始的交换机A，这个包又会被A再次复制多份广播出去。如此循环，数据包会不停得转发，而且越来越多，最终占满带宽，或者使解析协议的硬件过载，行成广播风暴。





## 交换机与 VLAN

- STP协议

  - 最小生成树，计算机网络中，生成树的算法叫作STP

  - Priority Vector，优先级向量。可以比喻为实力 （值越小越牛）

    - > [Root Bridge ID, Root Path Cost, Bridge ID, and Port ID]
      >
      > ​	老大的ID
      > ​	我距离我的老大的距离
      > ​	我自己的ID

- 如何解决广播问题和安全问题？

  - VLAN，或者叫虚拟局域网
    - 使用VLAN，一个交换机上会连属于多个局域网的机器，那交换机怎么区分哪个机器属于哪个局域网呢？
    - 我们只需要在原来的二层的头上加一个TAG，里面有一个VLAN ID，一共12位。为什么是12位呢？因为12位可以划分4096个VLAN。
    - 只有相同VLAN的包，才会互相转发，不同VLAN的包，是看不到的。这样广播问题和安全问题就都能够解决了。
  - 交换机之间怎么连接呢
    - 对于支持VLAN的交换机，有一种口叫作Trunk口。可以允许多个VLAN通过,可以接收和发送多个VLAN 报文。交换机之间可以通过这种口相互连接。

  





## ICMP与ping

- ICMP协议的格式
  - ping是基于ICMP协议工作的。ICMP，就是互联网控制报文协议。这里面的关键词是“控制”，那具体是怎么控制的呢？
  - ICMP报文是封装在IP包里面的。
- 查询报文类型
  - 常用的ping就是查询报文，是一种主动请求，并且获得主动应答的ICMP协议。
- 差错报文类型
  - 举几个ICMP差错报文的例子：终点不可达为3，源抑制为4，超时为11，重定向为5







## 网关

- 在任何一台机器上，当要访问另一个IP地址的时候，都会先判断，这个目标IP地址，和当前机器的IP地址，是否在同一个网段。怎么判断同一个网段呢？需要CIDR和子网掩码
  - 如果不是同一网段，这就需要发往默认网关Gateway
  - 网关往往是一个路由器，是一个三层转发的设备
    - 把MAC头和IP都取下来，然后根据里面的内容，看看接下来把包往哪里转发的设备。
    - 很多情况下，人们把网关就叫作路由器。其实不完全准确，而另一种比喻更加恰当：路由器是一台设备，它有五个网口或者网卡，相当于有五只手，分别连着五个局域网。每只手的IP地址都和局域网的IP地址相同的网段，每只手都是它握住的那个局域网的网关。
    - 任何一个想发往其他局域网的包，都会到达其中一只手，被拿进来，拿下MAC头和IP头，看看，根据自己的路由算法，选择另一只手，加上IP头和MAC头，然后扔出去。
- 静态路由，其实就是在路由器上，配置一条一条规则。
- IP头和MAC头哪些变、哪些不变？
  - MAC地址是一个局域网内才有效的地址，MAC地址只要过网关，就必定会改变，因为已经换了局域网。两者主要的区别在于IP地址是否改变。不改变IP地址的网关，我们称为转发网关；改变IP地址的网关，我们称为NAT网关
  - 现在大家每家都有家用路由器，家里的网段都是192.168.1.x，所以你肯定访问不了你邻居家的这个私网的IP地址的。所以，当我们家里的包发出去的时候，都被家用路由器NAT成为了运营商的地址了。







## 路由协议

- 如何配置路由

  - 路由器就是一台网络设备，它有多张网卡。当一个入口的网络包送到路由器时，它会根据一个本地的转发信息库，来决定如何正确地转发流量。这个转发信息库通常被称为路由表。

  - > 一张路由表中会有多条路由规则。每一条规则至少包含这三项信息
    > 	目的网络
    > 	出口设备
    > 	下一跳网关
    >
    > 
    >
    > 通过route命令和ip route命令都可以进行查询或者配置
    > 	ip route add 10.176.48.0/20 via 10.173.32.1 dev eth0
    > 	10.176.48.0/20这个目标网络，要从eth0端口出去，经过10.173.32.1。

- 如何配置策略路由

  - 除了可以根据目的ip地址配置路由外，还可以根据多个参数来配置路由，这就称为策略路由。

    - > ip rule add from 192.168.1.0/24 table 10 
      >
      > ip rule add from 192.168.2.0/24 table 20
      >
      > 表示从192.168.1.10/24这个网段来的，使用table 10中的路由表
      >
      > 而从192.168.2.0/24网段来的，使table20的路由表。
      >
      > 
      >
      > ip route add default via 60.190.27.189 dev eth3 table chao

    - > 在一条路由规则中，也可以走多条路径
      >
      > ip route add default scope global nexthop via 100.100.100.1 weight 1 nexthop via 200.200.200.1 weight 2
      >
      > 下一跳有两个地方，分别是100.100.100.1和200.200.200.1，权重分别为1比2。

- 动态路由算法

  - 距离矢量路由算法
    - 这种算法的基本思路是，每个路由器都保存一个路由表，包含多行，每行对应网络中的一个路由器，每一行包含两部分信息，一个是要到目标路由器，从那条线出去，另一个是到目标路由器的距离。
    - 每个路由器都知道自己和邻居之间的距离，每过几秒，每个路由器都将自己所知的到达所有的路由器的距离告知邻居，每个路由器也能从邻居那里得到相似的信息。
    - 第一个问题就是好消息传得快，坏消息传得慢。
    - 第二个问题是，每次发送的时候，要发送整个全局路由表
  - 链路状态路由算法
    - 当一个路由器启动的时候，首先是发现邻居，向邻居say hello，邻居都回复。然后计算和邻居的距离，发送一个echo，要求马上返回，除以二就是距离。然后将自己和邻居之间的链路状态包广播出去，发送到整个网络的每个路由器。这样每个路由器都能够收到它和邻居之间的关系的信息。因而，每个路由器都能在自己本地构建一个完整的图，然后针对这个图使用Dijkstra算法，找到两点之间的最短路径。
    - 不像距离距离矢量路由协议那样，更新时发送整个路由表。链路状态路由协议只广播更新的或改变的网络拓扑，这使得更新信息更小，节省了带宽和CPU利用率。而且一旦一个路由器挂了，它的邻居都会广播这个消息，可以使得坏消息迅速收敛。

- 动态路由协议

  - 基于距离矢量路由算法的BGP
    - 我们称为外网路由协议（BGP）
    - 每个自治系统都有边界路由器，通过它和外面的世界建立联系。
  - 基于链路状态路由算法的OSPF(开放式最短路径优先)
    - 主要用在数据中心内部，用于路由决策，因而称为内部网关协议（IGP）。
    - 有了等价路由，到一个地方去可以有相同的两个路线，可以分摊流量，还可以当一条路不通的时候，走另外一条路
      - 一般应用的接入层会有负载均衡LVS。它可以和OSPF一起，实现高吞吐量的接入层设计。
  - BGP基于TCP，OSPF基于UDP 









## ARP

（ARP地址解析协议,  RARP反向地址解析协议），我们习惯上把它们认为是链路层的协议，实际上，从分层的角度来看，更准确的说是，（应该是一种介于网络IP层与链路层之间的一种协议）

我们知道在ISO/OSI模型中，数据在传输的过程中，有不断封装过程，到了链路层的话(以太网传输)，在以太网的帧格式中会出现目的主机的MAC地址，但是我们从一开始就只知道目的主机的IP地址，所以这里用到了ARP协议

源主机先在自己的ARP缓冲区中寻找映射，如果有（直接填充于以太网帧中），如果没有，通过路由广播请求，这时一些联网的主机就会收到这个请求，并将这个请求传回网络层，对比IP地址，检验是否可以接受，如果不行，则直接丢失这个信息，如果可以那么回复ARP请求，并且将源主机的MAC地址加入到目的ARP缓冲区中，形成映射，源主机接受到请求后，将目的的MAC地址加入到ARP缓冲区，也形成映射，并将mac地址传输至连接层。此时转化完成

![img](https://img-blog.csdn.net/20160219211855357?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)

**什么是ARP欺骗**

举个栗子：

| 主机 |     IP      |        MAC        |
| :--: | :---------: | :---------------: |
|  A   | 192.168.1.1 | 0A-11-22-33-44-01 |
|  B   | 192.168.1.2 | 0A-11-22-33-44-02 |
|  C   | 192.168.1.3 | 0A-11-22-33-44-03 |

> 1.主机A要和主机C通信，主机A发出ARP包询问谁是192.168.1.3?请回复192.168.1.1
>
> 2.这时主机B在疯狂的向主机A回复，我是192.168.1.3，我的地址是0A-11-22-33-44-02
>
> 3.由于ARP协议不会验证回复者的身份，造成主机A错误的将192.168.1.3的MAC映射为0A-11-22-33-44-02。

**ARP欺骗的分类**

> 1.主机欺骗，如同上面的栗子，主机B欺骗局域网中的主机A。
>
> 2.网关欺骗，局域网中的主机欺骗网关，从而获取其他主机的进流量。

**如何防御ARP欺骗**

ARP欺骗是通过重复应答实现的，那么只需要在本机添加一条静态的ARP映射，这样就不需要询问网关MAC地址了，这种方法只对主机欺骗有效。对于网关欺骗还需要在网关中也添加一条到主机的静态ARP映射。1.用管理身份运行命令提示符；输入netsh i i show in，查看一下本机有哪些网络连接

```
netsh i i show in
```

2.查看一下网关的MAC地址。注意如果正遭受ARP欺骗攻击，通过此方法查处的可能是虚假的MAC地址。输入arp -a命令查询本机的arp映射表，如果找不到网关的信息，可以先ping一下网关。

3.输入：netsh -c "i i" add neighbors 连接的Idx号 网关IP 网关MAC 添加一条静态映射,我已经添加过了，所以会显示 对象已存在

```
netsh -c "i i" add neighbors 连接的Idx号 网关IP 网关MAC
netsh -c "i i" add neighbors 9 10.60.12.1 4c-5e-0c-64-73-f5
```



## DNS

- 你肯定记得住网站的名称，但是很难记住网站的IP地址，因而也需要一个地址簿，就是**DNS**服务器。
  - 由此可见，DNS在日常生活中多么重要。每个人上网，都需要访问它，但是同时，这对它来讲也是非常 大的挑战。一旦它出了故障，整个互联网都将瘫痪。另外，上网的人分布在全世界各地，如果大家都去 同一个地方访问某一台服务器，时延将会非常大。因而，**DNS服务器，一定要设置成高可用、高并发 和分布式的**。
  - 于是，就有了**树状的层次结构**。
    - 根DNS服务器 :返回顶级域DNS服务器的IP地址 
    - 顶级域DNS服务器:返回权威DNS服务器的IP地址 
    - 权威DNS服务器 :返回相应主机的IP地址
- **DNS**解析流程
  - 客户端——本地DNS缓存（etc/hosts）—本地DNS服务器——根DNS服务器——顶级域DNS服务器——权威DNS服务器
- 负载均衡
  - 在客户端角度，这是一次**DNS递归查询过程。**因为本地DNS全权为它效劳，它只要坐等结果即可。 在这个过程中，DNS除了可以通过名称映射为IP地址，它还可以做另外一件事，就是**负载均衡**。
  - DNS首先可以做**内部负载均衡**。
    - **例如，一个应用要访问数据库**，在这个应用里面应该配置这个数据库的IP地址，还是应该配置这个数据 库的域名呢?显然应该配置域名，因为一旦这个数据库，因为某种原因，换到了另外一台机器上，而如 果有多个应用都配置了这台数据库的话，一换IP地址，就需要将这些应用全部修改一遍。但是如果配置 了域名，则只要在DNS服务器里，将域名映射为新的IP地址，这个工作就完成了，大大简化了运维。
    - **在这个基础上，我们可以再进一步。例如，某个应用要访问另外一个应用**，如果配置另外一个应用 的IP地址，那么这个访问就是一对一的。但是当被访问的应用撑不住的时候，我们其实可以部署多个。 但是，访问它的应用，如何在多个之间进行负载均衡?只要配置成为域名就可以了。在域名解析的时 候，我们只要配置策略，这次返回第一个IP，下次返回第二个IP，就可以实现负载均衡了。
  - 另外一个更加重要的是，DNS还可以做**全局负载均衡**。
    - 为了保证我们的应用高可用，往往会部署在多个机房，每个地方都会有自己的IP地址。**当用户访问某个 域名的时候，这个IP地址可以轮询访问多个数据中心。**如果一个数据中心因为某种原因挂了，只要在DNS服务器里面，将这个数据中心对应的IP地址删除，就可以实现一定的高可用。
    - **另外，我们肯定希望北京的用户访问北京的数据中心，上海的用户访问上海的数据中心，这样，客户体 验就会非常好，访问速度就会超快。这就是全局负载均衡的概念**。
- 在域名和IP的映射过程中，给了应用基于域名做负载均衡的机会，可以是简单的负载均衡，也可以根据地域和运营商做全局的负载均衡。
  - 对于复杂的应用，尤其是跨地域跨运营商的大型应用，则需要更加复杂的全局负载均衡机制，因而 需要专门的设备或者服务器来做这件事情，这就是**全局负载均衡器**(**GSLB**，**Global Server Load Balance**)。
  - 例如两层的GSLB，是因为分运营商和地域。我们希望不同运营商的客户，可以访问相同运营商机 房中的资源，这样不跨运营商访问，有利于提高吞吐量，减少时延。
    - 第一层GSLB，通过查看请求它的本地DNS服务器所在的运营商，就知道用户所在的运营商。假设 是移动，通过CNAME的方式，通过另一个别名 object.yd.yourcompany.com，告诉本地DNS服务 器去请求第二层的GSLB。
    - 第二层GSLB，通过查看请求它的本地DNS服务器所在的地址，就知道用户所在的地理位置，然后 将距离用户位置比较近的Region里面，六个**内部负载均衡**(**SLB**，S**erver Load Balancer**)的地 址，返回给本地DNS服务器。
    - 本地DNS服务器将结果返回给本地DNS解析器。
    - 本地DNS解析器将结果缓存后，返回给客户端。
    - 客户端开始访问属于相同运营商的距离较近的Region 1中的对象存储，当然客户端得到了六个IP地 址，它可以通过负载均衡的方式，随机或者轮询选择一个可用区进行访问。对象存储一般会有三个 备份，从而可以实现对存储读写的负载均衡。

- 层次树状结构的联机分布式数据库系统
  - 产生于应用层上的域名系统 NDS就可以用来把互联网上的主机名转换成 IP 地址
  - 把待解析的域名放在 DNS 的请求报中，以 UDP 用户数据报方式发送给本地域名服务器。本地域名服务器在查找域名后，把对应的 IP 地址放在回答报文中返回。获得 IP 地址的后主机即可进行通信
- 域名解析过程
  - 域名解析过程
    - 本地域名服务器向根域名服务器的查询方式通常采取迭代查询
  - 递归查询
    - 主机向本地域名服务器的查询一般都采用递归查询

**DNS劫持 vs HTTP劫持**

开始正式介绍DNS劫持之前，先与HTTP劫持做一个比较，可能有助于有些同学对下文更容易理解更深入一点。

DNS劫持现象：你输入一个google.com网址，出来的是百度的页面

HTTP劫持现象：访问着github的页面，右下角出现了一个格格不入的广告弹窗

在dns解析过程中，有哪一环节出现问题的话，都可能会导致DNS解析错误，导致客户端（浏览器）得到一个假的ip地址，从而引导用户访问到这个冒名顶替，恶意的网站。

**下面大概说几种DNS劫持方法**

**1.本机DNS劫持**

攻击者通过某些手段使用户的计算机感染上木马病毒，或者恶意软件之后，恶意修改本地DNS配置，比如修改本地hosts文件，缓存等

**2. 路由DNS劫持**

很多用户默认路由器的默认密码，攻击者可以侵入到路由管理员账号中，修改路由器的默认配置

**3.攻击DNS服务器**

直接攻击DNS服务器，例如对DNS服务器进行DDOS攻击，可以是DNS服务器宕机，出现异常请求，还可以利用某些手段感染dns服务器的缓存，使给用户返回来的是恶意的ip地址

**2.DNS的防范**

> 就这上面的劫持方法，说几种方法手段

1.加强本地计算机病毒检查，开启防火墙等，防止恶意软件，木马病毒感染计算机

2.改变路由器默认密码，防止攻击者修改路由器的DNS配置指向恶意的DNS服务器

3.企业的话可以准备两个以上的域名，一旦一个域名挂掉，还可以使用另一个

4.用HTTP DNS 代替 Local DNS

> 对于DNS劫持，往往单靠个人设置很难解决，如果已经出现了劫持现象的话，对电脑进去杀毒，清理，检查hosts文件，核查网络设置的DNS配置（可以使用写公共的DNS服务器





## HTTPDNS

- 传统**DNS**存在哪些问题?
  - 域名缓存问题
    - 它可以在本地做一个缓存，也就是说，不是每一个请求，它都会去访问权威DNS服务器，而是访问过一 次就把结果缓存到自己本地，当其他人来问的时候，直接就返回这个缓存数据。
    - 另外，有的运营商会把一些静态页面，缓存到本运营商的服务器内，这样用户请求的时候，就不用跨运 营商进行访问，这样既加快了速度，也减少了运营商之间流量计算的成本。
    - **很多情况下是看不出问题的，但是当页面更新，用户会访问到老的页面，问题就出来了。**
    - **再就是本地的缓存，往往使得全局负载均衡失败**，因为上次进行缓存的时候，缓存中的地址不一定是这 次访问离客户最近的地方，如果把这个地址返回给客户，那肯定就会绕远路。
  - 域名转发问题
    - 缓存问题还是说本地域名解析服务，还是会去权威DNS服务器中查找，只不过不是每次都要查找。
    - 如果是A运营商的客户，访问自己运营商的DNS服务器，如果A运营商去权威DNS服务 器查询的话，权威DNS服务器知道你是A运营商的，就返回给一个部署在A运营商的网站地址，这样针 对相同运营商的访问，速度就会快很多。
    - 但是A运营商偷懒，将解析的请求转发给B运营商，B运营商去权威DNS服务器查询的话，权威服务器会误认为，你是B运营商的，那就返回给你一个在B运营商的网站地址吧，结果**客户的每次访问都要跨运营商，速度就会很慢**。
  - 出口**NAT**问题
    - 出口的时候，很多机房都会配置**NAT**，也即**网络地址转换**，使得从 这个网关出去的包，都换成新的IP地址，当然请求返回的时候，在这个网关，再将IP地址转换回去，所 以对于访问来说是没有任何问题。
    - **但是一旦做了网络地址的转换，权威的DNS服务器，就没办法通过这个地址，来判断客户到底是来自哪个运营商，**而且极有可能因为转换过后的地址，误判运营商，导致跨运营商的访问。
  - 域名更新问题
    - **本地DNS服务器是由不同地区、不同运营商独立部署的。对域名解析缓存的处理上，实现策略也有区别**，有的会偷懒，忽略域名解析结果的TTL时间限制，在权威DNS服务器解析变更的时候，解析结果在 全网生效的周期非常漫长。但是有的时候，在DNS的切换中，场景对生效时间要求比较高。
    - 例如**双机房部署**的时候，**跨机房的负载均衡和容灾多使用DNS来做**。当一个机房出问题之后，需要修改 权威DNS，将域名指向新的IP地址，但是如果更新太慢，那很多用户都会出现访问异常。
  - 解析延迟问题
    - DNS的查询过程需要递归遍历多个DNS服务器，才能获得最终的解析结 果，这会带来一定的时延，甚至会解析超时。
  
- **HTTPDNS**的工作模式
  
  - **HTTPNDS**其实就是，不走传统的**DNS**解析，而是自己搭建基于**HTTP**协议的**DNS**服务器集群，分布在多个地点和多个运营商。**当客户端需要DNS解析的时候，直接通过HTTP协议进行请求这个服务器集群，得到就近的地址。**
  - 这就相当于每家基于HTTP协议，自己实现自己的域名解析，自己做一个自己的地址簿，而不使用统一 的地址簿。但是默认的域名解析都是走DNS的，因而使用HTTPDNS需要绕过默认的DNS路径，就不能 使用默认的客户端。使用HTTPDNS的，往往是手机应用，需要在手机端嵌入支持HTTPDNS的客户 端SDK。
  
- **HTTPDNS的工作模式**

  - 在客户端的SDK里动态请求服务端，获取HTTPDNS服务器的IP列表，缓存到本地。随着不断地解析域名，SDK也会在本地缓存DNS域名解析的结果。
  - 当手机应用要访问一个地址的时候，首先看是否有本地的缓存，如果有就直接返回。这个缓存和本 地DNS的缓存不一样的是，这个是手机应用自己做的，而非整个运营商统一做的。如何更新、何时更 新，手机应用的客户端可以和服务器协调来做这件事情。
  - 如果本地没有，就需要请求HTTPDNS的服务器，在本地HTTPDNS服务器的IP列表中，选择一个发 出HTTP的请求，会返回一个要访问的网站的IP列表。
    - 手机客户端自然知道手机在哪个运营商、哪个地址。由于是直接的HTTP通信，HTTPDNS服务器能够准确知道这些信息，因而可以做精准的全局负载均衡。

- **HTTPDNS**的缓存设计

  - 解析DNS过程复杂，通信次数多，对解析速度造成很大影响。为了加快解析，因而有了缓存，但是这又 会产生缓存更新速度不及时的问题。**最要命的是，这两个方面都掌握在别人手中，也即本地DNS服务器 手中，它不会为你定制，你作为客户端干着急没办法。**
  - 而HTTPDNS就是将解析速度和更新速度全部掌控在自己手中。一方面，解析的过程，不需要本 地DNS服务递归的调用一大圈，一个HTTP的请求直接搞定，要实时更新的时候，马上就能起作用;另一方面为了提高解析速度，本地也有缓存，缓存是在客户端SDK维护的，过期时间、更新时间，都可以 自己控制。
  - SDK中的缓存会严格按照缓存过期时间，如果缓存没有命中，或者已经过期，而且客户端不允许使用过 期的记录，则会发起一次解析，保障记录是更新的。
  - 解析可以**同步进行**，也就是直接调用HTTPDNS的接口，返回最新的记录，更新缓存;也可以**异步进 行**，添加一个解析任务到后台，由后台任务调用HTTPDNS的接口。
    - **同步更新**的**优点**是实时性好，缺点是如果有多个请求都发现过期的时候，同时会请求HTTPDNS多次， 其实是一种浪费。同步更新的方式对应到应用架构中缓存的**Cache-Aside机制**，也即先读缓存，不命中读数据库，同时将结果写入缓存。
    - **异步更新**的**优点**是，可以将多个请求都发现过期的情况，合并为一个对于HTTPDNS的请求任务，只执 行一次，减少HTTPDNS的压力。同时可以在即将过期的时候，就创建一个任务进行预加载，防止过期 之后再刷新，称为**预加载**。
      - 它的**缺点**是当前请求拿到过期数据的时候，如果客户端允许使用过期数据，需要冒一次风险。如果过期 的数据还能请求，就没问题;如果不能请求，则失败一次，等下次缓存更新后，再请求方能成功。
      - 异步更新的机制对应到应用架构中缓存的**Refresh-Ahead****机制**，即业务仅仅访问缓存，当过期的时候定期刷新。

- **HTTPDNS**的调度设计

  - HTTPDNS通过客户端SDK和服务端，通过HTTP直接调用解析DNS的方式，绕过了传统DNS的这些缺 点，实现了智能的调度。

  



## CDN

- 内容分发网络，解决的是如何将数据快速可靠从源站传递到用户

- 全球有这么多的数据中心，无论在哪里上网，临近不远的地方基本上都有数据中心。是不是可以在这些 数据中心里部署几台机器，形成一个缓存的集群来缓存部分数据那么用户访问数据的时候，就可以就近访问了呢?

  - 当然是可以的。这些分布在各个地方的各个数据中心的节点，就称为**边缘节点**。
  - 由于边缘节点数目比较多，但是每个集群规模比较小，不可能缓存下来所有东西，因而可能无法命中， 这样就会在边缘节点之上有**区域节点**，规模就要更大，缓存的数据会更多，命中的概率也就更大。在 区域节点之上是中心节点，规模更大，缓存数据更多。如果还不命中，就只好回源网站访问了。
  - 这就是**CDN的分发系统的架构**。CDN系统的缓存，也是一层一层的，能不访问后端真正的源，就不打 扰它。

- 基于DNS的全局负载均衡，这个负载均衡主要用来选择一个就近的同样运营商的服 务器进行访问。你会发现，CDN分发网络也是一个分布在多个区域、多个运营商的分布式系统，也可 以用相同的思路选择最合适的边缘节点。

  - **在没有CDN的情况下**，用户向浏览器输入www.web.com 这个域名，客户端访问本地DNS服务器的时 候，如果本地DNS服务器有缓存，则返回网站的地址;如果没有，递归查询到网站的权威DNS服务器， 这个权威DNS服务器是负责web.com的，它会返回网站的IP地址。本地DNS服务器缓存下IP地址， 将IP地址返回，然后客户端直接访问这个IP地址，就访问到了这个网站。
  - 然而**有了CDN之后，情况发生了变化**。在web.com这个权威DNS服务器上，会设置一个CNAME别名， **指向另外一个域名 www.web.cdn.com，返回给本地DNS服务器。**
  - 当本地DNS服务器拿到这个新的域名时，需要继续解析这个新的域名。这个时候，再访问的就不是web.com的权威DNS服务器了，而是web.cdn.com的权威DNS服务器，**这是CDN自己的权威DNS服务器**。**在这个服务器上，还是会设置一个CNAME，指向另外一个域名，也即CDN网络的全局负载均衡器。**
    - 接下来，本地DNS服务器去请求CDN的全局负载均衡器解析域名，全局负载均衡器会为用户选择一台 合适的缓存服务器提供服务，选择的依据包括:
      - 根据用户IP地址，判断哪一台服务器距用户最近;
      - 用户所处的运营商; 
      - 根据用户所请求的URL中携带的内容名称，判断哪一台服务器上有用户所需的内容;
      - 查询各个服务器当前的负载情况，判断哪一台服务器尚有服务能力。
  - 基于以上这些条件，**进行综合分析之后，全局负载均衡器会返回一台缓存服务器的IP地址。**
  - **本地DNS服务器缓存这个IP地址，然后将IP返回给客户端，客户端去访问这个边缘节点，下载资源。**缓存服务器响应用户请求，将用户所需内容传送到用户终端。如果这台缓存服务器上并没有用户想要的内 容，那么这台服务器就要向它的上一级缓存服务器请求内容，直至追溯到网站的源服务器将内容拉到本 地。

- **CDN可以进行缓存的内容有很多种**

  - 在进入数据中心的时候，我们希望通过最外层接入层的缓存，将大部 分静态资源的访问拦在边缘。而CDN则更进一步，将这些静态资源缓存到离用户更近的数据中心外。 越接近客户，访问性能越好，时延越低。
  - 但是静态内容中，有一种特殊的内容，也大量使用了CDN，这个就是前面讲过的流媒体。
    - CDN支持**流媒体协议**，例如RTMP协议。在很多情况下，这相当于一个代理，从上一级缓存 读取内容，转发给用户。由于流媒体往往是连续的，因而可以进行预先缓存的策略，也可以预先推送到 用户的客户端。
    - 对于静态页面来讲，内容的分发往往采取**拉取**的方式，也即当发现未命中的时候，再去上一级进行拉 取。但是，流媒体数据量大，如果出现**回源**，压力会比较大，所以往往采取主动**推送**的模式，将热点数 据主动推送到边缘节点。
    - 对于流媒体来讲，很多CDN还提供**预处理服务**，也即文件在分发之前，经过一定的处理。例如将视频 转换为不同的码流，以适应不同的网络带宽的用户需求;再如对视频进行分片，降低存储压力，也使得 客户端可以选择使用不同的码率加载不同的分片。这就是我们常见的，“我要看超清、标清、流畅等”。
    - 对于流媒体CDN来讲，有个关键的问题是**防盗链**问题。因为视频是要花大价钱买版权的，为了挣点 钱，收点广告费，如果流媒体被其他的网站盗走，在人家的网站播放，那损失可就大了。
    - 最常用也最简单的方法就是**HTTP头的refer字段**， 当浏览器发送请求的时候，一般会带上referer，告 诉服务器是从哪个页面链接过来的，服务器基于此可以获得一些信息用于处理。如果refer信息不是来自本站，就阻止访问或者跳到其它链接。

- **refer的机制相对比较容易破解，所以还需要配合其他的机制**

  - 一种常用的机制是**时间戳防盗链**。使用CDN的管理员可以在配置界面上，和CDN厂商约定一个加密字符串。
    - 客户端取出当前的时间戳，要访问的资源及其路径，连同加密字符串进行签名算法得到一个字符串，然 后生成一个下载链接，带上这个**签名字符串和截止时间戳**去访问CDN。
    - 在CDN服务端，根据取出过期时间，和当前 CDN 节点时间进行比较，确认请求是否过期。然后CDN服 务端有了资源及路径，时间戳，以及约定的加密字符串，根据相同的签名算法计算签名，如果匹配则一 致，访问合法，才会将资源返回给客户。

- 动态的数据，比较难以缓存。怎么办呢?现在也有**动态CDN，主要有两种模式**

  - **边缘计算的模式**
    - **既然数据是动态生成的，所以数据的逻辑计算和存 储，也相应的放在边缘的节点。其中定时从源数据那里同步存储的数据，然后在边缘进行计算得到结果。**就像对生鲜的烹饪是动态的，没办法事先做好缓存，因而将生鲜超市放在你家旁边，既能够 送货上门，也能够现场烹饪，也是边缘计算的一种体现。
  - **路径优化的模式**
    - **数据不是在边缘计算生成的，而是在源站生成的，但是数据的下发则可以通过CDN的网络，对路径进行优化**。因为CDN节点较多，能够找到离源站很 近的边缘节点，也能找到离用户很近的边缘节点。中间的链路完全由CDN来规划，选择一个更加可 靠的路径，使用类似专线的方式进行访问。
  - 对于常用的TCP连接，在公网上传输的时候经常会丢数据，导致TCP的窗口始终很小，发送速度上不 去。根据前面的TCP流量控制和拥塞控制的原理，在CDN加速网络中可以调整TCP的参数，使得TCP可 以更加激进地传输数据。
    - 可以通过多个请求复用一个连接，保证每次动态请求到达时。连接都已经建立了，不必临时三次握手或 者建立过多的连接，增加服务器的压力。另外，可以通过对传输数据进行压缩，增加传输效率。

  

## HTTP

- HTTP幂等性
  - 幂等性是数学中的一个概念，表达的是N次变换与1次变换的结果相同
  - HTTP POST和PUT，二者均可用于创建资源，更为本质的差别是在幂等性方面
    - POST所对应的URI并非创建的资源本身，而是资源的接收者
      - 两次相同的POST请求会在服务器端创建两份资源，它们具有不同的URI；所以，POST方法不具备幂等性
    - PUT所对应的URI是要创建或更新的资源本身
      - 对同一URI进行多次PUT的副作用和一次PUT是相同的；因此，PUT方法具有幂等性
- HTTP协议的瓶颈及其优化技巧都是基于TCP协议本身的特性
  - 三次握手有1.5个RTT（round-trip time）的延迟
  - 不同策略的http长链接方案
    - HTTP的长连接和短连接本质上是TCP长连接和短连接
  - TCP在建立连接的初期有慢启动
- http和socket长连接和短连接区别
  - Socket是应用层与TCP/IP协议族通信的中间软件抽象层，它是一组接口
  - 门面模式，它把复杂的TCP/IP协议族隐藏在Socket接口后面，对用户来说，一组简单的接口就是全部，让Socket去组织数据，以符合指定的协议
- HTTP2.0
  - HTTP1.x有以下几个主要缺点：
    - 一次只允许在一个TCP连接上发起一个请求
    - HTTP/1.1使用的流水线技术也只能部分处理请求并发，仍然会存在队列头阻塞问题，因此客户端在需要发起多次请求时，通常会采用建立多连接来减少延迟。
    - 单向请求，只能由客户端发起。
    - 请求报文与响应报文首部信息冗余量大。
    - 数据未压缩，导致数据的传输量大。
  - 多路复用 (Multiplexing)
    - 所谓多路复用，即在一个TCP连接中存在多个流
    - HTTP/2 通信都在一个连接上完成，这个连接可以承载任意数量的双向数据流。在过去， HTTP 性能优化的关键并不在于高带宽，而是低延迟
  - 二进制分帧
    - 很容易的去实现多流并行而不用依赖建立多个 TCP 连接，HTTP/2 把 HTTP 协议通信的基本单位缩小为一个一个的帧
    - HTTP2.0中，有两个概念非常重要：帧（frame）和流（stream）。
      帧是最小的数据单位，每个帧会标识出该帧属于哪个流，流是多个帧组成的数据流。
  - 首部压缩（Header Compression）
  - 服务端推送（Server Push）
- 哈希算法
  - 将任意长度的信息转换为较短的固定长度的值，通常其长度要比信息小得多，且算法不可逆。







## VPN

- **VPN**，全名**Virtual Private Network**，**虚拟专用网**，就是利用开放的公众网络，**建立专用数据传输通道**，将远程的分支机构、移动办公人员等连接起来。

- **VPN**是如何工作的

  - VPN通过隧道技术在公众网络上仿真一条点到点的专线，是通过利用一种协议来传输另外一种协议的技术，这里面涉及三种协议:**乘客协议**、**隧道协议**和**承载协议**。
    - 在广州这边开车是有“协议”的，例如靠右行驶、红灯停、绿灯行，这个就相当于“被封装”的**乘客协议**。 当然在海南那面，开车也是同样的协议。这就相当于需要连接在一起的一个公司的两个分部。
    - 那我的车如何从广州到海南呢?这就需要你遵循开车的协议，将车开上轮渡，所有通过轮渡的车都关在船舱里面，按照既定的规则排列好，这就是**隧道协议**。
    - 但是在海上坐船航行，也有它的协议，例如要看灯塔、要按航道航行等。这就是外层的**承载协议**。

- 直接使用公网太不安全，所以接下来我们来看一种十分安全的VPN，**IPsec VPN**。这是基于IP协议的**安全隧道协议**，为了保证在公网上面信息的安全，因而采取了一定的机制保证 安全性。

  - 机制一:**私密性**，防止信息泄漏给未经授权的个人，通过加密把数据从明文变成无法读懂的密文， 从而确保数据的私密性。
    - 加密可以分为对称加密和非对称加密。对称加密速度快一些。而**VPN一 旦建立，需要传输大量数据，因而我们采取对称加密。**但是同样，**对称加密还是存在加密秘钥如何传输的问题，这里需要用到因特网密钥交换(IKE，Internet Key Exchange)协议。**
  - 机制二:**完整性**，数据没有被非法篡改，通过对数据进行hash运算，产生类似于指纹的数据摘要， 以保证数据的完整性。
  - 机制三:**真实性**，数据确实是由特定的对端发出，通过身份认证可以保证数据的真实性。

- 基于以上三个特性，组成了**IPsec VPN的协议簇**。

  - 在这个协议簇里面，有两种协议，这两种协议的区别在于封装网络包的格式不一样。
    - 一种协议称为**AH**(**Authentication Header**)，只能进行数据摘要 ，不能实现数据加密。 还有一种**ESP**(**Encapsulating Security Payload**)，能够进行数据加密和数据摘要。
    - 在这个协议簇里面，还有两类算法，分别是**加密算法**和**摘要算法**。
    - 这个协议簇还包含两大组件，一个用于VPN的双方要进行对称密钥的交换的**IKE组件**，另一个是VPN的双方要对连接进行维护的**SA(Security Association)组件**。

- **IPsec VPN**的建立过程

  - **第一个阶段，建立IKE自己的SA**。这个SA用来维护一个通过身份认证和安全保护的通道，为第二个阶 段提供服务。在这个阶段，通过DH(Diffe-Hellman)算法计算出一个对称密钥K。
    - DH算法是一个比较巧妙的算法。客户端和服务端约定两个公开的质数p和q，然后客户端随机产生一个 数a作为自己的私钥，服务端随机产生一个b作为自己的私钥，客户端可以根据p、q和a计算出公钥A， 服务端根据p、q和b计算出公钥B，然后双方交换公钥A和B。
    - **到此客户端和服务端可以根据已有的信息，各自独立算出相同的结果K**，就是**对称密钥**。但是这个过 程，对称密钥从来没有在通道上传输过，只传输了生成密钥的材料，通过这些材料，截获的人是无法算 出的。
  - 有了这个对称密钥K，接下来是**第二个阶段，建立IPsec SA**。**在这个SA里面，双方会生成一个随机的对 称密钥M，由K加密传给对方，然后使用M进行双方接下来通信的数据**。对称密钥M是有过期时间的， 会过一段时间，重新生成一次，从而防止被破解。
  - 当IPsec建立好，接下来就可以开始打包封装传输了。
    - 在原始的IP包IP头里面，会指定上一层的协议为TCP。**ESP要对IP包进行封装，因而IP头里面的 上一层协议为ESP。**在ESP的正文里面，ESP的头部有双方商讨好的SPI，以及这次传输的序列号。
    - 接下来全部是加密的内容。可以通过对称密钥进行解密，解密后在正文的最后，指明了里面的协议是什 么。如果是IP，则需要先解析IP头，然后解析TCP头，这是从隧道出来后解封装的过程。
    - **有了IPsec VPN之后，客户端发送的明文的IP包，都会被加上ESP头和IP头，在公网上传输**，由于加密， 可以保证不被窃取，到了对端后，去掉ESP的头，进行解密。

- **这种点对点的基于IP的VPN，能满足互通的要求，但是速度往往比较慢，这是由底层IP协议的特性决定 的。**IP不是面向连接的，是尽力而为的协议，每个IP包自由选择路径，到每一个路由器，都自己去找下 一跳，丢了就丢了，是靠上一层TCP的重发来保证可靠性。

- 和IP对应的另一种技术称为**ATM**。这种协议和IP协议的不同在于，它是面向连接的。你可以说TCP也是 面向连接的啊。这两个不同，ATM和IP是一个层次的，和TCP不是一个层次的。

  - 另外，TCP所谓的面向连接，是不停地重试来保证成功，其实下层的IP还是不面向连接的，丢了就丢 了。ATM是传输之前先建立一个连接，形成一个虚拟的通路，一旦连接建立了，所有的包都按照相同 的路径走，不会分头行事。
  - ATM技术虽然没有成功，但其屏弃了繁琐的路由查找，改为简单快速的标签交换，将具有全局意义的 路由表改为只有本地意义的标签表，这些都可以大大提高一台路由器的转发功力。

- 将两者的优点结合起来——**多协议标签交换**(**MPLS**，**Multi-Protocol Label Switching**)

  

  

  

## 云中网络

- 数据中心里面堆着一大片一大片的机器，用网络连接起来，机器数目一旦非常多，人们 就发现，维护这么一大片机器还挺麻烦的，有好多不灵活的地方。
  - **我们常把物理机比喻为自己拿地盖房子，而虚拟机则相当于购买公寓，更加灵活方面，随时可买可卖。**
  - 虚拟机用的是**软件模拟硬件**的方式。数据中心里面用的qemu-kvm，名字上来讲，emu就 是Emulator(模拟器)的意思，主要会模拟CPU、内存、网络、硬盘，使得虚拟机感觉自己在使用独立 的设备，但是真正使用的时候，当然还是使用物理的设备。
    - 例如，多个虚拟机轮流使用物理CPU，内存也是使用虚拟内存映射的方式，最终映射到物理内存上。硬 盘在一块大的文件系统上创建一个N个G的文件，作为虚拟机的硬盘。
- 虚拟网卡的原理
  - 首先，虚拟机要有一张网卡。对于qemu-kvm来说，这是通过Linux上的一种TUN/TAP 技术来实现的。
  - 虚拟机是物理机上跑着的一个软件。这个软件可以像其他应用打开文件一样，打开一个称为**TUN/TAP的Char Dev(字符设备文件)**。打开了这个字符设备文件之后，**在物理机上就能看到一张 虚拟TAP网卡。**
    - 虚拟化软件作为“骗子”，会将打开的这个文件，在虚拟机里面虚拟出一张网卡，**让虚拟机里面的应用觉 得它们真有一张网卡。于是，所有的网络包都往这里发。**
    - 当然，网络包会到虚拟化软件这里。它会将网络包转换成为文件流，写入字符设备，就像写一个文件一 样。内核中TUN/TAP 字符设备驱动会收到这个写入的文件流，交给TUN/TAP 的虚拟网卡驱动。这个驱 动将文件流再次转成网络包，交给TCP/IP协议栈，最终从虚拟TAP网卡发出来，成为标准的网络包。
  - 就这样，几经转手，数据终于从虚拟机里面，发到了虚拟机外面。
- 虚拟网卡连接到云中
  - 云计算中的网络都需要注意哪些点
    - **共享**:尽管每个虚拟机都会有一个或者多个虚拟网卡，但是物理机上可能只有有限的网卡。那这么 多虚拟网卡如何共享同一个出口?
    - **隔离**:分两个方面，一个是安全隔离，两个虚拟机可能属于两个用户，那怎么保证一个用户的数据 不被另一个用户窃听?一个是流量隔离，两个虚拟机，如果有一个疯狂下片，会不会导致另外一个 上不了网?
    - **互通**:分两个方面，一个是如果同一台机器上的两个虚拟机，属于同一个用户的话，这两个如何相 互通信?另一个是如果不同物理机上的两个虚拟机，属于同一个用户的话，这两个如何相互通信?
    - **灵活**:虚拟机和物理不同，会经常创建、删除，从一个机器漂移到另一台机器，有的互通、有的不 通等等，灵活性比物理网络要好得多，需要能够灵活配置。
- 共享与互通问题
  - 在物理机上，应该有一个虚拟的交换机，在Linux上有一个命令叫作brctl，可以**创建虚拟的网桥brctl addbr br0**。创建出来以后，将**两个虚拟机的虚拟网卡，都连接到虚拟网桥brctl addif br0 tap0上**，这样将两个虚拟机配置相同的子网网段，**两台虚拟机就能够相互通信了**。
  - 那这些虚拟机如何连外网呢
    - host-only 的网络对应的，其实就是上面两个虚拟机连到一个br0虚拟网桥上，而且不考虑访问 外部的场景，只要虚拟机之间能够相互访问就可以了。
  - **如果要访问外部，往往有两种方式。**
    - 一种方式称为**桥接**。
      - 每个虚拟机都会有虚拟网卡，在你的笔记本电脑上，会发现多了几个网卡，其实是虚拟交换机。**这个虚拟交换机将虚拟机连接在一起**。**在桥接模式下，物理网卡也连接到这个虚拟交换机上。**
      - **如果使用桥接网络，当你登录虚拟机里看IP地址的时候会发现，你的虚拟机的地址和你的笔记本电脑 的，以及你旁边的同事的电脑的网段是一个网段**。这是为什么呢?这其实相当于将物理机和虚拟机放在 同一个网桥上，相当于这个网桥上有三台机器，是一个网段的，全部打平了。
      - 在**数据中心**里面，采取的也是类似的技术，只不过都是Linux，**在每台机器上都创建网桥br0，虚拟机的 网卡都连到br0上，物理网卡也连到br0上，所有的br0都通过物理网卡出来连接到物理交换机上。**
        - 在这种方式下，不但解决了同一台机器的互通问题，也解决了跨物理机的互通问题，因为都在一个二层 网络里面，彼此用相同的网段访问就可以了。但是当规模很大的时候，会存在问题。
        - **在一个二层网络里面，最大的问题是广播**。一个数据中心的物理机已经很多了，广播已经 非常严重，需要通过VLAN进行划分。如果使用了虚拟机，假设一台物理机里面创建10台虚拟机，全部 在一个二层网络里面，那广播就会很严重，所以除非是你的桌面虚拟机或者数据中心规模非常小，才可以使用这种相对简单的方式。
    -  另外一种方式称为**NAT**。
      - 在这种方式下，你登录到虚拟机里面**查看IP地址**，会发现虚拟机的网络是虚拟机的，物理机的网络是物 理机的，**两个不相同**。虚拟机要想访问物理机的时候，需要将地址NAT 成为物理机的地址。
      - 除此之外，**它还会在你的笔记本电脑里内置一个DHCP服务器，为笔记本电脑上的虚拟机动态分配IP地 址。因为虚拟机的网络自成体系，需要进行IP管理**。为什么桥接方式不需要呢?因为**桥接将网络打平 了，虚拟机的IP地址应该由物理网络的DHCP服务器分配。**
      - 虚拟机是你的电脑，路由器和DHCP Server相当于家用路由器或者寝室长的电脑，物理网卡相当于你们 宿舍的外网网口，用于访问互联网。所有电脑都通过内网网口连接到一个网桥br0上，**虚拟机要想访问 互联网，需要通过br0连到路由器上，然后通过路由器将请求NAT 成为物理网络的地址，转发到物理网络。**
- 隔离问题
  - **如果一台机器上的两个虚拟机不属于同一个用户**，怎么办呢?好在brctl创建的网桥也是支持VLAN功能 的，可以设置两个虚拟机的tag，这样在这个虚拟网桥上，两个虚拟机是不互通的。
  - **但是如何跨物理机互通，并且实现VLAN的隔离呢**。有一个命令**vconfig**，可以基于物理网卡eth0创建带VLAN的虚拟网卡，所有从这个虚拟网卡出去的包，都带这个VLAN，如果这样，跨物理机的互通和隔离就可以通过这个网卡来实现。
    - **首先为每个用户分配不同的VLAN**，例如有一个用户VLAN 10，一个用户VLAN 20。在一台物理机上， 基于物理网卡，为每个用户用vconfg创建一个带VLAN的网卡。**不同的用户使用不同的虚拟网桥， 带VLAN的虚拟网卡也连接到虚拟网桥上。**
    - **不同的用户由于网桥不通，不能相互通信，一旦出了网桥，由 于VLAN不同，也不会将包转发到另一个网桥上。另外，出了物理机，也是带着VLAN ID的。**只要物理 交换机也是支持VLAN的，到达另一台物理机的时候，VLAN ID依然在，它只会将包转发给相同VLAN的 网卡和网桥，所以跨物理机，不同的VLAN也不会相互通信。
  - **使用brctl创建出来的网桥功能是简单的，基于VLAN的虚拟网卡也能实现简单的隔离。但是这都不是大 规模云平台能够满足的，一个是VLAN的隔离，数目太少**。VLAN ID只有4096个，明显 不够用。**另外一点是这个配置不够灵活**。谁和谁通，谁和谁不通，流量的隔离也没有实现，还有大量改进的空间。







## 云中的网络安全

- **对于公有云上的虚拟机，我的建议是仅仅开放需要的端口，而将其他的端口一概关闭。这个时候， 你只要通过安全措施守护好这个唯一的入口就可以了**。采用的方式常常是用**ACL**(Access Control List，**访问控制列表**)来控制IP和端口。

  - 在云平台上，这些规则的集合常称为**安全组**。

- 当一个网络包进入一台机器的时候，都会做什么事情

  - 首先拿下MAC头看看，是不是我的。如果是，则拿下IP头来。得到目标IP之后呢，就开始进行路由判断。在路由判断之前，这个节点我们称为**PREROUTING**。
  - 如果发现IP是我的，包就应该是我的，就发 给上面的传输层，这个节点叫作**INPUT**。如果发现IP不是我的，就需要转发出去，这个节点称 为**FORWARD**。
  - 如果是我的，上层处理完毕完毕后，一般会返回一个处理结果，这个处理结果会发出 去，这个节点称为**OUTPUT**，无论是FORWARD 还是OUTPUT，都是路由判断之后发生的，最后一个节点是**POSTROUTING**。

- 在Linux内核中，有一个框架叫**Netflter。它可以在这些节点插入hook函数**。这些函数可以截获 数据包，对数据包进行干预。例如做一定的修改，然后决策是否接着交给TCP/IP协议栈处理;或者可以 交回给协议栈，那就是**ACCEPT**;或者过滤掉，不再传输，就是**DROP**;还有就是**QUEUE**，发送给某个用户态进程处理。

- 一个著名的实现，就是**内核模块ip_tables**。它在这五个节点上埋下函数，从而可以根据规则进行包的处理

  - 按功能可分为四大类:连接跟踪(conntrack)、数据包的过滤(flter)、网络地址转换(nat) 和数据包的修改(mangle)。其中连接跟踪是基础功能，被其他功能所依赖。其他三个可以实现包的 过滤、修改和网络地址转换。
  - 在用户态，有一个客户端程序iptables，用命令行来干预内核的规则。内核的功能对 应iptables的命令行来讲，就是**表和链**的概念。
    - iptables的表分为四种:raw–>mangle–>nat–>flter。这四个优先级依次降低，raw不常用，所以主要 功能都在其他三种表里实现。每个表可以设置多个链。
    - **flter表处理过滤功能**，主要包含三个链:
      - INPUT链:过滤所有目标地址是本机的数据包; 
      - FORWARD 链:过滤所有路过本机的数据包; 
      - OUTPUT链:过滤所有由本机产生的数据包。
    - **nat表主要是处理网络地址转换**，可以进行Snat(改变数据包的源地址)、Dnat(改变数据包的目标地 址)，包含三个链:
      - PREROUTING链:可以在数据包到达防火墙时改变目标地址; 
      - OUTPUT链:可以改变本地产生的数据包的目标地址; 
      - POSTROUTING链:在数据包离开防火墙时改变数据包的源地址。
    - mangle表主要是修改数据包

- 将iptables的表和链加入到上面的过程中

  - 数据包进入的时候，先进mangle表的PREROUTING链。在这里可以根据需要，改变数据包头内容 之后，进入nat表的PREROUTING链，在这里可以根据需要做Dnat，也就是目标地址转换。
  - 进入路由判断，要判断是进入本地的还是转发的。
  - 如果是进入本地的，就进入INPUT链，之后按条件过滤限制进入。
  - 之后进入本机，再进入OUTPUT链，按条件过滤限制出去，离开本地。
  - 如果是转发就进入FORWARD链，根据条件过滤限制转发。
  - 之后进入POSTROUTING链，这里可以做Snat，离开网络接口。

- 有了iptables命令，我们就可以在云中实现一定的安全策略。

  - 例如我们可以处理前面的偷窥事件。首先 我们将所有的门都关闭。

  - ```shell
    # -s表示源IP地址段，-d表示目标地址段，DROP表示丢弃，也即无论从哪里来的，要想访问我这台机 器，全部拒绝，谁也黑不进来。
    iptables -t filter -A INPUT -s 0.0.0.0/0.0.0.0 -d X.X.X.X -j DROP
    
    # 如果这台机器是提供的是web服务，80端口也应该打开，当然一旦打开，这个80端口就需要很好的防 护，但是从规则角度还是要打开。
    iptables -A INPUT -s 0.0.0.0/0.0.0.0 -d X.X.X.X -p tcp --dport 80 -j ACCEPT
    
    # 这样就搞定了，其他的账户都封死，就一个防盗门可以进出，只要防盗门是五星级的，就比较安全了。
    ```

    - 在云平台上，一般允许一个或者多个虚拟机属于某个安全组，而属于不同安全组的虚拟机 之间的访问以及外网访问虚拟机，都需要通过安全组进行过滤。

- 这些安全组规则都可以自动下发到每个在安全组里面的虚拟机上，从而控制一大批虚拟机的安全策略。 这种批量下发是怎么做到的呢

  - **需要有一个地方统一配置这些iptables规则。**
    - **可以多加一个网桥**，在这个网桥上配置iptables规则，将在用户在界面上配置的规则，放到这个网桥 上。**然后在每台机器上跑一个Agent，将用户配置的安全组变成iptables规则，配置在这个网桥上。**

- 在设计云平台的时候，我们想让虚拟机之间的网络和物理网络进行隔离，**但是虚 拟机毕竟还是要通过物理网和外界通信的**，因而需要在出物理网的时候，做一次网络地址转换，也 即**nat，这个就可以用iptables来做。**

  - 云平台里面的虚拟机也是这样子的，它只有私网IP地址，到达外网网口要做一次Snat，转换成为机房 网IP，然后出数据中心的时候，再转换为公网IP。

  - 这里有一个问题是，在外网网口上做Snat的时候，是全部转换成一个机房网IP呢，还是每个虚拟机都对 应一个机房网IP，最终对应一个公网IP呢?前面也说过了，公网IP非常贵，虚拟机也很多，当然不能每个都有单独的机房网和公网IP了，于是**这种Snat是一种特殊的Snat，MASQUERADE(地址伪装)**。

  - **这种方式下，所有的虚拟机共享一个机房网和公网的IP地址**，所有从外网网口出去的，都转换成为这 个IP地址。那又一个问题来了，都**变成一个公网IP了，当163网站返回结果的时候，给谁呢，再nat成为 哪个私网的IP呢?**

    - **这就是Netflter的连接跟踪(conntrack)功能了**。对于TCP 协议来讲，肯定是上来先建立一个连接，可 以用“源/目的IP+源/目的端口”唯一标识一条连接，这个连接会放在**conntrack表**里面。当时是这台机器 去请求163网站的，虽然源地址已经Snat成公网IP地址了，但是conntrack表里面还是有这个连接的记录 的。当163网站返回数据的时候，会找到记录，从而找到正确的私网IP地址。

  - 这是虚拟机做客户端的情况，**如果虚拟机做服务器呢**

    - **这个时候就需要给这个网站配置固定的物理网的IP地址和公网IP地址了。这时候就需要显示的配 置Snat规则和Dnat规则了。**

    - 当外部访问进来的时候，外网网口会通过Dnat规则将公网IP地址转换为私网IP地址，到达虚拟机，虚拟 机里面是163网站，返回结果，外网网口会通过Snat规则，将私网IP地址转换为那个分配给它的固定的 公网IP地址。

    - ```shell
      # 类似的规则如下:
      # 源地址转换(Snat):
      iptables -t nat -A -s 私网IP -j Snat --to-source 外网IP 
      
      # 目的地址转换(Dnat):
      iptables -t nat -A -PREROUTING -d 外网IP -j Dnat --to-destination 私网IP
      ```

      




