

## Linux内存是怎么工作的

- **内存映射**

  - 我们通常所说的内存容量，就像我刚刚提到的 8GB，其实指的是物理内存。物理内存也称为主存，大多数计算机用的主存都是动态随机访问内存（DRAM）。只有内核才可以直接访问物理内存。那么，进程要访问内存时，该怎么办呢？
  - **Linux 内核给每个进程都提供了一个独立的虚拟地址空间，并且这个地址空间是连续的。**这样，进程就可以很方便地访问内存，更确切地说是访问虚拟内存
    - 虚拟地址空间的内部又被分为内核空间和用户空间两部分
    - 进程在用户态时，只能访问用户空间内存；只有进入内核态后，才可以访问内核空间内存。
    - 虽然每个进程的地址空间都包含了内核空间，但这些内核空间，其实关联的都是相同的物理内存。这样，进程切换到内核态后，就可以很方便地访问内核空间内存。 既然每个进程都有一个这么大的地址空间，那么所有进程的虚拟内存加起来，自然要比实际的物理内存大得多。所以，并不是所有的虚拟内存都会分配物理内存，只有那些实际使用的虚拟内存才分配物理内存，并且分配后的物理内存，是通过内存映射来管理的。
  - **内存映射，其实就是将虚拟内存地址映射到物理内存地址**
    - 为了完成内存映射，内核为每个进程都维护了一张页表，记录虚拟地址与物理地址的映射关系
    - 页表实际上存储在 CPU 的内存管理单元 MMU 中，这样，正常情况下，处理器就可以直接通过硬件，找出要访问的内存。
    - 当进程访问的虚拟地址在页表中查不到时，系统会产生一个缺页异常，进入内核空间分配物理内存、更新进程页表，最后再返回用户空间，恢复进程的运行
    - TLB（转译后备缓冲器）会影响 CPU 的内存访问性能。TLB 其实就是 MMU 中页表的高速缓存。
    - MMU 并不以字节为单位来管理内存，而是规定了一个内存映射的最小单位，也就是页，通常是 4 KB 大小。这样，每一次内存映射，都需要关联 4 KB 或者 4KB 整数倍的内存空间
      - 页的大小只有 4 KB ，导致的另一个问题就是，整个页表会变得非常大
      - Linux 提供了两种机制，也就是多级页表和大页（HugePage）。

- **虚拟内存空间分布**

  - 进一步了解虚拟内存空间的分布情况。最上方的内核空间不用多讲，下方的用户空间内存，其实又被分成了多个不同的段。以 32 位系统为例，从下往上为

    1. 只读段，包括代码和常量等。
    2. 数据段，包括全局变量等。
    3. 堆，包括动态分配的内存，从低地址开始向上增长。
    4. 文件映射段，包括动态库、共享内存等，从高地址开始向下增长。
    5. 栈，包括局部变量和函数调用的上下文等。栈的大小是固定的，一般是 8 MB。

    - 在这五个内存段中，堆和文件映射段的内存是动态分配的。

- 如何查看内存使用情况

  - free 输出的是一个表格，其中的数值都默认以字节为单位
  - free 显示的是整个系统的内存使用情况。比如总内存、已用内存、缓存、可用内存等。其中缓存是 Buffer 和 Cache 两部分的总和 。
  - 如果你想查看进程的内存使用情况，可以用 top 或者 ps 等工具（top 按下 M 切换到内存排序）。

- free 数据的来源

  - Buffer 和 Cache 还是我们用 free 获得的指标
  - Buffers 是内核缓冲区用到的内存，对应的是 /proc/meminfo 中的 Buffers 值。
    - Buffers 是对原始磁盘块的临时存储，也就是用来缓存磁盘的数据
  - Cache 是内核页缓存和 Slab 用到的内存，对应的是 /proc/meminfo 中的 Cached 与SReclaimable 之和。
    - Cached 是从磁盘读取文件的页缓存，也就是用来缓存从文件读取的数据。这样，下次访问这些文件数据时，就可以直接从内存中快速获取，而不需要再次访问缓慢的磁盘。
  - Buffer 是对磁盘数据的缓存，而 Cache 是文件数据的缓存，它们既会用在读请求中，也会用在写请求中。

  



## 内存中的Buffer和Cache

- Buffer 既可以用作“将要写入磁盘数据的缓存”，也可以用作“从磁盘读取数据的缓存”。

- Cache 既可以用作“从文件读取数据的页缓存”，也可以用作“写文件的页缓存”。

- dd 来模拟磁盘和文件的 I/O

  - > if=文件名：输入文件名，缺省为标准输入。即指定源文件。
    >
    > of=文件名：输出文件名，缺省为标准输出。即指定目的文件。
    >
    > ibs=bytes：一次读入bytes个字节，即指定一个块大小为bytes个字节。
    >
    > obs=bytes：一次输出bytes个字节，即指定一个块大小为bytes个字节。
    >
    > bs=bytes：同时设置读入/输出的块大小为bytes个字节。
    >
    > count=blocks：仅拷贝blocks个块，块大小等于ibs指定的字节数。

  - 文件写

    - > dd if=/dev/urandom of=/tmp/file bs=1M count=500

  - 文件读

    - > dd if=/tmp/file of=/dev/null

  - 磁盘写

    - > dd if=/dev/urandom of=/dev/sdb1 bs=1M count=2048

  - 磁盘读

    - > dd if=/dev/sda1 of=/dev/null bs=1M count=1024

- 关于磁盘和文件的区别

  - 磁盘是一个块设备，可以划分为不同的分区；在分区之上再创建文件系统，挂载到某个目录，之后才可以在这个目录中读写文件。
  - Linux 中“一切皆文件”，而文中提到的“文件”是普通文件，磁盘是块设备文件，可以执行 "ls -l <路径>" 查看它们的区别
  - 在读写普通文件时，会经过文件系统，由文件系统负责与磁盘交互
  - 而读写磁盘或者分区时，就会跳过文件系统，也就是所谓的“裸I/O“
  - 这两种读写方式所使用的缓存是不同的，也就是文中所讲的 Cache 和 Buffer 区别。





## 系统缓存

- 概述

  - Buffer 和Cache 的设计目的，是为了提升系统的 I/O 性能
  - 它们利用内存，充当起慢速磁盘与快速CPU 之间的桥梁，可以加速 I/O 的访问速度。
  - 为了方便你理解，Buffer 和 Cache 我仍然用英文表示，避免跟“缓存”一词混淆。而文中的“缓存”，通指数据在内存中的临时存储。

- **缓存命中率**

  - cachestat 提供了整个操作系统缓存的读写命中情况。
  - cachetop 提供了每个进程的缓存命中情况。
    - cachetop 工具并不把直接 I/O 算进来。

- 使用 pcstat 这个工具，来查看文件在内存中的缓存大小以及缓存比例

  - > pcstat /bin/ls
    >
    > 它展示了 /bin/ls 这个文件的缓存情况

  - 如果你执行一下 ls 命令，再运行相同的命令来查看的话，就会发现 /bin/ls 都在缓存中了

- **要判断应用程序是否用了直接 I/O，最简单的方法当然是观察它的系统调用**，查找应用程序在调用它们时的选项。使用什么工具来观察系统调用呢？自然还是 strace。

  - 1024 次缓存全部命中，读的命中率是 100%，看起来全部的读请求都经过了系统缓存。但是问题又来了，如果真的都是缓存 I/O，读取速度不应该这么慢。
    - 内存以页为单位进行管理，而每个页的大小是 4KB。所以，在 5 秒的时间间隔里，命中的缓存为 1024*4K/1024 = 4MB，再除以 5 秒，可以得到每秒读的缓存是0.8MB，显然跟案例应用的 32 MB/s 相差太多。
  - 直接从磁盘读写的速度，自然远慢于对缓存的读写。这也是缓存存在的最大意义了。
    - **直接IO是跳过Buffer，裸IO是跳过文件系统（还是有buffer的）**
    - 修改源代码，删除O_DIRECT 选项，让应用程序使用缓存 I/O ，而不是直接 I/O，就可以加速磁盘读取速度
    - 读的命中率还是 100%，HITS （即命中数）却变成了 40960，同样的方法计算一下，换算成每秒字节数正好是 32 MB（即40960*4k/5/1024=32M）。







## 内存泄漏

- 当进程通过 malloc() 申请虚拟内存后，系统并不会立即为其分配物理内存，而是在首次访问时，才通过缺页异常陷入内核中分配内存。

- 管理内存的过程中，也很容易发生各种各样的“事故”

  - 没正确回收分配后的内存，导致了泄漏
  - 访问的是已分配内存边界外的地址，导致程序异常退出

- **内存的分配和回收**

  - 栈内存由系统自动分配和管理。一旦程序运行超出了这个局部变量的作用域，栈内存就会被系统自动回收，所以不会产生内存泄漏的问题。
  - 很多时候，我们事先并不知道数据大小，所以你就要用到标准库函数 malloc() __ 在程序中动态分配内存。这时候，系统就会从内存空间的堆中分配内存。
    - 堆内存由应用程序自己来分配和管理。除非程序退出，这些堆内存并不会被系统自动释放，而是需要应用程序明确调用库函数 free() 来释放它们。如果应用程序没有正确释放堆内存，就会造成内存泄漏
  - 内存映射段，包括动态链接库和共享内存，其中共享内存由程序动态分配和管理。所以，如果程序在分配后忘了回收，就会导致跟堆内存类似的泄漏问题
  - 只读段，包括程序的代码和常量，由于是只读的，不会再去分配新的内存，所以也不会产生内存泄漏。
  - 数据段，包括全局变量和静态变量，这些变量在定义时就已经确定了大小，所以也不会产生内存泄漏

- memleak

  - 专门用来检测内存泄漏的工具

  - memleak 可以跟踪系统或指定进程的内存分配、释放请求，然后定期输出一个未释放内存和相应调用栈的汇总情况（默认 5 秒）。

  - > 1 # -a 表示显示每个内存分配请求的大小以及地址 2 # -p 指定案例应用的 PID 号 
    >
    > 3 $ /usr/share/bcc/tools/memleak -a -p $(pidof app)





## Swap升高

- 当发生了内存泄漏时，或者运行了大内存的应用程序，导致系统的内存资源紧张时，会导致两种可能结果，内存回收和 OOM 杀死进程

  - 第一个可能的结果，内存回收，也就是系统释放掉可以回收的内存，比如缓存和缓冲区，就属于可回收内存。它们在内存管理中，通常被叫做文件页（Filebacked Page）
  - 应用程序动态分配的堆内存，也就是我们在内存管理中说到的匿名页是不是也可以回收呢
    - 如果这些内存在分配后很少被访问，似乎也是一种资源浪费。是不是可以把它们暂时先存在磁盘里，释放内存给其他更需要的进程
    - 其实，这正是 Linux 的 Swap 机制。Swap 把这些不常访问的内存先写到磁盘中，然后释放这些内存，给其他更需要的进程使用。再次访问这些内存时，重新从磁盘读入内存就可以了

- Swap 原理

  - Swap 其实是把系统的可用内存变大了。这样，即使服务器的内存不足，也可以运行大内存的应用程序。
  - Swap 说白了就是把一块磁盘空间或者一个本地文件当成内存来使用。它包括换出和换入两个过程。

- Linux 到底在什么时候需要回收内存呢

  - 一个最容易想到的场景就是，有新的大块内存分配请求，但是剩余内存不足。这个时候系统就需要回收一部分内存（比如前面提到的缓存），进而尽可能地满足新内存请求。这个过程通常被称为直接内存回收。
  - 除了直接内存回收，还有一个专门的内核线程用来定期回收内存，也就是kswapd0
    - 为了衡量内存的使用情况，kswapd0 定义了三个内存阈值（watermark，也称为水位），分别是页最小阈值（pages_min）、页低阈值（pages_low）和页高阈值（pages_high）。
    - kswapd0 定期扫描内存的使用情况，并根据剩余内存落在这三个阈值的空间位置，进行内存的回收操作。
      - 剩余内存小于页最小阈值，说明进程可用内存都耗尽了，只有内核才可以分配内存。
      - 剩余内存落在页最小阈值和页低阈值中间，说明内存压力比较大，剩余内存不多了。这时kswapd0 会执行内存回收，直到剩余内存大于页高阈值为止。
      - 剩余内存落在页低阈值和页高阈值中间，说明内存有一定压力，但还可以满足新内存请求。

- NUMA 与 Swap

  - 很多情况下，你明明发现了 Swap 升高，可是在分析系统的内存使用时，却很可能发现，系统剩余内存还多着呢。为什么剩余内存很多的情况下，也会发生 Swap 呢？

  - 这正是处理器的 NUMA 架构导致的。在 NUMA 架构下，多个处理器被划分到不同 Node 上，且每个 Node 都拥有自己的本地内存空间。

  - > 通过 numactl 命令，来查看处理器在 Node 的分布情况，以及每个 Node 的内存使用情况 $ numactl --hardware

- kubernetes关闭swap

  - 一个是性能问题，开启swap会严重影响性能（包括内存和I/O）
  - 另一个是管理问题，开启swap后通过cgroups设置的内存上限就会失效

- 磁盘 I/O 的案例

  - > 1 # 间隔 1 秒输出一组数据 2 # -r 表示显示内存使用情况，-S 表示显示 Swap 使用情况 3 $ sar -r -S 1
    >
    > sar 的输出结果是两个表格，第一个表格表示内存的使用情况，第二个表格表示 Swap 的使用情况

  - 刚开始，剩余内存（kbmemfree）不断减少，而缓冲区（kbbuffers）则不断增大，由此可知，剩余内存不断分配给了缓冲区。

  - 为什么 Swap 也跟着升高了呢？直观来说，缓冲区占了系统绝大部分内存，还属于可回收内存，内存不够用时，不应该先回收缓冲区吗？

    - 这种情况，我们还得进一步通过 /proc/zoneinfo ，观察剩余内存、内存阈值以及匿名页和文件页的活跃情况

    - > -A 表示仅显示 Normal 行以及之后的 15 行输出 $ watch -d grep -A 15 'Normal' /proc/zoneinf

    - 你可以发现，剩余内存（pages_free）在一个小范围内不停地波动。当它小于页低阈值（pages_low) 时，又会突然增大到一个大于页高阈值（pages_high）的值

  - 一段时间后，剩余内存已经很小，而缓冲区占用了大部分内存。这时候，Swap 的使用开始逐渐增大，缓冲区和剩余内存则只在小范围内波动。

    - 当剩余内存小于页低阈值时，系统会回收一些缓存和匿名内存，使剩余内存增大。其中，缓存的回收导致 sar 中的缓冲区减小，而匿名内存的回收导致了 Swap 的使用增大。
    - 紧接着，由于 dd 还在继续，剩余内存又会重新分配给缓存，导致剩余内存减少，缓冲区增大。

- 用smem --sort swap命令可以直接将进程按照swap使用量排序显示

- 关闭 Swap	

  - swapoff -a

- 关闭 Swap 后再重新打开，也是一种常用的 Swap 空间清理方法	

  - swapoff -a && swapon -a

- 在内存资源紧张时，Linux 会通过 Swap ，把不常访问的匿名页换出到磁盘中，下次访问的时候再从磁盘换入到内存中来

- 当 Swap 变高时，你可以用 sar、/proc/zoneinfo、/proc/pid/status 等方法，查看系统和进程的内存使用情况，进而找出 Swap 升高的根源和受影响的进程。

- 反过来说，通常，降低 Swap 的使用，可以提高系统的整体性能





## 如何找到系统内存的问题

- 内存**性能指标**

  - 首先，你最容易想到的是**系统内存使用情况**，比如已用内存、剩余内存、共享内存、可用内存、缓存和缓冲区的用量等。
    - 已用内存和剩余内存很容易理解，就是已经使用和还未使用的内存。 
    - 共享内存是通过 tmpfs 实现的，所以它的大小也就是 tmpfs 使用的内存大小。tmpfs 其实也是一种特殊的缓存。 
    - 可用内存是新进程可以使用的最大内存，它包括剩余内存和可回收缓存。 
    - 缓存包括两部分，一部分是磁盘读取文件的页缓存，用来缓存从磁盘读取的数据，可以加快以后再次访问的速度。另一部分，则是 Slab 分配器中的可回收内存。 
    - 缓冲区是对原始磁盘块的临时存储，用来缓存将要写入磁盘的数据。这样，内核就可以把分散的写集中起来，统一优化磁盘写入。
  - 第二类很容易想到的，应该是**进程内存使用情况**，比如进程的虚拟内存、常驻内存、共享内存以及 Swap 内存等。
    - 虚拟内存，包括了进程代码段、数据段、共享内存、已经申请的堆内存和已经换出的内存等。这里要注意，已经申请的内存，即使还没有分配物理内存，也算作虚拟内存。 
    - 常驻内存是进程实际使用的物理内存，不过，它不包括 Swap 和共享内存。
    - 共享内存，既包括与其他进程共同使用的真实的共享内存，还包括了加载的动态链接库以及程序的代码段等。 
    - Swap 内存，是指通过 Swap 换出到磁盘的内存。 
  - 除了这些很容易想到的指标外，我还想再强调一下，缺页异常。
    - 系统调用内存分配请求后，并不会立刻为其分配物理内存，而是在请求首次访问时，通过缺页异常来分配
    - 缺页异常又分为下面两种场景
      - 可以直接从物理内存中分配时，被称为次缺页异常。
      - 需要磁盘 I/O 介入（比如 Swap）时，被称为主缺页异常		
      - 显然，主缺页异常升高，就意味着需要磁盘 I/O，那么内存访问也会慢很多。
  - 除了系统内存和进程内存，第三类重要指标就是 **Swap** 的使用情况，比如 Swap 的已用空间、剩余空间、换入速度和换出速度等

- **内存性能工具**

  - 所有的案例中都用到了 free。这是个最常用的内存工具，可以查看系统的整体内存和 Swap 使用情况
  - 通过 proc 文件系统，找到了内存指标的来源；并通过 vmstat，动态观察了内存的变化情况。与 free 相比，vmstat 除了可以动态查看内存变化，还可以区分缓存和缓冲区、Swap 换入和换出的内存大小。
  - 为了弄清楚缓存的命中情况，我们又用了 cachestat，查看整个系统缓存的读写命中情况，并用 cachetop 来观察每个进程缓存的读写命中情况
  - 我们用 vmstat，发现了内存使用在不断增长，又用memleak，确认发生了内存泄漏。通过 memleak 给出的内存分配栈，我们找到了内存泄漏的可疑位置。
  - 用 sar 发现了缓冲区和 Swap 升高的问题。通过cachetop，我们找到了缓冲区升高的根源；通过对比剩余内存跟 /proc/zoneinfo 的内存阈，我们发现 Swap 升高是内存回收导致的。案例最后，我们还通过 /proc 文件系统，找出了 Swap 所影响的进程。

- **内存性能工具**

  - ![img](https://img2018.cnblogs.com/blog/1075436/201905/1075436-20190510153449496-214029835.png)
  - ![img](https://img2018.cnblogs.com/blog/1075436/201905/1075436-20190510153515384-1920005016.png)

  

- 如何迅速分析内存的**性能瓶颈**

  - 为了迅速定位内存问题，我通常会先运行几个覆盖面比较大的性能工具，比如free、top、vmstat、pidstat 等。
    - 具体的分析思路主要有这几步。 
      1. 先用 free 和 top，查看系统整体的内存使用情况。 
      2. 再用 vmstat 和 pidstat，查看一段时间的趋势，从而判断出内存问题的类型。 
      3. 最后进行详细分析，比如内存分配分析、缓存 / 缓冲区分析、具体进程的内存使用分析等。 
    - ![img](https://img2018.cnblogs.com/blog/1075436/201905/1075436-20190510153550118-342346961.png)
  - 当你通过 free，发现大部分内存都被缓存占用后，可以使用 vmstat 或者 sar观察一下缓存的变化趋势，确认缓存的使用是否还在继续增大
    - 如果继续增大，则说明导致缓存升高的进程还在运行，那你就能用缓存 / 缓冲区分析工具（比如 cachetop、slabtop 等），分析这些缓存到底被哪里占用。
  - 当你 free 一下，发现系统可用内存不足时，首先要确认内存是否被缓存 / 缓冲区占用。
    - 排除缓存 / 缓冲区后，你可以继续用 pidstat 或者 top，定位占用内存最多的进程
    - 找出进程后，再通过进程内存空间工具（比如 pmap），分析进程地址空间中内存的使用情况就可以了。
  - 当你通过 vmstat 或者 sar 发现内存在不断增长后，可以分析中是否存在内存泄漏的问题。
    - 比如你可以使用内存分配分析工具 memleak ，检查是否存在内存泄漏。如果存在内存泄漏问题，memleak 会为你输出内存泄漏的进程以及调用堆栈

- 在我看来，内存调优最重要的就是，保证应用程序的热点数据放到内存中，并尽量减少换页和交换。 常见的优化思路有这么几种

  1. **最好禁止 Swap**。如果必须开启 Swap，降低 swappiness 的值，减少内存回收时 Swap 的使用倾向。
  2. **减少内存的动态分配**。比如，可以使用内存池、大页（HugePage）等。
  3. **尽量使用缓存和缓冲区来访问数据**。比如，可以使用堆栈明确声明内存空间，来存储需要缓存的数据；或者用 Redis 这类的外部缓存组件，优化数据的访问。
  4. **使用 cgroups 等方式限制进程的内存使用情况**。这样，可以确保系统内存不会被异常进程耗尽。
  5. **通过 /proc/pid/oom_adj ，调整核心应用的 oom_score**。这样，可以保证即使内存紧张，核心应用也不会被 OOM 杀死

  





## **怎么评估系统的网络性能**

- **性能指标回顾**

  - 首先，**带宽**，表示链路的最大传输速率，单位是 b/s（比特 / 秒）。在你为服务器选购网卡 时，带宽就是最核心的参考指标。常用的带宽有 1000M、10G、40G、100G 等。 
  - 第二，**吞吐量**，表示没有丢包时的最大数据传输速率，单位通常为 b/s （比特 / 秒）或者 B/s（字节 / 秒）。吞吐量受带宽的限制，吞吐量 / 带宽也就是该网络链路的使用率。 
  - 第三，**延时**，表示从网络请求发出后，一直到收到远端响应，所需要的时间延迟。这个指标 在不同场景中可能会有不同的含义。它可以表示建立连接需要的时间（比如 TCP 握手延时），或者一个数据包往返所需时间（比如 RTT）。 
  - 最后，**PPS**，是 Packet Per Second（包 / 秒）的缩写，表示以网络包为单位的传输速 率。PPS 通常用来评估网络的转发能力，而基于 Linux 服务器的转发，很容易受到网络包 大小的影响（交换机通常不会受到太大影响，即交换机可以线性转发）。 

- **网络基准测试**

  - 基于 HTTP 或者 HTTPS 的 Web 应用程序，显然属于应用层，需要我们测试 HTTP/HTTPS 的性能；
  - 而对大多数游戏服务器来说，为了支持更大的同时在线人数，通常会基于 TCP 或 UDP ，与客户端进行交互，这时就需要我们测试 TCP/UDP 的性能

- **各协议层的性能测试**

  - 转发性能

    - hping3 作为一个 SYN 攻击的工具来使用

    - Linux 内核自带的高性能网络测试工具 pktgen。 pktgen 支持丰富的自定义选项，方便你根据实际需要构造所需网络包，从而更准确地测试 出目标服务器的性能。

    - 不过，在 Linux 系统中，你并不能直接找到 pktgen 命令。因为 pktgen 作为一个内核线 程来运行，需要你加载 pktgen 内核模块后，再通过 /proc 文件系统来交互。

    - ```shell
      1 $ modprobe pktgen
      2 $ ps -ef | grep pktgen | grep -v grep
      
      5 $ ls /proc/net/pktgen/
      
      
      ```

    - pktgen 在每个 CPU 上启动一个内核线程，并可以通过 /proc/net/pktgen 下面的同名文 件，跟这些线程交互；而 pgctrl 则主要用来控制这次测试的开启和停止。

  - $ cat /proc/net/pktgen/eth0

    - 你可以看到，测试报告主要分为三个部分： 
      - 第一部分的 Params 是测试选项； 
      - 第二部分的 Current 是测试进度，其中， packts so far（pkts-sofar）表示已经发送了 100 万个包，也就表明测试已完成。 
      - 第三部分的 Result 是测试结果，包含测试所用时间、网络包数量和分片、PPS、吞吐量 以及错误数。
      - PPS 为 12 万
        - 作为对比，你可以计算一下千兆交换机的 PPS。交换机可以达到线速（满负载时，无差错 转发），它的 PPS 就是 1000Mbit 除以以太网帧的大小，即 1000Mbps/((64+20)*8bit) = 1.5 Mpps（其中，20B 为以太网帧前导和帧间距的大小）。 你看，即使是千兆交换机的 PPS，也可以达到 150 万 PPS，比我们测试得到的 12 万大多 了。

  - TCP/UDP 性能

    - 相应的测试工具，比如 iperf 或者 netperf

      - 特别是现在的云计算时代，在你刚拿到一批虚拟机时，首先要做的，应该就是用 iperf ，测 试一下网络性能是否符合预期。 iperf 和 netperf 都是最常用的网络性能测试工具，测试 TCP 和 UDP 的吞吐量。它们都以 客户端和服务器通信的方式，测试一段时间内的平均吞吐量。

    - ```shell
      1 # -s 表示启动服务端，-i 表示汇报间隔，-p 表示监听端口
      2 $ iperf3 -s -i 1 -p 10000
      ```

    - ```shell
      # 接着，在另一台机器上运行 iperf 客户端，运行测试
      1 # -c 表示启动客户端，192.168.0.30 为目标服务器的 IP
      2 # -b 表示目标带宽 (单位是 bits/s)
      3 # -t 表示测试时间
      4 # -P 表示并发数，-p 表示目标服务器监听端口
      5 $ iperf3 -c 192.168.0.30 -b 1G -t 15 -P 2 -p 10000
      ```

    - 稍等一会儿（15 秒）测试结束后，回到目标服务器，查看 iperf 的报告

      - 最后的 SUM 行就是测试的汇总结果，包括测试时间、数据传输量以及带宽等。按照发送和 接收，这一部分又分为了 sender 和 receiver 两行。 从测试结果你可以看到，这台机器 TCP 接收的带宽（吞吐量）为 860 Mb/s， 跟目标的 1Gb/s 相比，还是有些差距的。 

  - HTTP 性能

    - 从传输层再往上，到了应用层。有的应用程序，会直接基于 TCP 或 UDP 构建服务。当 然，也有大量的应用，基于应用层的协议来构建服务，HTTP 就是最常用的一个应用层协 议。比如，常用的 Apache、Nginx 等各种 Web 服务，都是基于 HTTP。

    - 要测试 HTTP 的性能，也有大量的工具可以使用，比如 ab、webbench 等，都是常用的 HTTP 压力测试工具。其中，ab 是 Apache 自带的 HTTP 压测工具，主要测试 HTTP 服务 的每秒请求数、请求延迟、吞吐量以及请求延迟的分布情况等。

      - ```shell
        1 # -c 表示并发请求数为 1000，-n 表示总的请求数为 10000
        2 $ ab -c 1000 -n 10000 http://192.168.0.30/
        ```

      - 可以看到，ab 的测试结果分为三个部分，分别是请求汇总、连接时间汇总还有请求延迟汇 总。以上面的结果为例，我们具体来看。

        - **Requests per second** 为 1074； 

          每个请求的延迟（Time per request）分为两行，第一行的 927 ms 表示平均延迟，包 括了线程运行的调度时间和网络请求响应时间，而下一行的 0.927ms ，则表示实际请求 的响应时间； 

        - **Transfer rate 表示吞吐量（BPS）**为 890 KB/s。 

          连接时间汇总部分，则是分别展示了建立连接、请求、等待以及汇总等的各类时间，包括最 小、最大、平均以及中值处理时间。 

        - 最后的**请求延迟汇总部分**，则给出了不同时间段内处理请求的百分比，比如， 90% 的请 求，都可以在 274ms 内完成。

  - 应用负载性能

    - 当你用 iperf 或者 ab 等测试工具，得到 TCP、HTTP 等的性能数据后，这些数据是否就能 表示应用程序的实际性能呢？我想，你的答案应该是否定的。

      - 比如，你的应用程序基于 HTTP 协议，为最终用户提供一个 Web 服务。这时，使用 ab 工 具，可以得到某个页面的访问性能，但这个结果跟用户的实际请求，很可能不一致。因为用户请求往往会附带着各种各种的负载（payload），而这些负载会影响 Web 应用程序内部 的处理逻辑，从而影响最终性能。

    - 幸运的是，我们还可以用 wrk、TCPCopy、Jmeter 或 者 LoadRunner 等实现这个目标。

      - wrk 的命令行参数比较简单。比如，我们可以用 wrk ，来重新测一下前面已经启动的 Nginx 的性能。

        - ```shell
          1 # -c 表示并发连接数 1000，-t 表示线程数为 2
          2 $ wrk -c 1000 -t 2 http://192.168.0.30/
          ```

        - 你可以看到，每秒请求 数为 9641，吞吐量为 7.82MB，平均延迟为 65ms，比前面 ab 的测试结果要好很多。

        



## 网络性能优化的几个思路

- **确定优化目标**

  - 实际上，虽然网络性能优化的整体目标，是降低网络延迟（如 RTT）和提高吞吐量（如 BPS 和 PPS），但具体到不同应用中，每个指标的优化标准可能会不同，优先级顺序也大 相径庭。 
    - NAT 网关来说，由于其直接影响整个数据中心的网络出入性能，所以 NAT 网关通常需要达到或接近线性转发，也就是说， PPS 是最主要的性能目标。 
    - 再如，对于数据库、缓存等系统，快速完成网络收发，即低延迟，是主要的性能目标。
    - 而对于我们经常访问的 Web 服务来说，则需要同时兼顾

- Linux 网络协议栈

  - ![img](https://img2018.cnblogs.com/blog/1075436/201909/1075436-20190920105407329-759102076.png)
  - **首先是网络接口层和网络层**，它们主要负责网络包的封装、寻址、路由，以及发送和接收。 每秒可处理的网络包数 PPS，就是它们最重要的性能指标（特别是在小包的情况下）。你可以用内核自带的发包工具 pktgen ，来测试 PPS 的性能。
  - **再向上到传输层的 TCP 和 UDP**，它们主要负责网络传输。对它们而言，吞吐量（BPS）、 连接数以及延迟，就是最重要的性能指标。你可以用 iperf 或 netperf ，来测试传输层的性 能。 
    - 不过要注意，网络包的大小，会直接影响这些指标的值。所以，通常，你需要测试一系列不 同大小网络包的性能。
  - 最后，再往上到了应用层，最需要关注的是吞吐量（BPS）、每秒请求数以及延迟等指标。 你可以用 wrk、ab 等工具，来测试应用程序的性能

- 网络性能工具

  - 网络性能指标的工具
    - ![img](https://img2018.cnblogs.com/blog/1075436/201909/1075436-20190920105438150-732021018.png)
    - 性能工具
      - ![img](https://img2018.cnblogs.com/blog/1075436/201909/1075436-20190920105457114-2056652101.png)

- 应用程序

  - 从网络 I/O 的角度来说，主要有下面两种优化思路

    - 从网络 I/O 的角度来说，主要有下面两种优化思路。 第一种是最常用的 I/O 多路复用技术 epoll，主要用来取代 select 和 poll。这其实是解决 C10K 问题的关键，也是目前很多网络应用默认使用的机制。
    - 第二种是使用异步 I/O（Asynchronous I/O，AIO）。AIO 允许应用程序同时发起很多 I/O 操作，而不用等待这些操作完成。等到 I/O 完成后，系统会用事件通知的方式，告诉应 用程序结果。不过，AIO 的使用比较复杂，你需要小心处理很多边缘情况

  - 而从进程的工作模型来说，也有两种不同的模型用来优化

    - 第一种，主进程 + 多个 worker 子进程。其中，主进程负责管理网络连接，而子进程负责 实际的业务处理。这也是最常用的一种模型。 

      第二种，监听到相同端口的多进程模型。在这种模型下，所有进程都会监听相同接口，并且 开启 SO_REUSEPORT 选项，由内核负责，把请求负载均衡到这些监听进程中去。 

  - 应用层的网络协议优化

    - 使用长连接取代短连接，可以显著降低 TCP 建立连接的成本。在每秒请求次数较多时， 这样做的效果非常明显。 
    - 使用内存等方式，来缓存不常变化的数据，可以降低网络 I/O 次数，同时加快应用程序 的响应速度。 
    - 使用 Protocol Buffer 等序列化的方式，压缩网络 I/O 的数据量，可以提高应用程序的吞 吐。 
    - 使用 DNS 缓存、预取、HTTPDNS 等方式，减少 DNS 解析的延迟，也可以提升网络 I/O 的整体速度

- 套接字

  - 套接字可以屏蔽掉 Linux 内核中不同协议的差异，为应用程序提供统一的访问接口。每个 套接字，都有一个读写缓冲区。 

    - 读缓冲区，缓存了远端发过来的数据。如果读缓冲区已满，就不能再接收新的数据。 
    - 写缓冲区，缓存了要发出去的数据。如果写缓冲区已满，应用程序的写操作就会被阻塞。

  - > 增大每个套接字的缓冲区大小 net.core.optmem_max； 
    >
    > 增大套接字接收缓冲区大小 net.core.rmem_max 和发送缓冲区大小 
    >
    > net.core.wmem_max； 
    >
    > 增大 TCP 接收缓冲区大小 net.ipv4.tcp_rmem 和发送缓冲区大小 
    >
    > net.ipv4.tcp_wmem。
    >
    > 
    >
    > 发送缓冲区大小，理想数值是吞吐量 * 延迟，这样才可以达到最大网络利用 率

- 传输层

  - 第一类，在请求数比较大的场景下，你可能会看到大量处于 TIME_WAIT 状态的连接，它 们会占用大量内存和端口资源。这时，我们可以优化与 TIME_WAIT 状态相关的内核选 项，比如采取下面几种措施。
    - 增大处于 TIME_WAIT 状态的连接数量 net.ipv4.tcp_max_tw_buckets ，并增大连接跟踪表的大小 net.netfilter.nf_conntrack_max。 
    - 减小 net.ipv4.tcp_fin_timeout 和 net.netfilter.nf_conntrack_tcp_timeout_time_wait ，让系统尽快释放它们所占用的资源。 
    - 开启端口复用 net.ipv4.tcp_tw_reuse。这样，被 TIME_WAIT 状态占用的端口，还能用 到新建的连接中。 
    - 增大本地端口的范围 net.ipv4.ip_local_port_range 。这样就可以支持更多连接，提高整 体的并发能力。 
    - 增加最大文件描述符的数量。你可以使用 fs.nr_open 和 fs.file-max ，分别增大进程和 
    - 系统的最大文件描述符数；或在应用程序的 systemd 配置文件中，配置 LimitNOFILE ，设置应用程序的最大文件描述符数。
  - 第二类，为了缓解 SYN FLOOD 等，利用 TCP 协议特点进行攻击而引发的性能问题，你可 以考虑优化与 SYN 状态相关的内核选项，比如采取下面几种措施。 
    - 增大 TCP 半连接的最大数量 net.ipv4.tcp_max_syn_backlog ，或者开启 TCP SYN Cookies net.ipv4.tcp_syncookies ，来绕开半连接数量限制的问题（注意，这两个选项 不可同时使用）。 
    - 减少 SYN_RECV 状态的连接重传 SYN+ACK 包的次数 net.ipv4.tcp_synack_retries。 
  - 第三类，在长连接的场景中，通常使用 Keepalive 来检测 TCP 连接的状态，以便对端连接 断开后，可以自动回收。但是，系统默认的 Keepalive 探测间隔和重试次数，一般都无法 满足应用程序的性能要求。所以，这时候你需要优化与 Keepalive 相关的内核选项，比 如
    - 缩短最后一次数据包到 Keepalive 探测包的间隔时间 net.ipv4.tcp_keepalive_time；缩短发送 Keepalive 探测包的间隔时间 net.ipv4.tcp_keepalive_intvl； 
    - 减少 Keepalive 探测失败后，一直到通知应用程序前的重试次数net.ipv4.tcp_keepalive_probes。 
  - UDP 的优化
    - UDP 提供了面向数据报的网络协议，它不需要网络连接，也不提供可靠性保障。所以， UDP 优化，相对于 TCP 来说，要简单得多。这里我也总结了常见的几种优化方案。 
      - 跟上篇套接字部分提到的一样，增大套接字缓冲区大小以及 UDP 缓冲区范围； 
      - 跟前面 TCP 部分提到的一样，增大本地端口号的范围； 
      - 根据 MTU 大小，调整 UDP 数据包的大小，减少或者避免分片的发生。

- 网络层

  - 网络层，负责网络包的封装、寻址和路由，包括 IP、ICMP 等常见协议。在网络层，最主 要的优化，其实就是对路由、 IP 分片以及 ICMP 等进行调优。 
  - 第一种，从路由和转发的角度出发，你可以调整下面的内核选项。 
    - 在需要转发的服务器中，比如用作 NAT 网关的服务器或者使用 Docker 容器时，开启 IP 转发，即设置 net.ipv4.ip_forward = 1。 
    - 调整数据包的生存周期 TTL，比如设置 net.ipv4.ip_default_ttl = 64。注意，增大该值会 降低系统性能。 
    - 开启数据包的反向地址校验，比如设置 net.ipv4.conf.eth0.rp_filter = 1。这样可以防止 IP 欺骗，并减少伪造 IP 带来的 DDoS 问题。
  - 第二种，从分片的角度出发，最主要的是调整 MTU（Maximum Transmission Unit）的 大小。 
    - 通常，MTU 的大小应该根据以太网的标准来设置。以太网标准规定，一个网络帧最大为 1518B，那么去掉以太网头部的 18B 后，剩余的 1500 就是以太网 MTU 的大小。在使用 VXLAN、GRE 等叠加网络技术时，要注意，网络叠加会使原来的网络包变大，导致 MTU 也需要调整。 
    - 比如，就以 VXLAN 为例，它在原来报文的基础上，增加了 14B 的以太网头部、 8B 的 VXLAN 头部、8B 的 UDP 头部以及 20B 的 IP 头部。换句话说，每个包比原来增大了 50B。 
    - 所以，我们就需要把交换机、路由器等的 MTU，增大到 1550， 或者把 VXLAN 封包前 （比如虚拟化环境中的虚拟网卡）的 MTU 减小为 1450。
  - 第三种，从 ICMP 的角度出发，为了避免 ICMP 主机探测、ICMP Flood 等各种网络问 题，你可以通过内核选项，来限制 ICMP 的行为。 
    - 比如，你可以禁止 ICMP 协议，即设置 net.ipv4.icmp_echo_ignore_all = 1。这样，外 部主机就无法通过 ICMP 来探测主机。 
    - 或者，你还可以禁止广播 ICMP，即设置 net.ipv4.icmp_echo_ignore_broadcasts = 1。

## 服务吞吐量下降很厉害，怎么分析

- 测试一下，案例中 Nginx 的吞吐量。

  - ```shell
    1 # 默认测试时间为 10s，请求超时 2s
    2 $ wrk --latency -c 1000 http://192.168.0.30
    
     1910 requests in 10.10s, 573.56KB read
     Non-2xx or 3xx responses: 1910
    ```

  - 从 wrk 的结果中，你可以看到吞吐量（也就是每秒请求数）只有 189，并且所有 1910 个 请求收到的都是异常响应（非 2xx 或 3xx）。这些数据显然表明，吞吐量太低了，并且请 求处理都失败了。这是怎么回事呢？

  - 根据 wrk 输出的统计结果，我们可以看到，总共传输的数据量只有 573 KB，那就肯定不会 是带宽受限导致的。所以，我们应该从请求数的角度来分析。 分析请求数，特别是 HTTP 的请求数，有什么好思路吗？当然就要**从 TCP 连接数入手**。

- 连接数优化

  - 要查看 TCP 连接数的汇总情况，首选工具自然是 ss 命令。为了观察 wrk 测试时发生的问 题，我们在终端二中再次启动 wrk，并且把总的测试时间延长到 30 分钟

    - ```shell
      # 测试时间 30 分钟
      $ wrk --latency -c 1000 -d 1800 http://192.168.0.30
      
      #观察 TCP 连接数
      $ ss -s
      ```

    - 从这里看出，wrk 并发 1000 请求时，建立连接数只有 5，而 closed 和 timewait 状态的 连接则有 1100 多 。其实从这儿你就可以发现两个问题

      - 一个是建立连接数太少了； 
      - 另一个是 timewait 状态连接太多了

    - 分析问题，自然要先从相对简单的下手。我们先来看第二个关于 timewait 的问题。在之前 的 NAT 案例中，我已经提到过，内核中的连接跟踪模块，有可能会导致 timewait 问题。 我们今天的案例还是基于 Docker 运行，而 Docker 使用的 iptables ，就会使用**连接跟踪 模块**来管理 NAT。那么，怎么确认是不是连接跟踪导致的问题呢？

      - 其实，最简单的方法，就是通过 dmesg 查看系统日志，如果有连接跟踪出了问题，应该会 看到 nf_conntrack 相关的日志

        - $ dmesg | tail

        - 从日志中，你可以看到 nf_conntrack: table full, dropping packet 的错误日志。这说明， 正是连接跟踪导致的问题。 

        - 这种情况下，我们应该想起前面学过的两个内核选项——连接跟踪数的最大限制 nf_conntrack_max ，以及当前的连接跟踪数 nf_conntrack_count。

        - 这次的输出中，你可以看到最大的连接跟踪限制只有 200，并且全部被占用了。200 的限 制显然太小，不过相应的优化也很简单，调大就可以了。

        - ```shell
          # 我们执行下面的命令，将 nf_conntrack_max 增大：
          # 将连接跟踪限制增大到 1048576
           $ sysctl -w net.netfilter.nf_conntrack_max=1048576
          
          ```

      - 从 wrk 的输出中，你可以看到，连接跟踪的优化效果非常好，吞吐量已经从刚才的 189 增 大到了 5382。看起来性能提升了将近 30 倍

        - 别急，我们再来看看 wrk 汇报的其他数据。果然，10s 内的总请求数虽然增大到了 5 万， 但是有 4 万多响应异常，说白了，真正成功的只有 8000 多个（54221-45577=8644）

- 工作进程优化

  - 由于这些响应并非 Socket error，说明 Nginx 已经收到了请求，只不过，响应的状态码并 不是我们期望的 2xx （表示成功）或 3xx（表示重定向）。所以，这种情况下，搞清楚 Nginx 真正的响应就很重要了。 

    - ```shell
      $ docker logs nginx --tail 3
      
      
      ```

    - 从 Nginx 的日志中，我们可以看到，响应状态码为 499。499 并非标准的 HTTP 状态码，而是由 Nginx 扩展而来，表示服务器端还没来得及响应 时，客户端就已经关闭连接了。换句话说，问题在于服务器端处理太慢，客户端因为超时 （wrk 超时时间为 2s），主动断开了连接。

    - 既然问题出在了服务器端处理慢，而案例本身是 Nginx+PHP 的应用，那是不是可以猜 测，是因为 PHP 处理过慢呢

    - ```shell
      # 查询 PHP 容器日志
      $ docker logs phpfpm --tail 5
      
      
      ```

    - 从这个日志中，我们可以看到两条警告信息，server reached max_children setting (5)， 并建议增大 max_children。

      - 一般来说，每个 php-fpm 子进程可能会占用 20 MB 左右的内存。所以，你可以根据内存 和 CPU 个数，估算一个合理的值。这儿我把它设置成了 20，并将优化后的配置重新打包 成了 Docker 镜像。

      - ```shell
        # 停止旧的容器
        $ docker rm -f nginx phpfpm
        # 使用新镜像启动 Nginx 和 PHP
        $ docker run --name nginx --network host --privileged -itd feisky/nginx-tp:1
        $ docker run --name phpfpm --network host --privileged -itd feisky/php-fpm-tp:1
        
        
        ```

    - 从 wrk 的输出中，可以看到，虽然吞吐量只有 4683，比刚才的 5382 少了一些；但是测试 期间成功的请求数却多了不少，从原来的 8000，增长到了 15000（47210- 31692=15518）。 

- 套接字优化

  - 观察有没有发生套接字的丢包现象

    - ```shell
      1 # 只关注套接字统计
      2 $ netstat -s | grep socket
      
      
      ```

    - 根据两次统计结果中 socket overflowed 和 sockets dropped 的变化，你可以看到，有大 量的套接字丢包，并且丢包都是套接字队列溢出导致的。所以，接下来，我们应该分析连接 队列的大小是不是有异常。

  - 查看套接字的队列大小$ ss -ltnp

    - 这次可以看到，Nginx 和 php-fpm 的监听队列 （Send-Q）只有 10，而 nginx 的当前监 听队列长度 （Recv-Q）已经达到了最大值，php-fpm 也已经接近了最大值。很明显，套 接字监听队列的长度太小了，需要增大

  - 关于套接字监听队列长度的设置，既可以在应用程序中，通过套接字接口调整，也支持通过 内核选项来配置

    - ```shell
      1 # 查询 nginx 监听队列长度配置
      2 $ docker exec nginx cat /etc/nginx/nginx.conf | grep backlog
      3 listen 80 backlog=10;
      45 # 查询 php-fpm 监听队列长度
      6 $ docker exec phpfpm cat /opt/bitnami/php/etc/php-fpm.d/www.conf | grep backlog
      7 ; Set listen(2) backlog.
      8 ;listen.backlog = 511
      9
      10 # somaxconn 是系统级套接字监听队列上限
      11 $ sysctl net.core.somaxconn
      12 net.core.somaxconn = 10
      
      
      ```

    - 从输出中可以看到，Nginx 和 somaxconn 的配置都是 10，而 php-fpm 的配置也只有 511，显然都太小了。那么，优化方法就是增大这三个配置，比如，可以把 Nginx 和 phpfpm 的队列长度增大到 8192，而把 somaxconn 增大到 65536

    - 现在的吞吐量已经增大到了 6185，并且在测试的时候，如果你在终端一中重新执行 netstat -s | grep socket，还会发现，现在已经没有套接字丢包问题了

  - 不过，这次 Nginx 的响应，再一次全部失败了，都是 Non-2xx or 3xx

    - 你可以看到，Nginx 报出了无法连接 fastcgi 的错误，错误消息是 Connect 时， Cannot assign requested address。这个错误消息对应的错误代码为 EADDRNOTAVAIL，表示 IP 地址或者端口号不可用。 
    - 在这里，显然只能是端口号的问题

- 端口号优化

  - 我们执行下面的命令，就可以查询系统配置的临时端口号范围

    - ```shell
      1 $ sysctl net.ipv4.ip_local_port_range
      2 net.ipv4.ip_local_port_range=20000 20050
      
      ```

  - 你可以看到，临时端口的范围只有 50 个，显然太小了 。优化方法很容易想到，增大这个 范围就可以了。比如，你可以执行下面的命令，把端口号范围扩展为 “10000 65535”：

    - ```shell
      1 $ sysctl -w net.ipv4.ip_local_port_range="10000 65535"
      2 net.ipv4.ip_local_port_range = 10000 65535
      
      ```

  - 这次，异常的响应少多了 ，不过，吞吐量也下降到了 3208。并且，这次还出现了很多 Socket read errors。显然，还得进一步优化。

- 火焰图

  - 前面我们已经优化了很多配置。这些配置在优化网络的同时，却也会带来其他资源使用的上 升

  - 执行 top ，观察 CPU 和内存的使用

  - 从 top 的结果中可以看到，可用内存还是很充足的，但系统 CPU 使用率（sy）比较高，两 个 CPU 的系统 CPU 使用率都接近 50%，且空闲 CPU 使用率只有 2%。再看进程部分， CPU 主要被两个 Nginx 进程和两个 docker 相关的进程占用，使用率都是 30% 左右。

    - CPU 使用率上升了，该怎么进行分析呢？我想，你已经还记得我们多次用到的 perf，再配 合前两节讲过的火焰图，很容易就能找到系统中的热点函数。 

    - ```shell
      1 # 执行 perf 记录事件
      2 $ perf record -g
      34 # 切换到 FlameGraph 安装路径执行下面的命令生成火焰图
      
      ```

  - 根据我们讲过的火焰图原理，这个图应该从下往上、沿着调用栈中最宽的函数，来分析执行 次数最多的函数。 

  - 如果有大量连接 占用着端口，那么检查端口号可用的函数，不就会消耗更多的 CPU 吗

    - $ ss -s

    - 这回可以看到，有大量连接（这儿是 32768）处于 timewait 状态，而 timewait 状态的连 接，本身会继续占用端口号。如果这些端口号可以重用，那么自然就可以缩短 __init_check_established 的过程。而 Linux 内核中，恰好有一个 tcp_tw_reuse 选项，用 来控制端口号的重用。

      ```shell
      # 我们在终端一中，运行下面的命令，查询它的配置：
      
      1 $ sysctl net.ipv4.tcp_tw_reuse
      2 net.ipv4.tcp_tw_reuse = 0
      # 你可以看到，tcp_tw_reuse 是 0，也就是禁止状态。其实看到这里，我们就能理解，为什么临时端口号的分配会是系统运行的热点了。当然，优化方法也很容易，把它设置成 1 就可以开启了。
      
      
      ```

      



## **综合：分析性能问题的一般步骤**

- **系统资源瓶颈**
  - 使用率、饱和度以及错误数这三类指标来衡量。系统的资源，可以分为硬件资源和软件资源两 类。
    - 如 CPU、内存、磁盘和文件系统以及网络等，都是最常见的硬件资源。 
    - 而文件描述符数、连接跟踪数、套接字缓冲区大小等，则是典型的软件资源。
- **CPU** **性能分析**
  - ![img](https://img2018.cnblogs.com/blog/1075436/201909/1075436-20190925163759359-2129947612.png)
  - 比如说，当你收到系统的用户 CPU 使用率过高告警时，从监控系统中直接查询到，导致 CPU 使用率过高的进程；然后再登录到进程所在的 Linux 服务器中，分析该进程的行为。 
  - 你可以使用 strace，查看进程的系统调用汇总；也可以使用 perf 等工具，找出进程的热点 函数；甚至还可以使用动态追踪的方法，来观察进程的当前执行过程，直到确定瓶颈的根 源。
- **内存性能分析**
  - ![img](https://img2018.cnblogs.com/blog/1075436/201909/1075436-20190925164214302-575822803.png)
  - 比如说，当你收到内存不足的告警时，首先可以从监控系统中。找出占用内存最多的几个进 程。然后，再根据这些进程的内存占用历史，观察是否存在内存泄漏问题。
  - 确定出最可疑的 进程后，再登录到进程所在的 Linux 服务器中，分析该进程的内存空间或者内存分配，最 后弄清楚进程为什么会占用大量内存
- **磁盘和文件系统** **I/O** **性能分析**
  - ![img](https://img2018.cnblogs.com/blog/1075436/201909/1075436-20190925164238972-714799998.png)
  - 当你使用 iostat ，发现磁盘 I/O 存在性能瓶颈（比如 I/O 使用率过 高、响应时间过长或者等待队列长度突然增大等）后，可以再通过 pidstat、 vmstat 等， 确认 I/O 的来源。接着，再根据来源的不同，进一步分析文件系统和磁盘的使用率、缓存 以及进程的 I/O 等，从而揪出 I/O 问题的真凶
  - 比如说，当你发现某块磁盘的 I/O 使用率为 100% 时，首先可以从监控系统中，找出 I/O 最多的进程。然后，再登录到进程所在的 Linux 服务器中，借助 strace、lsof、perf 等工具，分析该进程的 I/O 行为。最后，再结合应用程序的原理，找出大量 I/O 的原因。 
- **网络性能分析**
  - 最后的网络性能，其实包含两类资源，即网络接口和内核资源
  - 而要分析网络的性能，自然也是要从这几个协议层入手，通过使用率、饱和度以及错误数这 几类性能指标，观察是否存在性能问题。比如 ： 
    - 在链路层，可以从网络接口的吞吐量、丢包、错误以及软中断和网络功能卸载等角度分 析； 
    - 在网络层，可以从路由、分片、叠加网络等角度进行分析； 
    - 在传输层，可以从 TCP、UDP 的协议原理出发，从连接数、吞吐量、延迟、重传等角度 进行分析；在应用层，可以从应用层协议（如 HTTP 和 DNS）、请求数（QPS）、套接字缓存等角 度进行分析。 
  - 比如，当你收到网络不通的告警时，就可以从监控系统中，查找各个协议层的丢包指标，确 认丢包所在的协议层。然后，从监控系统的数据中，确认网络带宽、缓冲区、连接跟踪数等 软硬件，是否存在性能瓶颈。最后，再登录到发生问题的 Linux 服务器中，借助 netstat、 tcpdump、bcc 等工具，分析网络的收发数据，并且结合内核中的网络选项以及 TCP 等网 络协议的原理，找出问题的来源
- **应用程序瓶颈**
  - 除了以上这些来自网络资源的瓶颈外，还有很多瓶颈，其实直接来自应用程序。比如，最典 型的应用程序性能问题，就是吞吐量（并发请求数）下降、错误率升高以及响应时间增大。 
  - 不过，在我看来，这些应用程序性能问题虽然各种各样，但就其本质来源，实际上只有三 种，也就是资源瓶颈、依赖服务瓶颈以及应用自身的瓶颈。 
    - 第一种**资源瓶颈**，其实还是指刚才提到的 CPU、内存、磁盘和文件系统 I/O、网络以及内 核资源等各类软硬件资源出现了瓶颈，从而导致应用程序的运行受限。对于这种情况，我们 就可以用前面系统资源瓶颈模块提到的各种方法来分析。 
    - 第二种**依赖服务的瓶颈**，也就是诸如数据库、分布式缓存、中间件等应用程序，直接或者间 接调用的服务出现了性能问题，从而导致应用程序的响应变慢，或者错误率升高。这说白了 就是跨应用的性能问题，使用全链路跟踪系统，就可以帮你快速定位这类问题的根源。 
    - 最后一种，**应用程序自身的性能问题**，包括了多线程处理不当、死锁、业务算法的复杂度过 高等等。对于这类问题，在我们前面讲过的应用程序指标监控以及日志监控中，观察关键环 节的耗时和内部执行过程中的错误，就可以帮你缩小问题的范围。 
  - 不过，由于这是应用程序内部的状态，外部通常不能直接获取详细的性能数据，所以就需要 应用程序在设计和开发时，就提供出这些指标，以便监控系统可以了解应用程序的内部运行状态。 
  - 如果这些手段过后还是无法找出瓶颈，你还可以用系统资源模块提到的各类进程分析工具， 来进行分析定位。比如： 
    - 你可以用 strace，观察系统调用； 
    - 使用 perf 和火焰图，分析热点函数； 
    - 甚至使用动态追踪技术，来分析进程的执行状态。
  - 当然，系统资源和应用程序本来就是相互影响、相辅相成的一个整体。实际上，很多资源瓶 颈，也是应用程序自身运行导致的。比如，进程的内存泄漏，会导致系统内存不足；进程过 多的 I/O 请求，会拖慢整个系统的 I/O 请求等。



## **综合：优化性能问题的一般方法**

- 我们可以从系统资源瓶颈和应用程序瓶颈，这两个角度来分析性能问题的根源

  - 从系统资源瓶颈的角度来说，USE 法是最为有效的方法，即从使用率、饱和度以及错误数 这三个方面，来分析 CPU、内存、磁盘和文件系统 I/O、网络以及内核资源限制等各类软硬件资源。
  - 从应用程序瓶颈的角度来说，可以把性能问题的来源，分为资源瓶颈、依赖服务瓶颈以及应 用自身的瓶颈这三类。 
  - 资源瓶颈的分析思路，跟系统资源瓶颈是一样的。 
  - 依赖服务的瓶颈，可以使用全链路跟踪系统，进行快速定位。 
  - 而应用自身的问题，则可以通过系统调用、热点函数，或者应用自身的指标和日志等，进 行分析定位。 

- **CPU** **优化**

  - CPU 性能优化的核心，在于排除所有不必要的工作、充分利用 CPU 缓存并减少进程调度对性能的*影响
  - 最典型 的三种优化方法。
  - 第一种，把进程绑定到一个或者多个 CPU 上，充分利用 CPU 缓存的本地性，并减少进 程间的相互影响。 
  - 第二种，为中断处理程序开启多 CPU 负载均衡，以便在发生大量中断时，可以充分利用多 CPU 的优势分摊负载。 
  - 第三种，使用 Cgroups 等方法，为进程设置资源限制，避免个别进程消耗过多的 CPU。 同时，为核心应用程序设置更高的优先级，减少低优先级任务的影响。 

- **内存优化**

  - 内存问题，比如可用内存不足、内存泄漏、 Swap 过多、缺页异常过多以及缓存过多等等。所以，说白了，内存性能的优化，也就是要 解决这些内存使用的问题。
  - 你可以通过以下几种方法
    - 第一种，除非有必要，Swap 应该禁止掉。这样就可以避免 Swap 的额外 I/O ，带来内 存访问变慢的问题。 
    - 第二种，使用 Cgroups 等方法，为进程设置内存限制。这样就可以避免个别进程消耗过多内存，而影响了其他进程。对于核心应用，还应该降低 oom_score，避免被 OOM 杀 死。 
    - 第三种，使用大页、内存池等方法，减少内存的动态分配，从而减少缺页异常。

- **磁盘和文件系统** **I/O** **优化**

  - 三种最典型的方 法。
    - 第一种，也是最简单的方法，通过 SSD 替代 HDD、或者使用 RAID 等方法，提升 I/O性能。 
    - 第二种，针对磁盘和应用程序 I/O 模式的特征，选择最适合的 I/O 调度算法。比如， SSD 和虚拟机中的磁盘，通常用的是 noop 调度算法；而数据库应用，更推荐使用deadline 算法。 
    - 第三，优化文件系统和磁盘的缓存、缓冲区，比如优化脏页的刷新频率、脏页限额，以及 内核回收目录项缓存和索引节点缓存的倾向等等。
  - 除此之外，使用不同磁盘隔离不同应用的数据、优化文件系统的配置选项、优化磁盘预读、 增大磁盘队列长度等，也都是常用的优化思路。

- **网络优化**

  - 针对每个协议层的工作原理 进行优化。这里，我同样强调一下，最典型的几种网络优化方法。
    - 首先，从内核资源和网络协议的角度来说，我们可以对内核选项进行优化，比如： 
      - 你可以增大套接字缓冲区、连接跟踪表、最大半连接数、最大文件描述符数、本地端口范 围等内核资源配额； 
      - 也可以减少 TIMEOUT 超时时间、SYN+ACK 重传数、Keepalive 探测时间等异常处理 参数； 
      - 还可以开启端口复用、反向地址校验，并调整 MTU 大小等降低内核的负担。 
    - 其次，从网络接口的角度来说，我们可以考虑对网络接口的功能进行优化，比如： 
      - 你可以将原来 CPU 上执行的工作，卸载到网卡中执行，即开启网卡的 GRO、GSO、 RSS、VXLAN 等卸载功能； 
      - 也可以开启网络接口的多队列功能，这样，每个队列就可以用不同的中断号，调度到不同 CPU 上执行； 
      - 还可以增大网络接口的缓冲区大小以及队列长度等，提升网络传输的吞吐量。

- **应用程序优化**

  - 第一个例子，是系统 CPU 使用率（sys%）过高的问题。有时候出现问题，虽然表面现象是系统 CPU 使用率过高，但待你分析过后，很可能会发现，应用程序的不合理系统调用才是罪魁祸首。这种情况下，优化应用程序内部系统调用的逻辑，显然要比优化内核要简单也有 用得多。 
  - 再比如说，数据库的 CPU 使用率高、I/O 响应慢，也是最常见的一种性能问题。这种问 题，一般来说，并不是因为数据库本身性能不好，而是应用程序不合理的表结构或者 SQL查询语句导致的。这时候，优化应用程序中数据库表结构的逻辑或者 SQL 语句，显然要比优化数据库本身，能带来更大的收益。
  - 所以，在观察性能指标时，你应该先查看**应用程序的响应时间、吞吐量以及错误率**等指标，因为它们才是性能优化要解决的终极问题。以终为始，从这些角度出发，你一定能想到很多优化方法，而我比较推荐下面几种方法。
    - 第一，从 CPU 使用的角度来说，简化代码、优化算法、异步处理以及编译器优化等，都 是常用的降低 CPU 使用率的方法，这样可以利用有限的 CPU 处理更多的请求。 
    - 第二，从数据访问的角度来说，使用缓存、写时复制、增加 I/O 尺寸等，都是常用的减 少磁盘 I/O 的方法，这样可以获得更快的数据处理速度。
    - 第三，从内存管理的角度来说，使用大页、内存池等方法，可以预先分配内存，减少内存的动态分配，从而更好地内存访问性能。 
    - 第四，从网络的角度来说，使用 I/O 多路复用、长连接代替短连接、DNS 缓存等方法，可以优化网络 I/O 并减少网络请求数，从而减少网络延时带来的性能问题。 
    - 第五，从进程的工作模型来说，异步处理、多线程或多进程等，可以充分利用每一个 CPU 的处理能力，从而提高应用程序的吞吐能力。 
    - 除此之外，你还可以使用消息队列、CDN、负载均衡等各种方法，来优化应用程序的架构，将原来单机要承担的任务，调度到多台服务器中并行处理。这样也往往能获得更好的整体性能

  
  
  

