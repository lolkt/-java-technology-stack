



##  **到底什么是微服务**

- 解决单体应用架构，**服务化**的思想也就应运而生
  - 服务化就是把 传统的单机应用中通过 JAR 包依赖产生的本地方法调用，改造成通过 RPC 接口产生的远程方法调用。
  - 以微博系统为例，微博既包含了内容模块，也包含了消息模块和用户模块等。为此，首先可以把用户模块从单体应用中拆分出来，独立成一个服务部署，以 RPC 接口的形式对外提供服务。
- 得益于以 Docker 为代表的容器化技术的成熟以及 DevOps 文化的兴起，服务化的思想进一步演化，演变为今天我们所熟知的微服务
  - 集开发、测试和运维三者 角色于一体
  - 而要实现 DevOps，就必须开发完成代码开发后，能自动进行测试，测试通过后，能自动发布到线上。对应的这两个过程就是 CI 和 CD，具体来讲就是
    - CI（Continuous Integration），持续集成。开发完成代码开发后，能自动地进行代码检查、单元测试、打包部署到测试环境，进行集成测试，跑自动化测试用例。
    - CD（Continuous Deploy），持续部署。代码测试通过后，能自动部署到类生产环境中进行集成测试，测试通过后再进行小流量的灰度验证，验证通过后代码就达到线上发布的要求了，就可以把代码自动部署到线上。
  - DevOps要解决的还是软件研发交付能力和业务需求快速多变之间的矛盾。

- 微服务相比于服务化又有什么不同呢
  - **服务拆分粒度更细**。微服务可以说是更细维度的服务化，小到一个子模块，只要该模块依赖的资源与其他模块都没有关系，那么就可以拆分为一个微服务。 
    - 服务化拆分最好不要涉及跨数据库表交互
    - 拆分微服务先要把业务梳理清 楚，做到心中有数，梳理清楚了那么业务的边界自然清晰了，自然而然对应拆分哪些服务也出来了
  - **服务独立部署**。每个微服务都严格遵循独立打包部署的准则，互不影响。比如一台物理机上可以部署多个 Docker 实例，每个 Docker 实例可以部署一个微服务的代码。 
  - **服务独立维护**。每个微服务都可以交由一个小团队甚至个人来开发、测试、发布和运维，并对整个生命周期负责。
  - **服务治理能力要求高**。因为拆分为微服务之后，服务的数量变多，因此需要有统一的服务治理平台，来对各个服务进行管理。 





## **从单体应用走向服务化**

- 什么时候进行服务化拆分
  - 一旦单体应用同时进行开发的人员超过 10 人，这个时候就该考虑进行服务化拆分了。

- 服务化拆分的两种姿势
  - **纵向拆分**，是从业务维度进行拆分。
  - **横向拆分**，是从公共且独立功能维度拆分。标准是按照是否有公共的被多个其他服务调用，且依赖的资源独立不与其他业务耦合。 
    - 以前面提到的社交 App 举例，无论是首页信息流、评论、消息箱还是个人主页，都需要显示用户的昵称。假如用户的昵称功能有产品需求的变更，你需要上线几乎所有的服务， 这个成本就有点高了。显而易见，如果我把用户的昵称功能单独部署成一个独立的服务，那 么有什么变更我只需要上线这个服务即可，其他服务不受影响，开发和上线成本就大大降低 了。 

- 服务化拆分的前置条件

  - **服务如何定义**

    - 对于微服务来说，每个服务都运行在各自的进程之中，应该以何种形式向外界传达自己的信息呢？

    - 答案就是接口，无论采用哪种通讯协议，是 HTTP 还是 RPC，服务之间的调用都通过接口描述来约定，约定内容包括接口名、接口参数以及接口返回值。
    - 常用的服务描述方式包括 RESTful API、XML 配置以及 IDL 文件三种。

  - **服务如何发布和订阅**

    - 单体应用由于部署在同一个 WAR 包里，接口之间的调用属于进程内的调用。
    - 拆分为微服务独立部署后，这个时候你就需要一个类似登记处的地方，能够记录每个服务提供者的地址以供服务调用者查询，在微服务架构里，这个地方就是注册中心。

  - **服务框架**

    - 通过注册中心，服务消费者就可以获取到服务提供者的地址，有了地址后就可以发起调用。但在发起调用之前你还需要解决以下几个问题。 
      - 服务通信采用什么协议、数据传输采用什么方式、数据压缩采用什么格式

  - **服务如何监控**

    - 这时候你就需要一种通用的监控方案，能够覆盖业务埋点、数据收集、数据处理，最后到数据展示的全链路功能。 

  - **服务如何治理**

    - 可以设定一个调用性能阈值，如果一段时间内一直超过这个值，那么依赖服务的调用可以直接返回，这就是熔断，也是服务治理最常用的手段之一。

  - **故障如何定位**

    - 拆分为微服务之后，一次用户调用可能依赖多个服务，每个服 务又部署在不同的节点上，如果用户调用出现问题，你需要有一种解决方案能够将一次用 户请求进行标记，并在多个依赖的服务系统中继续传递，以便串联所有路径，从而进行故障定位。 
    - 服务追踪的工作原理大致如下
      - 服务消费者发起调用前，会在本地按照一定的规则生成一个 requestid，发起调用时，将requestid 当作请求参数的一部分，传递给服务提供者。 
      - 服务提供者接收到请求后，记录下这次请求的 requestid，然后处理请求。如果服务提供 者继续请求其他服务，会在本地再生成一个自己的 requestid，然后把这两个 requestid都当作请求参数继续往下传递。

- 对拆分的考量：

  - 一是业务维度聚类，业务和数据关系密切的应该放在一起。
  - 二是功能维度聚类，公共功能聚合为一个服务。
  - 三是人员聚类，这是个实际中的考量，如果某几个业务就是这几个人比较熟，那么最好放在一起，未来开发部署都好办。
  - 四是性能聚类，性能要求高的并发大的和性能要求低的并发小的，要分开为不同的服务，这样部署和运行都独立，好维护。

- 必须要解决的一个问题是 拆分后带来的跨服务事务的一致性问题









##  **服务端出现故障时该如何应对**

- 微服务系统可能出现故障的种类，主要有三种故障
  - 集群故障
  - 单 IDC 故障
  - 单机故障
- 应付集群故障的思路，主要有两种：**限流**和**降级**。
  - 一般选择工作线程数来作为限流的指标，给系统设置一个总的最大工作线程数以及单个服务的最大工作线程数
  - 降级一般是如何实现的呢， 一种可行的方案是通过开关来实现。
    - 开关一般用在两种地方，一种是新增的业务逻辑
    - 另一种是依赖的服务或资 源，因为依赖的服务或者资源不总是可靠的
- 在现实情况下，整个 IDC 脱网的事情时有发生，多半是因为不可抗力比如机房着火、光缆被挖断等
  - 国内大部分的互联网业务多采用多 IDC 部署
    - 采用多 IDC 部署的最大好处就是当有一个 IDC 发生故障时，可以把原来访问故障 IDC 的流量切换到正常的 IDC，来保证业务的正常访问。
  - 流量切换的方式一般有两种，
    - 基于 DNS 解析的流量切换
    - 基于 RPC 分组的流量切换
- 处理单机故障一个有效的办法就是自动重启。
  - 具体来讲，你可以设置一个阈值，比如以某个接口的平均耗时为准，当监控单机上某个接口的平均耗时超过一定阈值时，就认为这台机器有问题，这个时候就需要把有问题的机器从线上集群中摘除掉，然后在重启服务后，重新加入到集群中。 
  - 不过这里要注意的是，需要防止网络抖动造成的接口超时从而触发自动重启。一种方法是在收集单机接口耗时数据时，多采集几个点，比如每 10s 采集一个点，采集 5 个点，当 5 个点中有超过 3 个点的数据都超过设定的阈值范围，才认为是真正的单机问题，这时会触发自动重启策略。
  - 除此之外，为了防止某些特殊情况下，短时间内被重启的单机过多，造成整个服务池可用节 点数太少，最好是设置一个可重启的单机数量占整个集群的最大比例，一般这个比例不要超过 10%，因为正常情况下，不大可能有超过 10% 的单机都出现故障。







##  **微服务为什么要容器化**

- 保证开发人员将自己本地部署测试通过的代码和运行环境，能够复制到测试环境中去，测试通过后再复制到线上环境进行发布

- 利用 Docker 镜像的**分层机制**，在每一层通过编写 Dockerfile 文件来逐层打包镜像。

  - 微博的 Docker 镜像大致分为四层
    - 基础环境层。这一层定义操作系统运行的版本、时区、语言、yum 源、TERM 等。 
    - 运行时环境层。这一层定义了业务代码的运行时环境，比如 Java 代码的运行时环境 JDK的版本。 
    - Web 容器层。这一层定义了业务代码运行的容器的配置，比如 Tomcat 容器的 JVM 参数。 
    - 业务代码层。这一层定义了实际的业务代码的版本，比如是 V4 业务还是 blossom 业务

  





## **微服务容器化运维：镜像仓库和资源调度**

- Docker 容器运行依托的是 Docker 镜像，也就是说要发布服务，首先必须把镜像发布到各个机器上去，这个时候问题就来了，这个镜像该放在哪？如何把镜像发布到各个机器上去？这时候你就要依靠**镜像仓库**了。 

  - 权限控制
  - 镜像同步
  - 高可用性

- 解决了 Docker 镜像存储和访问的问题后，新问题又随之而来了，Docker 镜像要分发到哪些机器上去？这些机器是从哪里来的？这其实涉及的是**资源调度**的问题。 

  - 物理机集群
  - 虚拟机集群
  - 公有云集群

  





## **微服务容器化运维：容器调度和服务编排**

- Kubernetes 解决方案在容器调度、服务编排方面都有成熟的组件，并且经过大业务量的实际验证。









## 下一代微服务架构Service Mesh

- 概念
  - Service Mesh 是一种新型的用于处理服务与服务之间通信的技术，尤其适用以云原生应用形式部署的服务，能够保证服务与服务之间调用的可靠性。
  - 在实际部署时，Service Mesh 通常以轻量级的网络代理的方式跟应用的代码部署在一起，从而以应用无感知的方式实现服务治理。
  - 而对于云原生应用来说，可以在每个服务部署的实例上，都同等的部署一个 Linkerd（框架） 实例。服务 A 要想调用服务 B，首先调用本地的 Linkerd 实例，经过本地的 Linked 实例转发给服务 B 所在节点上的 Linkerd 实例，最后再由服务 B 本地的Linkerd 实例把请求转发给服务 B。这样的话，所有的服务调用都得经过 Linkerd 进行代理 转发，所有的 Linkerd 组合起来就像一个网格一样，这也是为什么我们把这项技术称为Service Mesh，也就是“服务网格”的原因

- Service Mesh 的实现原理
  - Service Mesh 实现的关键就在于两点：
    - 一个是上面提到的轻量级的网络代理也叫 SideCar，它的作用就是转发服务之间的调用
      - 在 Service Mesh 架构中，服务框架的功能都集中实现在 SideCar 里，并在每一个服务消费者和服务提供者的本地都部署一个 SideCar，服务消费者和服务提供者只管自己的业务实现，服务消费者向本地的 SideCar 发起请求，本地的 SideCar 根据请求的路径向注册中 心查询，得到服务提供者的可用节点列表后，再根据负载均衡策略选择一个服务提供者节 点，并向这个节点上的 SideCar 转发请求，服务提供者节点上的 SideCar 完成流量统计、限流等功能后，再把请求转发给本地部署的服务提供者进程，从而完成一次服务请求。
    - 一个是基于 SideCar 的服务治理也被叫作 Control Plane，它的作用是向 SideCar 发送各种指令，以完成各种服务治理功能
      - 既然 SideCar 能实现服务之间的调用拦截功能，那么服务之间的所有流量都可以通过 SideCar 来转发，这样的话所有的 SideCar 就组成了一个服务网格，再通过一个统一的地方与各个 SideCar 交互，就能控制网格中流量的运转了，这个统一的地方就在 Sevice Mesh 中就被称为 Control Plane。
      - 服务发现
      - 负载均衡
      - 请求路由
      - 故障处理
      - 安全认证
      - 监控上报
      - 日志记录
      - 配额控制
-  Istio
  - 它是采用模块化设计，并且各个模块之间高 度解耦，Proxy 专注于负责服务之间的通信，Pilot 专注于流量控制，Mixer 专注于策略控制以及监控日志功能，而 Citadel 专注于安全。

















