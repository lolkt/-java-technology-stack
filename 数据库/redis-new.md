# 前言

- 相关的性能问题

  - **为了保证数据的可靠性**，Redis 需要在磁盘上读写 AOF 和 RDB，但在高并发场景里，这就会直接带来两个新问题:一个是写 AOF 和 RDB 会造成 Redis 性能抖动，另一个是 Redis 集群数据同步和实例恢复时，读 RDB 比较 慢，限制了同步和恢复速度。
  - 一个可行的解决方 案就是使用非易失内存 NVM，因为它既能保证高速的读写，又能快速持久化数据。

- 遇见的“坑”，总体来说集中在四个方面:

  - CPU 使用上的“坑”，例如数据结构的复杂度、跨 CPU 核的访问; 
  - 内存使用上的“坑”，例如主从同步和 AOF 的内存竞争; 
  - 存储持久化上的“坑”，例如在 SSD 上做快照的性能抖动;
  -  网络通信上的“坑”，例如多实例时的异常网络丢包。

- Redis 知识全景图

  - 应用维度：缓存、集群、数据结构
  - 系统维度：高性能主线、高可靠主线、高可扩展主线
    - 处理层：线程模型、主从复制、数据分片
    - 内存层：数据结构、哨兵机制、——
    - 存储层：持久化、——、负载均衡
    - 网络层：epoll网络框架、——、——

- 知识体系

  - 在应用维度上，我建议你按照两种方式学习: “**应用场景驱动**”和“**典型案例驱 动**”，一个是“面”的梳理，一个是“点”的掌握。
    - 我们知道，缓存和集群是 Redis 的两大广泛的应用场景。在这些场景中，本身就具有一条 显式的技术链。比如说，提到缓存场景，你肯定会想到缓存机制、缓存替换、缓存异常等 一连串的问题。
    - 可以用“典型案例驱动”的方式学习。我们可以重点解读一些对 Redis 的“三高”特性影响较大的使用案例，例如，多家大厂在万亿级访问量和万亿级数据量的 情况下对 Redis 的深度优化，解读这些优化实践，非常有助于你透彻地理解 Redis。而 且，你还可以梳理一些方法论，做成 Checklist，就像是一个个锦囊，之后当你遇到问题的 时候，就可以随时拿出自己的“锦囊妙计”解决问题了。
  - Redis 的问题画像
    - ![img](https://static001.geekbang.org/resource/image/70/b4/70a5bc1ddc9e3579a2fcb8a5d44118b4.jpeg)

  

  

  

# **基本架构**

- **Redis 能够在实际业务场景中得到广泛的应用，就是得益于支持多样化类型的 value**。
- 大体来说，一个键值数据库包括了**访问框架、索引模块、操作模块和存储模块**四部分
  - PUT hello world：键值数据库网络框架接收到网络包，并按照相应的协议进行解析之后，就可以知道，客户端想写入一个键值对，并开始实际的写入流程。此时，我们会遇到一个系统设计上的问 题，简单来说，就是网络连接的处理、网络请求的解析，以及数据存取的处理，是用一个线程、多个线程，还是多个进程来交互处理呢?该如何进行设计和取舍呢?我们一般把这 个问题称为 **I/O 模型设计**。不同的 I/O 模型对键值数据库的性能和可扩展性会有不同的影 响。
  - **索引的作用是让 键值数据库根据 key 找到相应 value 的存储位置，进而执行操作**。
    - 索引的类型有很多，常见的有哈希表、B+ 树、字典树等。不同的索引结构在性能、空间消 耗、并发控制等方面具有不同的特征。
    - 一般而言，内存键值数据库(例如 Redis)采用**哈希表**作为索引，很大一部分原因在于， 其键值数据基本都是保存在内存中的，而内存的高性能随机访问特性可以很好地与哈希表 O(1) 的操作复杂度相匹配。
    - 对于 Redis 而言，很有意思的一点是，它的 value 支持多种类型，当我们通过索引找到一个 key 所对应的 value 后，仍然需要从 value 的复杂结构(例如集合和列表)中进一步找到 我们实际需要的数据，这个操作的效率本身就依赖于它们的实现结构。
    - **Redis 采用一些常见的高效索引结构作为某些 value 类型的底层数据结构，这一技术路线 为 Redis 实现高性能访问提供了良好的支撑。**
  - 对于 PUT 和 DELETE 两种操作来说，除了新写入和删除键值对，还 需要分配和释放内存。
  - Redis 也提供了持久化功能。不过，为了适应不同的业务场景，Redis 为持久化提供了诸多的执行机制和优化改进





# **数据结构**

- 重要的表现:它接收到一个键值对操作后， 能以**微秒级别**的速度找到数据，并快速完成操作。

  - 为啥 Redis 能有这么突出的表现呢?一方面，这是因为它是内存数据库， 所有操作都在**内存**上完成，内存的访问速度本身就很快。另一方面，这要归功于它的**数据结构**。

- String 类型的底层实现只有一种数据结构，也就是简单动态字符串。而 List、 Hash、Set 和 Sorted Set 这四种数据类型，都有两种底层实现结构。通常情况下，我们会 把这四种类型称为集合类型，它们的特点是**一个键对应了一个集合的数据**。

- **键和值用什么结构组织?**

  - **为了实现从键到值的快速访问，Redis 使用了一个哈希表来保存所有键值对。**
    - 因为这个哈希表保存了所有的键值对，所以，我也把它称为**全局哈希表**。哈希表的最大好 处很明显，就是让我们可以用 **O(1)** 的时间复杂度来快速查找到键值对——我们只需要计算 键的哈希值，就可以知道它所对应的哈希桶位置，然后就可以访问相应的 entry 元素
    - 一个哈希表，其实就是一个数组，数组的每个元素称为一个哈希桶。所以，我们常说，**一个哈希表是由多个哈希桶组成的，每个哈希桶中保存了键值对数据。**
    - **哈希桶中的元素保存的并不是值本身，而是指向具体值的指针。**
  - 但是，如果你只是了解了哈希表的 O(1) 复杂度和快速查找特性，那么，当你往 Redis 中 写入大量数据后，就可能发现操作有时候会突然变慢了。这其实是因为你忽略了一个潜在 的风险点，那就是**哈希表的冲突问题和 rehash 可能带来的操作阻塞。**

- **为什么哈希表操作变慢了**?

  - Redis 解决哈希冲突的方式，就是链式哈希。链式哈希也很容易理解，就是指**同一个哈希 桶中的多个元素用一个链表来保存，它们之间依次用指针连接**。

  - **哈希冲突**可能也会越来越多，这就会导致某些哈希冲突链 过长，进而导致这个链上的元素查找耗时长，效率降低。对于追求“快”的 Redis 来说， 这是不太能接受的。

  - 所以，Redis 会对哈希表做 **rehash 操作**。rehash 也就是增加现有的哈希桶数量，让逐渐 增多的 entry 元素能在更多的桶之间分散保存，减少单个桶中的元素数量，从而减少单个 桶中的冲突。

    - **为了使 rehash 操作更高效，Redis 默认使用了两个全局哈希表**

      - ```
        1. 给哈希表 2 分配更大的空间，例如是当前哈希表 1 大小的两倍; 2. 把哈希表 1 中的数据重新映射并拷贝到哈希表 2 中;
        3. 释放哈希表 1 的空间。
        ```

      - 这个过程看似简单，但是第二步涉及大量的数据拷贝，如果一次性把哈希表 1 中的数据都 迁移完，会造成 Redis 线程阻塞，无法服务其他请求。此时，Redis 就无法快速访问数据 了。

    - 为了避免这个问题，Redis 采用了**渐进式 rehash**。

      - 简单来说就是在第二步拷贝数据时，Redis 仍然正常处理客户端请求，每处理一个请求 时，从哈希表 1 中的第一个索引位置开始，顺带着将这个索引位置上的所有 entries 拷贝 到哈希表 2 中;等处理下一个请求时，再顺带拷贝哈希表 1 中的下一个索引位置的 entries。
      - 这样就巧妙地把一次性大量拷贝的开销，分摊到了多次处理请求的过程中，避免了耗时操作，保证了数据的快速访问。

- **有哪些底层数据结构?**

  -       a，string：简单动态字符串
          b，list：双向链表，压缩列表
          c，hash：压缩列表，哈希表
          d，Sorted Set：压缩列表，跳表
          e，set：哈希表，整数数组

  - 集合类型的底层数据结构主要有 5 种:整数数组、双向链表、哈 希表、压缩列表和跳表。

    - **压缩列表实际上类似于一个数组**，数组中的每一个元素都对应保存一个数据。和数组不同 的是，压缩列表在表头有三个字段 zlbytes、zltail 和 zllen，分别表示列表长度、列表尾的 偏移量和列表中的 entry 个数;
    - 压缩列表在表尾还有一个 zlend，表示列表结束。
    - 在压缩列表中，如果我们要查找定位第一个元素和最后一个元素，可以通过表头三个字段 的长度直接定位，复杂度是 O(1)。而查找其他元素时，就没有这么高效了，只能逐个查 找，此时的复杂度就是 O(N) 了。

  - 有序链表只能逐一查找元素，导致操作起来非常缓慢，于是就出现了跳表。具体来说，跳 表在链表的基础上，**增加了多级索引，通过索引位置的几个跳转，实现数据的快速定位。O(logn)**

- **不同操作的复杂度**

  - 单元素操作是基础;
  - 范围操作非常耗时;
  - 统计操作（是指**集合类型对集合中所有元素个数的记录**）通常高效;
    - 例如 LLEN 和 SCARD。这 类操作复杂度只有 O(1)，这是因为当集合类型采用压缩列表、双向链表、整数数组这些数 据结构时，这些结构中专门记录了元素的个数统计，因此可以高效地完成相关操作。
  - 例外情况只有几个。
    - 是指某些数据结构的特殊记录，例如**压缩列表和双向链表都会记录表头 和表尾的偏移量**。这样一来，对于 List 类型的 LPOP、RPOP、LPUSH、RPUSH 这四个操 作来说，它们是在列表的头尾增删元素，这就可以通过偏移量直接定位，所以它们的复杂 度也只有 O(1)，可以实现快速操作。

- 整数数组和压缩列表在查找时间复杂度方面并没有很大的优势，那为什么 Redis 还会把它 们作为底层数据结构呢?

  - **内存利用率**，数组和压缩列表都是非常紧凑的数据结构，它比链表占用的内存要更少。Redis是内存数据库，大量数据存到内存中，此时需要做尽可能的优化，提高内存的利用率。
  - **数组对CPU高速缓存支持更友好**，所以Redis在设计时，集合数据元素较少情况下，默认采用内存紧凑排列的方式存储，同时利用CPU高速缓存不会降低访问速度。当数据元素超过设定阈值后，避免查询时间复杂度太高，转为哈希和跳表数据结构存储，保证查询效率。





# 高性能IO模型

- 我们通常说，Redis 是单线程，主要是指 **Redis 的网络 IO 和键值对读写是由一个线程来完成的，这也是 Redis 对外提供键值存储服务的主要流程**。 但 Redis 的其他功能，比如持久化、异步删除、集群数据同步等，其实是由额外的线程执 行的。

- **Redis为什么用单线程?**

  - **多线程的开销**
    - **我们刚开始增加线程数时，系统吞吐率会增加，但是，再进一步增加线程时，系统吞吐率就增长迟缓了，有时甚至还会出现下降的情况。**一个关键的瓶颈在于，系统中通常会存在被多线程同时访问的共享资源，比如一个共享的数据结构。当有多个线程要修改这个共享资源时，为了保证共享资源的正确性，就需要有额外的机制进行保证，而这个额外的机制，就会带来额外的开销。
    - **多线程编程模式面临的共享资源的 并发访问控制问题**。
    - 采用多线程开发一般会引入同步原语来保护共享资源的并发访问，这也会降低系统 代码的易调试性和可维护性。为了避免这些问题，Redis 直接采用了单线程模式。
  - **单线程** **Redis** **为什么那么快?**
    - Redis 却能使用单线程模型达到每 秒数十万级别的处理能力
    - 一方面，Redis 的大部分操作在**内存**上完成，再加上它采用了高效的数据结构，例如哈希 表和跳表，这是它实现高性能的一个重要原因。另一方面，就是 Redis 采用了**多路复用机制**，使其在网络 IO 操作中能并发处理大量的客户端请求，实现高吞吐率。

- **基本** **IO** **模型与阻塞点**

  - 以 Get 请求为例，SimpleKV 为了处理一个 Get 请求，需要监听客户端请求 (bind/listen)，和客户端建立连接(accept)，从 socket 中读取请求(recv)，解析 客户端发送请求(parse)，根据请求类型读取键值数据(get)，最后给客户端返回结 果，即向 socket 中写回数据(send)。
    - **但是，在这里的网络 IO 操作中，有潜在的阻塞点，分别是 accept() 和 recv()。**当 Redis 监听到一个客户端有连接请求，但一直未能成功建立起连接时，会阻塞在 accept() 函数这 里，导致其他客户端无法和 Redis 建立连接。类似的，当 Redis 通过 recv() 从一个客户端 读取数据时，如果数据一直没有到达，Redis 也会一直阻塞在 recv()。
  - **非阻塞模式**
    - Socket 网络模型的非阻塞模式设置，主要体现在三个关键的函数调用上
    - 在 socket 模型中，不同操作调用后会返回不同的套接字类型。
      - socket() 方法会返回**主动套接字**
      - 然后调用 listen() 方法，将主动套接字转化为监听套接字，此时，可以监听来自客户 端的连接请求。
      - 最后，调用 accept() 方法接收到达的客户端连接，并返回**已连接套接字。**
    - 针对监听套接字，我们可以设置非阻塞模式:当 Redis 调用 accept() 但一直未有连接请求 到达时，Redis 线程可以返回处理其他操作，而不用一直等待。但是，你要注意的是，调用 accept() 时，已经存在监听套接字了。
    - 虽然 Redis 线程可以不用继续等待，但是总得有机制继续在监听套接字上等待后续连接请 求，并在有请求时通知 Redis。
      - 类似的，我们也可以针对已连接套接字设置非阻塞模式:Redis 调用 recv() 后，如果已连 接套接字上一直没有数据到达，Redis 线程同样可以返回处理其他操作。我们也需要有机 制继续监听该已连接套接字，并在有数据达到时通知 Redis。

- **基于多路复用的高性能** **I/O** **模型**

  - Linux 中的 IO 多路复用机制是指一个线程处理多个 IO 流，就是我们经常听到的 select/epoll 机制。简单来说，在 Redis 只运行单线程的情况下，**该机制允许内核中，同时存在多个监听套接字和已连接套接字**。
    - 内核会一直监听这些套接字上的连接请求或数据 请求。一旦有请求到达，就会交给 Redis 线程处理，这就实现了一个 Redis 线程处理多个 IO 流的效果。
    - Redis 网络框架调用 epoll 机制，让内核监听这些套接字FD。
    - 为了在请求到达时能通知到 Redis 线程，select/epoll 提供了**基于事件的回调机制**，即**针 对不同事件的发生，调用相应的处理函数**。
  - select/epoll 一旦监测到 FD 上有请求到达时，就会**触发相应的事件**。
    - 这些事件会被放进一个**事件队列**，Redis 单线程对该事件队列不断进行处理。这样一来， Redis 无需一直轮询是否有请求实际发生，这就可以避免造成 CPU 资源浪费。同时， Redis 在对事件队列中的事件进行处理时，会调用相应的处理函数，这就实现了**基于事件的回调**。因为 Redis 一直在对事件队列进行处理，所以能及时响应客户端请求，提升 Redis 的响应性能。
    - 以连接请求和读数据请求为例
      - 这两个请求分别对应 Accept 事件和 Read 事件，Redis 分别对这两个事件注册 accept 和 get 回调函数。当 Linux 内核监听到有连接请求或读数据请求时，就会触发 Accept 事件 和 Read 事件，此时，内核就会回调 Redis 相应的 accept 和 get 函数进行处理。
      - **这就像病人去医院瞧病**。在医生实际诊断前，每个病人(等同于请求)都需要先分诊、测 体温、登记等。如果这些工作都由医生来完成，医生的工作效率就会很低。所以，医院都 设置了分诊台，分诊台会一直处理这些诊断前的工作(类似于 Linux 内核监听请求)，然 后再转交给医生做实际诊断。这样即使一个医生(相当于 Redis 单线程)，效率也能提 升。

- Redis单线程处理IO请求性能瓶颈主要包括2个方面：

  1、任意一个请求在server中一旦发生耗时，都会影响整个server的性能，也就是说后面的请求都要等前面这个耗时请求处理完成，自己才能被处理到。耗时的操作包括以下几种：
  a、操作bigkey：写入一个bigkey在分配内存时需要消耗更多的时间，同样，删除bigkey释放内存同样会产生耗时；
  b、使用复杂度过高的命令：例如SORT/SUNION/ZUNIONSTORE，或者O(N)命令，但是N很大，例如lrange key 0 -1一次查询全量数据；
  c、大量key集中过期：Redis的过期机制也是在主线程中执行的，大量key集中过期会导致处理一个请求时，耗时都在删除过期key，耗时变长；
  d、淘汰策略：淘汰策略也是在主线程执行的，当内存超过Redis内存上限后，每次写入都需要淘汰一些key，也会造成耗时变长；
  e、AOF刷盘开启always机制：每次写入都需要把这个操作刷到磁盘，写磁盘的速度远比写内存慢，会拖慢Redis的性能；
  f、主从全量同步生成RDB：虽然采用fork子进程生成数据快照，但fork这一瞬间也是会阻塞整个线程的，实例越大，阻塞时间越久；
  2、并发量非常大时，单线程读写客户端IO数据存在性能瓶颈，虽然采用IO多路复用机制，但是读写客户端数据依旧是同步IO，只能单线程依次读取客户端的数据，无法利用到CPU多核。

  针对问题1，一方面需要业务人员去规避，一方面Redis在4.0推出了lazy-free机制，把bigkey释放内存的耗时操作放在了异步线程中执行，降低对主线程的影响。

  针对问题2，Redis在6.0推出了多线程，可以在高并发场景下利用CPU多核多线程读写客户端数据，进一步提升server性能，当然，只是针对客户端的读写是并行的，每个命令的真正操作依旧是单线程的。







# AOF日志

- **一旦服务器宕机，内存中的数据将全部丢失**
- 目前，Redis 的持久化主要有两大机制，即 AOF 日志和 RDB 快照。
  - 说到日志，我们比较熟悉的是数据库的写前日志(Write Ahead Log, WAL)，也就是 说，在实际写数据前，先把修改的数据记到日志文件中，以便故障时进行恢复。不过， **AOF 日志正好相反，它是写后日志**，“写后”的意思是 Redis 是先执行命令，把数据写入 内存，然后才记录日志
  - 传统数据库的日志，例如 redo log(重做日志)，记录的是修改后的数据，而 **AOF 里记录的是 Redis 收到的每一条命令，这些命令是以文本形式保存的。**
  - AOF 还有一个好处:它是在命令执行后才记录日志，所以**不会阻塞当前的写操 作**。
- 不过，AOF 也有两个潜在的风险。
  - 首先，如果刚执行完一个命令，还没有来得及记日志就宕机了，那么这个命令和相应的数 据就有丢失的风险。如果此时 Redis 是用作缓存，还可以从后端数据库重新读入数据进行恢复，但是，如果 Redis 是直接用作数据库的话，此时，因为命令没有记入日志，所以就 无法用日志进行恢复了。
  - 其次，AOF 虽然避免了对当前命令的阻塞，但可能会给下一个操作带来阻塞风险。这是因 为，AOF 日志也是在主线程中执行的，如果在把日志文件写入磁盘时，磁盘写压力大，就 会导致写盘很慢，进而导致后续的操作也无法执行了。
  - **这两个风险都是和 AOF 写回磁盘的时机相关的**。这也就意味 着，如果我们能够控制一个写命令执行完后 AOF 日志写回磁盘的时机，这两个风险就解除 了。
- **三种写回策略**
  - AOF 配置项 appendfsync 的三个可选值
    - **Always**，同步写回:每个写命令执行完，立马同步地将日志写回磁盘;
    - **Everysec**，每秒写回:每个写命令执行完，只是先把日志写到 AOF 文件的内存缓冲区，每隔一秒把缓冲区中的内容写入磁盘;
    - **No**，操作系统控制的写回:每个写命令执行完，只是先把日志写到 AOF 文件的内存缓冲区，由操作系统决定何时将缓冲区内容写回磁盘。
  - 想要获得高性能，就选择 No 策略;如果想要得到高可靠性保证，就选择 Always 策略;如果允许数据有一点丢失，又希望性能别受太大影响的话，那么就选择 Everysec 策略。
- AOF 是以文 件的形式在记录接收到的所有写命令。随着接收的写命令越来越多，AOF 文件会越来越 大。这也就意味着，我们一定要小心 AOF 文件过大带来的性能问题。
  - 这里的“性能问题”，主要在于以下三个方面:
    - 一是，文件系统本身对文件大小有限制， 无法保存过大的文件;
    - 二是，如果文件太大，之后再往里面追加命令记录的话，效率也会 变低;
    - 三是，如果发生宕机，AOF 中记录的命令要一个个被重新执行，用于故障恢复，如 果日志文件太大，整个恢复过程就会非常缓慢，这就会影响到 Redis 的正常使用。
- **日志文件太大了怎么办?**
  - 重写机制具有“多变一”功能。所谓 的“多变一”，也就是说，旧日志文件中的多条命令，在重写后的新日志中变成了一条命 令。
- **AOF重写会阻塞吗**
  - 和 AOF 日志由主线程写回不同，重写过程是由后台线程 bgrewriteaof 来完成的，这也是为了避免阻塞主线程，导致数据库性能下降。
  - 把重写的过程总结为“**一个拷贝，两处日志**”。
    - “一个拷贝”就是指，**每次执行重写时，主线程 fork 出后台的 bgrewriteaof 子进程**。fork子进程时，子进程是会拷贝父进程的页表，即虚实映射关系，而不会拷贝物理内存。子进程复制了父进程页表，也能共享访问父进程的内存数据 了，此时，类似于有了父进程的所有内存数据。然后，bgrewriteaof 子进程就可以在不影响主线程的情况下，逐一把拷贝的数 据写成操作，记入重写日志。
    - 因为主线程未阻塞，仍然可以处理新来的操作。此时，如果有写操作，第一处日志就是指 **正在使用的 AOF 日志**，**Redis 会把这个操作写到它的缓冲区。**这样一来，即使宕机了，这 个 AOF 日志的操作仍然是齐全的，可以用于恢复。
    - 而第二处日志，就是指**新的 AOF 重写日志**。**这个操作也会被写到重写日志的缓冲区**。这样，重写日志也不会丢失最新的操作。等到拷贝数据的所有操作记录重写完成后，重写日志记录的这些最新操作也会写入新的 AOF 文件，以保证数据库最新状态的记录。此时，我 们就可以用新的 AOF 文件替代旧文件了。
  - 总结来说，每次 AOF 重写时，Redis 会先执行一个内存拷贝，用于重写;然后，使用两个 日志保证在重写过程中，新写入的数据不会丢失。而且，因为 Redis 采用额外的线程进行 数据重写，所以，这个过程并不会阻塞主线程。
- Huge page在实际使用Redis时是建议关掉的
  - 这个特性在使用Redis也要注意。Huge page对提升TLB命中率比较友好，因为在相同的内存容量下，使用huge page可以减少页表项，TLB就可以缓存更 多的页表项，能减少TLB miss的开销。
  - 但是，这个机制对于Redis这种喜欢用fork的系统来说，的确不太友好，**尤其是在Redis的写入请求比较多的情况下**。**因为fork后，父进程修改数据采用写时复制**，复制的粒度为一个内存页。如果只是修改一个256B的数据，父进程需要读原来的内存页，然后再映射到新的物理地址写入。**一读一写会造成读写放大。**如果内存页越大(例如2MB的大页)，那么**读写放大也就越严重，对Redis性能造成影响。**





# **内存快照**

- 但是，也正因为记录的是操作命令，而不是实际的数据，所以，用 AOF 方法进行故障恢复 的时候，需要逐一把操作日志都执行一遍。如果操作日志非常多，Redis 就会恢复得很缓慢，影响到正常使用。这当然不是理想的结果。那么，还有没有既可以保证可靠性，还能在宕机时实现快速恢复的其他方法呢?
  - 和 AOF 相比，RDB 记录的是某一时刻的数据，并不是操作，所以，在做数据恢复时，我 们可以直接把 RDB 文件读入内存，很快地完成恢复。
- 考虑两个关键问题
  - 对哪些数据做快照?这关系到快照的执行效率问题;
  - 做快照时，数据还能被增删改吗?这关系到 Redis 是否被阻塞，能否同时正常处理请 求。
- **给哪些内存数据做快照?**
  - Redis 的数据都在内存中，为了提供所有数据的可靠性保证，它执行的是**全量快照**，也就 是说，把内存中的所有数据都记录到磁盘中
- 对于 Redis 而言，它的单线程模型就决定了，我们要尽量避免所有会阻塞主线程的操作
  - Redis 提供了两个命令来生成 RDB 文件，分别是 save 和 bgsave。
  - **save**:在主线程中执行，会导致阻塞;
  - **bgsave**:创建一个子进程，专门用于写入 RDB 文件，避免了主线程的阻塞，这也是 Redis RDB 文件生成的默认配置。
- **快照时数据能修改吗**?
  - **避免阻塞和正常处理写操作并不是一回事**。此时，主线程的确没有阻塞，可以正常接收请求，但 是，为了保证快照完整性，它只能处理读操作，因为不能修改正在执行快照的数据。
  - 为了快照而暂停写操作，肯定是不能接受的。所以这个时候，Redis 就会借助操作系统提 供的**写时复制技术(Copy-On-Write, COW)**，在执行快照的同时，正常处理写操作。
    - 简单来说，bgsave 子进程是由主线程 fork 生成的，可以共享主线程的所有内存数据。 bgsave 子进程运行后，开始读取主线程的内存数据，并把它们写入 RDB 文件。
    - 如果主线程对这些数据也都是读操作，那么，主线程和 bgsave 子进程相互不影响。但是，**如果主线程要修改一块数据， 那么，这块数据就会被复制一份，生成该数据的副本**。然后，bgsave 子进程会把这个副本 数据写入 RDB 文件，而在这个过程中，主线程仍然可以直接修改原来的数据。
- **可以每秒做一次快照吗?**
  - **如果频繁地执行全量 快照，也会带来两方面的开销**。
    - 一方面，频繁将全量数据写入**磁盘**，会给磁盘带来很大压力。
    - 另一方面，bgsave 子进程需要通过 fork 操作从主线程创建出来。虽然，子进程在创建后 不会再阻塞主线程，但是，**fork 这个创建过程本身会阻塞主线程**，而且主线程的内存越 大，阻塞时间越长。如果频繁 fork 出 bgsave 子进程，这就会频繁阻塞主线程了。
  - 此时，我们可以做增量快照，所谓增量快照，就是指，做了一次全量快照后，后续的快照只对修改的数据进行快照记录，这样可以避免每次全量快照的开销。
    - 为了“记住”修改，引入的 额外空间开销比较大。这对于内存资源宝贵的 Redis 来说，有些得不偿失。
- 虽然跟 AOF 相比，快照的恢复速度快，但是，快照的频率不好把 握，如果频率太低，两次快照间一旦宕机，就可能有比较多的数据丢失。如果频率太高， 又会产生额外开销，那么，**还有什么方法既能利用 RDB 的快速恢复，又能以较小的开销做 到尽量少丢数据呢?**
  - **Redis 4.0** 中提出了一个**混合使用 AOF 日志和内存快照**的方法。简单来说，内存快照以一 定的频率执行，在两次快照之间，使用 AOF 日志记录这期间的所有命令操作。
- 最后，关于 AOF 和 RDB 的选择问题，我想再给你提三点建议:
  - 数据不能丢失时，内存快照和 AOF 的混合使用是一个很好的选择; 
  - 如果允许分钟级别的数据丢失，可以只使用 RDB;
  - 如果只用 AOF，优先使用 everysec 的配置选项，因为它在可靠性和性能之间取了一个 平衡。









# **数据同步**

-  Redis 具有高可靠性
  - 其实，这里有两层含义:一是**数据尽量少丢失**，二是**服务尽量少中断**。
  - AOF 和 RDB 保证了前者，而对于后者，Redis 的做法就是**增加副本冗余量**，将一份数据同时保存在多个实例上。
    - 实际上，Redis 提供了主从库模式，以保证数据副本的一致，主从库之间采用的是读写分离的方式。
- 主从库间如何进行第一次同步?
  - 当我们启动多个 Redis 实例的时候，它们相互之间就可以通过 **replicaof(Redis 5.0 之前 使用 slaveof)命令形成主库和从库的关系，之后会按照三个阶段完成数据的第一次同步。**
- **主从库间数据第一次同步的三个阶段**
  - 第一阶段是主从库间建立连接、协商同步的过程，主要是为全量复制做准备。在这一步，**从库和主库建立起连接，并告诉主库即将进行同步，主库确认回复后，主从库间就可以开 始同步了**。
    - 具体来说，**从库给主库发送 psync 命令**，表示要进行数据同步，主库根据这个命令的参数 来启动复制。psync 命令包含了**主库的 runID** 和**复制进度 offset** 两个参数。
      - runID，是每个 Redis 实例启动时都会自动生成的一个随机 ID，用来唯一标记这个实 例。当从库和主库第一次复制时，因为不知道主库的 runID，所以将 runID 设 为“?”。
      - offset，此时设为 -1，表示第一次复制。
    - **主库收到 psync 命令后，会用 FULLRESYNC 响应命令带上两个参数**:主库 runID 和主库 目前的复制进度 offset，返回给从库。从库收到响应后，会记录下这两个参数。
      - **FULLRESYNC 响应表示第一次复制采用的全量复制，也就是说， 主库会把当前所有的数据都复制给从库**。
  - 在第二阶段，**主库将所有数据同步给从库。从库收到数据后，在本地完成数据加载**。这个 过程依赖于内存快照生成的 RDB 文件。
    - 具体来说，**主库执行 bgsave 命令**，生成 RDB 文件，接着将文件发给从库。从库接收到 RDB 文件后，会先清空当前数据库，然后加载 RDB 文件。这是因为从库在通过 replicaof 命令开始和主库同步前，可能保存了其他数据。为了避免之前数据的影响，从库需要先把 当前数据库清空。
    - 在主库将数据同步给从库的过程中，主库不会被阻塞，仍然可以正常接收请求。否则， Redis 的服务就被中断了。但是，这些请求中的写操作并没有记录到刚刚生成的 RDB 文件 中。**为了保证主从库的数据一致性，主库会在内存中用专门的 replication buffer，记录 RDB 文件生成后收到的所有写操作。**
  - 第三个阶段，**主库会把第二阶段执行过程中新收到的写命令，再发送给从 库。**具体的操作是，当主库完成 RDB 文件发送后，就会把此时 replication buffer 中的修 改操作发给从库，从库再重新执行这些操作。这样一来，主从库就实现同步了。
- **主从级联模式分担全量复制时的主库压力**
  - **一次全量复制中，对于主库来 说，需要完成两个耗时的操作:生成 RDB 文件和传输 RDB 文件。**
  - 在刚才介绍的主从库模式中，所有的从库都是和主库连接，所有的全量复制也都是和主库 进行的。现在，我们可以**通过“主 - 从 - 从”模式将主库生成 RDB 和传输 RDB 的压力， 以级联的方式分散到从库上**。
    - 简单来说，我们在部署主从集群的时候，可以手动选择一个从库(比如选择内存资源配置较高的从库)，用于级联其他的从库。然后，我们可以再选择一些从库(例如三分之一的从库)，在这些从库上执行如下命令，让它们和刚才所选的从库，建立起主从关系。
    - 这样一来，这些从库就会知道，在进行同步时，不用再和主库进行交互了，只要和级联的从库进行写操作同步就行了，这就可以减轻主库上的压力
  - 听上去好像很简单，但不可忽视的是，这个过程中存在着风险点，最常见的就是**网络断连或阻塞**。如果网络断连，主从库之间就无法进行命令传播了，从库的数据自然也就没办法 和主库保持一致了，客户端就可能从从库读到旧数据。
- **主从库间网络断了怎么办?**
  - 从 Redis 2.8 开始，网络断了之后，主从库会采用增量复制的方式继续同步。听名字大概 就可以猜到它和全量复制的不同:全量复制是同步所有数据，而**增量复制**只会把主从库网 络断连期间主库收到的命令，同步给从库。
  - 当主从库断连后，主库会把断连期间收到的写操作命令，写入 replication buffer，同时也 会把这些操作命令也写入 **repl_backlog_buffer** 这个缓冲区。
    - repl_backlog_buffer 是一个环形缓冲区，**主库会记录自己写到的位置，从库则会记录自己 已经读到的位置**。
    - 刚开始的时候，主库和从库的写读位置在一起，这算是它们的起始位置。随着主库不断接 收新的写操作，它在缓冲区中的写位置会逐步偏离起始位置，我们通常用偏移量来衡量这 个偏移距离的大小，对主库来说，对应的偏移量就是 **master_repl_offset**。主库接收的新 写操作越多，这个值就会越大。
    - 主从库的连接恢复之后，从库首先会给主库发送 psync 命令，并把自己当前的 slave_repl_offset 发给主库，主库会判断自己的 master_repl_offset 和 slave_repl_offset 之间的差距。此时，主库只用把 master_repl_offset 和 slave_repl_offset 之间的命令操作同步给从库就行。
  - 强调一下，因为 repl_backlog_buffer 是一个环形缓冲区，所以在 缓冲区写满后，主库会继续写入，此时，就会覆盖掉之前写入的操作。**如果从库的读取速 度比较慢，就有可能导致从库还未读取的操作被主库新写的操作覆盖了，这会导致主从库 间的数据不一致**
    - 因此，我们要想办法避免这一情况，一般而言，我们可以调整 **repl_backlog_size** 这个参数。这个参数和所需的缓冲空间大小有关。缓冲空间的计算公式是:缓冲空间大小 = 主库 写入命令速度 * 操作大小 - 主从库间网络传输命令速度 * 操作大小。在实际应用中，考虑 到可能存在一些突发的请求压力，我们通常需要把这个缓冲空间扩大一倍，即 **repl_backlog_size = 缓冲空间大小 * 2，这也就是 repl_backlog_size 的最终值。**
  - 这样一来，增量复制时主从库的数据不一致风险就降低了。不过，如果并发请求量非常大，连两倍的缓冲空间都存不下新操作请求的话，此时，主从库数据仍然可能不一致。
    - 针对这种情况，**一方面**，你可以根据 Redis 所在服务器的内存资源再适当增加 repl_backlog_size 值，比如说**设置成缓冲空间大小的 4 倍**，**另一方面**，你可以考虑使用**切片集群**来分担单个主库的请求压力。







# **哨兵机制**

- **哨兵机制的基本流程**
  - 哨兵其实就是一个运行在特殊模式下的 Redis 进程，主从库实例运行的同时，它也在运 行。哨兵主要负责的就是三个任务:**监控、选主(选择主库)和通知。**
  - **监控**是指哨兵进程在运行时，**周期性地给所有的主从库发送 PING 命令， 检测它们是否仍然在线运行**。如果从库没有在规定时间内响应哨兵的 PING 命令，哨兵就 会把它标记为“下线状态”;同样，如果主库也没有在规定时间内响应哨兵的 PING 命 令，哨兵就会判定主库下线，然后开始**自动切换主库**的流程。
  - **这个流程首先是执行哨兵的第二个任务：选主**。主库挂了以后，哨兵就需要从很多个从库里，按照一定的规则选择一个从库实例，把它作为新的主库。这一步完成后，现在的集群里就有了新主库。
  - **然后，哨兵会执行最后一个任务：通知**。在执行通知任务时，哨兵会把新主库的连接信息 发给其他从库，让它们执行 replicaof 命令，和新主库建立连接，并进行数据复制。同时， 哨兵会把新主库的连接信息通知给客户端，让它们把请求操作发到新主库上。
- 在监控和选主这两个任务中，哨兵需要做出两个决策:
  - 在监控任务中，哨兵需要判断主库是否处于下线状态;
  - 在选主任务中，哨兵也要决定选择哪个从库实例作为主库。
- **主观下线和客观下线**
  - 如果哨兵发现主库或从库对 PING 命令的响应超时了，那么，哨兵就会先把它标记 为“主观下线”。
    - 如果检测的是从库，那么，哨兵简单地把它标记为“主观下线”就行了，因为从库的下线影响一般不太大，集群的对外服务不会间断。
    - **但是，如果检测的是主库，那么，哨兵还不能简单地把它标记为“主观下线”，开启主从切换**。因为很有可能存在这么一个情况:那就是哨兵误判了，其实主库并没有故障。可是，一旦启动了主从切换，后续的选主和通知操作都会带来额外的计算和通信开销。
  - 为了避免这些不必要的开销，我们要特别注意误判的情况。
    - 我们要知道啥叫误判。很简单，就是主库实际并没有下线，但是哨兵误以为它下线了。误判一般会发生在集群网络压力较大、网络拥塞，或者是主库本身压力较大的情况下。
    - 哨兵机制也是类似的，它**通常会采用多实例组成的集群模式进行部署，这也被称为哨兵集群**。引入多个哨兵实例一起来判断，就可以避免单个哨兵因为自身网络状况不好，而误判主库下线的情况。
  - 简单来说，“客观下线”的标准就是，当有 N 个哨兵实例时，最好要有 N/2 + 1 个实例判 断主库为“主观下线”，才能最终判定主库为“客观下线”。
- **如何选定新主库?**
  - 一般来说，我把哨兵选择新主库的过程称为“筛选 + 打分”。简单来说，我们在多个从库 中，先按照**一定的筛选条件**，把不符合条件的从库去掉。然后，我们再按照**一定的规则**， 给剩下的从库逐个打分，将得分最高的从库选为新主库。
  - 一般情况下，我们肯定要先保证所选的从库仍然在线运行。不过，在选主时从库正常在线，这只能表示从库的现状良好，并不代表它就是最适合做主库的。
    - 所以，在选主时，**除了要检查从库的当前在线状态，还要判断它之前的网络连接状态**。如 果从库总是和主库断连，而且断连次数超出了一定的阈值，我们就有理由相信，这个从库 的网络状况并不是太好，就可以把这个从库筛掉了。
    - 具体怎么判断呢?你使用配置项 down-after-milliseconds * 10。其中，**down-after- milliseconds 是我们认定主从库断连的最大连接超时时间。**如果在 down-after- milliseconds 毫秒内，主从节点都没有通过网络联系上，我们就可以认为主从节点断连 了。如果发生断连的次数超过了 10 次，就说明这个从库的网络状况不好，不适合作为新主 库。
  - 我们可以分别按照三个规则依次进行三轮打分，这三个 规则分别是**从库优先级、从库复制进度以及从库 ID 号**。只要在某一轮中，有从库得分最 高，那么它就是主库了，选主过程到此结束。如果没有出现得分最高的从库，那么就继续 进行下一轮。
    - 第一轮:优先级最高的从库得分高。
      - 用户可以通过 slave-priority 配置项，给不同的从库设置不同优先级
    - 第二轮:和旧主库同步程度最接近的从库得分高。
      - 主从库同步时有个命令传播的过程。在这个过程中，主库会用 master_repl_offset 记录当前的最新写操作在 repl_backlog_buffer 中的位置，而从库会 用 slave_repl_offset 这个值记录当前的复制进度。
      - 有从库的 slave_repl_offset 最接近 master_repl_offset，那么它的得分就 最高，可以作为新主库。
      - **实际的选主代码层面，sentinel是直接比较从库的slave_repl_offset，来选择和主库最接近的从库。哨兵在这一步，是 通过比较不同从库的slave_repl_offset，找出最大slave_repl_offset的从库。**
    - 第三轮:ID 号小的从库得分高。
      - 目前，Redis 在选主库 时，有一个默认的规定:**在优先级和复制进度都相同的情况下，ID 号最小的从库得分最 高，会被选为新主库**。







# **哨兵集群**

- 实际上，一旦多个实例组成了**哨兵集群**，即使有哨兵实例出现故障挂掉了，其他哨兵还能 继续协作完成主从库切换的工作，包括判定主库是不是处于下线状态，选择新主库，以及 通知从库和客户端。
- **基于** **pub/sub机制的哨兵集群组成**
  - 哨兵只要和主库建立起了连接，就可以在主库上发布消息了，比如说发布它自己的连接信 息(IP 和端口)。同时，它也可以从主库上订阅消息，获得其他哨兵发布的连接信息。当 多个哨兵实例都在主库上做了发布和订阅操作后，它们之间就能知道彼此的 IP 地址和端 口。
  - 在主从集群中，主库上有一个名为“__sentinel__:hello”的频道，不同哨兵就是通过 它来相互发现，实现互相通信的。
- **哨兵是如何知道从库的 IP 地址和端口的呢**
  - **这是由哨兵向主库发送 INFO 命令来完成的。**哨兵 2 给主库发送 INFO 命 令，主库接受到这个命令后，就会把从库列表返回给哨兵。接着，哨兵就可以根据从库列 表中的连接信息，和每个从库建立连接，并在这个连接上持续地对从库进行监控。哨兵 1 和 3 可以通过相同的方法和从库建立连接。
  - 但是，哨兵不能只和主、从库连接。因为，主从库切换后，客户端也需要知道新主库的连接信息，才能向新主库发送请求操作。所以，哨兵还需要完成把新主库的信息告诉客户端这个任务。
- **基于** **pub/sub** **机制的客户端事件通知**
  - **我们仍然可以依赖 pub/sub 机制，来帮助我们完成哨兵和客户端间的信息同步。**
  - 重要的频道汇总在了一起，涉及几个关键事件，包括主库下线判断、新主库选定、从库重新配置。
    - 知道了这些频道之后，你就可以**让客户端从哨兵这里订阅消息**了。具体的操作步骤是，客 户端读取哨兵的配置文件后，可以获得哨兵的地址和端口，和哨兵建立网络连接。然后， 我们可以在客户端执行订阅命令，来获取不同的事件消息。
    - 有了这些事件通知，客户端不仅可以在主从切换后得到新主库的连接信息，还可以监控到主从库切换过程中发生的各个重要事件。这样，客户端就可以知道主从切换进行到哪一步了，有助于了解切换进度。
- **由哪个哨兵执行主从切换?**
  - **任何一个实例只要自身判断主库“主观下线”后，就会给其他实例发送 is-master-down- by-addr 命令。**接着，其他实例会根据自己和主库的连接情况，做出 Y 或 N 的响应，Y 相 当于赞成票，N 相当于反对票。
  - 此时，这个哨兵就可以再给其他哨兵发送命令，表明希望由自己来执行主从切换，并让所 有其他哨兵进行投票。这个投票过程称为“Leader 选举”。因为最终执行主从切换的哨兵 称为 Leader，投票过程就是确定 Leader。
- **分享一个经验：要保证所有哨兵实例的配置是一致的，尤其是主观下线的判断值 down-after-milliseconds**。
  - 调大down-after-milliseconds值，对减少误判是不是有好处？
    - 是有好处的，适当调大down-after-milliseconds值，当哨兵与主库之间网络存在短时波动时，可以降低误判的概率。但是调大down-after-milliseconds值也意味着主从切换的时间会变长，对业务的影响时间越久，我们需要根据实际场景进行权衡，设置合理的阈值。









# **切片集群**

- 这么一个需求:要用 Redis 保存 5000 万个键值对，每个键值对大约是 512B， 为了能快速部署并对外提供服务，我们采用云主机来运行 Redis 实例，那么，该如何选择 云主机的内存容量呢?
  - 在使用 RDB 进行持久化时，Redis 会 fork 子进程来完 成，**fork 操作的用时和 Redis 的数据量是正相关的**，而 fork 在执行时会阻塞主线程。数 据量越大，fork 操作造成的主线程阻塞的时间越长。所以，在使用 RDB 对 25GB 的数据 进行持久化时，数据量较大，后台运行的子进程在 fork 创建时阻塞了主线程，于是就导致 Redis 响应变慢了。
  - 必须要寻找其他的方案：虽然组建切片集群比较麻烦，但是它可以保存大量数据，而且对 Redis 主线程的阻塞影响较小。
    - 切片集群，也叫分片集群，就是指启动多个 Redis 实例组成一个集群，然后按照一定的规 则，把收到的数据划分成多份，每一份用一个实例来保存。回到我们刚刚的场景中，如果 把 25GB 的数据平均分成 5 份(当然，也可以不做均分)，使用 5 个实例来保存，每个实 例只需要保存 5GB 数据。
- **如何保存更多数据?**
  - 在刚刚的案例里，为了保存大量数据，我们使用了大内存云主机和切片集群两种方法。实 际上，这两种方法分别对应着 Redis 应对数据量增多的两种方案:纵向扩展(scale up) 和横向扩展(scale out)。
    - **纵向扩展**:升级单个 Redis 实例的资源配置，包括增加内存容量、增加磁盘容量、使用 更高配置的 CPU。
    - **横向扩展**:横向增加当前 Redis 实例的个数
  - 首先，纵向扩展的好处是，**实施起来简单、直接**。不过，这个方案也面临两个潜在的问 题。
    - 第一个问题是，当使用 RDB 对数据进行持久化时，如果数据量增加，需要的内存也会增 加，主线程 fork 子进程时就可能会阻塞(比如刚刚的例子中的情况)。不过，如果你不要 求持久化保存 Redis 数据，那么，纵向扩展会是一个不错的选择。
    - 不过，这时，你还要面对第二个问题:**纵向扩展会受到硬件和成本的限制**。这很容易理 解，毕竟，把内存从 32GB 扩展到 64GB 还算容易，但是，要想扩充到 1TB，就会面临硬 件容量和成本上的限制了。
  - **在面向百万、千万级别的用户规模时，横向扩展的 Redis 切片集群会是一个非常好 的选择**。
  - 要想把切片集群用起来，我们就需要解决两大问题:
    - 数据切片后，在多个实例之间如何分布?
    - 客户端怎么确定想要访问的数据在哪个实例上?
- **数据切片和实例的对应分布关系**
  - 具体来说，Redis Cluster 方案采用哈希槽(Hash Slot，接下来我会直接称之为 Slot)， 来处理数据和实例之间的映射关系。在 Redis Cluster 方案中，一个切片集群共有 16384 个哈希槽，这些哈希槽类似于数据分区，每个键值对都会根据它的 key，被映射到一个哈 希槽中。
  - 具体的映射过程分为两大步:首先根据键值对的 key，按照CRC16 算法计算一个 16 bit 的值;然后，再用这个 16bit 值对 16384 取模，得到 0~16383 范围内的模数，每个模数代表一个相应编号的哈希槽。
  - 我们在部署 Redis Cluster 方案时，可以使用 cluster create 命令创建集群，此时，Redis 会自动把这些槽平均分布在集群实例上。例如，如果集群中有 N 个实例，那么，每个实例 上的槽个数为 16384/N 个。
    - 当然， 我们也可以使用 cluster meet 命令手动建立实例间的连接，形成集群，再使用 cluster addslots 命令，指定每个实例上的哈希槽个数。
    - **在手动分配哈希槽时，需要把 16384 个槽都分配完，否则 Redis 集群无法正常工作**。
- **客户端如何定位数据?**
  - 客户端收到哈希槽信息后，会把哈希槽信息缓存在本地。当客户端请求键值对时，会先计算键所对应的哈希槽，然后就可以给相应的实例发送请求了。
  - 但是，在集群中，实例和哈希槽的对应关系并不是一成不变的，最常见的变化有两个:
    - 在集群中，实例有新增或删除，Redis 需要重新分配哈希槽; 
    - 为了负载均衡，Redis 需要把哈希槽在所有实例上重新分布一遍。
  - 此时，实例之间还可以通过相互传递消息，获得最新的哈希槽分配信息，但是，客户端是无法主动感知这些变化的。这就会导致，它缓存的分配信息和最新的分配信息就不一致了，那该怎么办呢?
    - Redis Cluster 方案提供了一种**重定向机制，**所谓的“重定向”，就是指，客户端给一个实 例发送数据读写操作时，这个实例上并没有相应的数据，客户端要再给一个新实例发送操 作命令。
    - 当客户端把一个键值对的操作请 求发给一个实例时，如果这个实例上并没有这个键值对映射的哈希槽，那么，这个实例就 会给客户端返回下面的 MOVED 命令响应结果，这个结果中就包含了新实例的访问地址。
  - 在**迁移部分完成的情况**下，客户端就会收到一条 ASK 报错信息，如下所 示:
    - 这个结果中的 ASK 命令就表示，客户端请求的键值对所在的哈希槽 13320，在 172.16.19.5 这个实例上，但是这个哈希槽正在迁移。此时，客户端需要先给 172.16.19.5 这个实例发送一个 ASKING 命令。这个命令的意思是，让这个实例允许执行客户端接下来 发送的命令。然后，客户端再向这个实例发送 GET 命令，以读取数据。
    - ASK 命令表示两层含义:第一，表明 Slot 数据还在迁移中;第二，ASK 命令把客户端所 请求数据的最新实例地址返回给客户端，此时，客户端需要给实例 3 发送 ASKING 命令， 然后再发送操作命令。
    - 和 MOVED 命令不同，**ASK 命令并不会更新客户端缓存的哈希槽分配信息**。所以，在上图 中，如果客户端再次请求 Slot 2 中的数据，它还是会给实例 2 发送请求。这也就是说， ASK 命令的作用只是让客户端能给新实例发送一次请求，而不像 MOVED 命令那样，会更 改本地缓存，让后续所有命令都发往新实例。





# String

- String 类型并不是适用于所有场合的，它有 一个明显的短板，就是它保存数据时所消耗的内存空间较多。
  - 集合类型有非常节省内存空间的底 层实现结构，但是，集合类型保存的数据模式，是一个键对应一系列值，并不适合直接保 存单值的键值对。所以，使用二级编码的方法，实现了用集合类型保存单值键值对，Redis 实例的内存空间消耗明显下降了。
- **为什么** **String类型内存开销大?**
  - 除了记录实际数据，String 类型还需要额外的内存空间记录数据长度、空间使用等 信息，这些信息也叫作元数据。当实际保存的数据较小时，元数据的空间开销就显得比较 大了，有点“喧宾夺主”的意思。
  - 当你保存的数据中包含字符时，String 类型就会用简单动态字符串(Simple Dynamic String，SDS)结构体来保存
  - 另外，**对于 String 类型来说，除了 SDS 的额外开销，还有一个来自于 RedisObject 结构 体的开销。**
  - 因为 Redis 的数据类型有很多，而且，不同数据类型都有些相同的元数据要记录(比如最 后一次访问的时间、被引用的次数等)，所以，Redis 会用一个 RedisObject 结构体来统 一记录这些元数据，同时指向实际数据。
    - 一方面，当保存的是 Long 类型整数时，RedisObject 中的指针就直接赋值为整数数据 了，这样就不用额外的指针再指向整数了，节省了指针的空间开销。
    - 另一方面，当保存的是字符串数据，并且字符串小于等于 44 字节时，RedisObject 中的元 数据、指针和 SDS 是一块连续的内存区域，这样就可以避免内存碎片。这种布局方式也被 称为 embstr 编码方式。
    - 当然，当字符串大于 44 字节时，SDS 的数据量就开始变多了，Redis 就不再把 SDS 和 RedisObject 布局在一起了，而是会给 SDS 分配独立的空间，并用指针指向 SDS 结构。 这种布局方式被称为 raw 编码模式。
  - 因为 10 位数的图片 ID 和图片存储对象 ID 是 Long 类型整数，所以可以直接用 int 编码 的 RedisObject 保存。每个 int 编码的 RedisObject 元数据部分占 8 字节，指针部分被直 接赋值为 8 字节的整数了。此时，每个 ID 会使用 16 字节，加起来一共是 32 字节。但 是，另外的 32 字节去哪儿了呢?
    - Redis 会使用一个全局哈希表保存所有键值对，哈希表的每一项是 一个 dictEntry 的结构体，用来指向一个键值对。dictEntry 结构中有三个 8 字节的指针， 分别指向 key、value 以及下一个 dictEntry，三个指针共 24 字节
    - 但是，这三个指针只有 24 字节，为什么会占用了 32 字节呢?这就要提到 Redis 使用的内 存分配库 jemalloc 了。
    - jemalloc 在分配内存时，会根据我们申请的字节数 N，找一个比 N 大，但是最接近 N 的 2 的幂次数作为分配的空间，这样可以减少频繁分配的次数。
    - 举个例子。如果你申请 6 字节空间，jemalloc 实际会分配 8 字节空间;如果你申请 24 字 节空间，jemalloc 则会分配 32 字节。所以，在我们刚刚说的场景里，dictEntry 结构就占 用了 32 字节。
- **用什么数据结构可以节省内存?**
  - Redis 有一种底层数据结构，叫**压缩列表(ziplist)**，这是一种非常节省内存的结构。
  - 压缩列表的构成。表头有三个字段 zlbytes、zltail 和 zllen，分别表示列表长 度、列表尾的偏移量，以及列表中的 entry 个数。压缩列表尾还有一个 zlend，表示列表 结束。
  - 压缩列表之所以能节省内存，就在于它是用一系列连续的 entry 保存数据。
  - 每个 entry 的 元数据包括下面几部分。
    - **prev_len**，表示前一个 entry 的长度。prev_len 有两种取值情况:1 字节或 5 字节。 取值 1 字节时，表示上一个 entry 的长度小于 254 字节
    - **len**:表示自身长度，4 字节; 
    - **encoding**:表示编码方式，1 字节; 
    - **content**:保存实际数据。
  - 这些 entry 会挨个儿放置在内存中，不需要再用额外的指针进行连接，这样就可以节省指 针所占用的空间。
  - 我们以保存图片存储对象 ID 为例，来分析一下压缩列表是如何节省内存空间的。
    - 每个 entry 保存一个图片存储对象 ID(8 字节)，此时，每个 entry 的 prev_len 只需要 1 个字节就行，因为每个 entry 的前一个 entry 长度都只有 8 字节，小于 254 字节。这样一来，一个图片的存储对象 ID 所占用的内存大小是 14 字节(1+4+1+8=14)，实际分 配 16 字节。
    - Redis 基于压缩列表实现了 List、Hash 和 Sorted Set 这样的集合类型，这样做的最大好 处就是节省了 dictEntry 的开销。当你用 String 类型时，一个键值对就有一个 dictEntry， 要用 32 字节空间。但采用集合类型时，一个 key 就对应一个集合的数据，能保存的数据 多了很多，但也只用了一个 dictEntry，这样就节省了内存。
- **如何用集合类型保存单值的键值对?**
  - 在保存单值的键值对时，可以采用基于 Hash 类型的二级编码方法。这里说的二级编码， 就是把一个单值的数据拆分成两部分，前一部分作为 Hash 集合的 key，后一部分作为 Hash 集合的 value，这样一来，我们就可以把单值数据保存到 Hash 集合中了。
    - 以图片 ID 1101000060 和图片存储对象 ID 3302000080 为例，我们可以把图片 ID 的前 7 位(1101000)作为 Hash 类型的键，把图片 ID 的最后 3 位(060)和图片存储对象 ID 分别作为 Hash 类型值中的 key 和 value。
  - **二级编码方法中采用的 ID 长度是有讲 究的**。
    - Hash 类型底层结构什么时候使用压缩列表，什么时候使用哈希表呢?其实，Hash 类型设置了用压缩列表保存数据时的两个阈值，一旦超过了阈值，Hash 类型就会用哈希表 来保存数据了。
    - 两个阈值分别对应以下两个配置项:
      - hash-max-ziplist-entries:表示用压缩列表保存时哈希集合中的最大元素个数。 
      - hash-max-ziplist-value:表示用压缩列表保存时哈希集合中单个元素的最大长度
  - 一旦从压缩列表转为了哈希表，Hash 类型就会一直用哈希表进行保存，而不会再转回压缩 列表了。在节省内存空间方面，哈希表就没有压缩列表那么高效了。
  - **为了能充分使用压缩列表的精简内存布局，我们一般要控制保存在 Hash 集合中的元素个 数**。所以，在刚才的二级编码中，我们只用图片 ID 最后 3 位作为 Hash 集合的 key，也就 保证了 Hash 集合的元素个数不超过 1000，同时，我们把 hash-max-ziplist-entries 设置 为 1000，这样一来，Hash 集合就可以一直使用压缩列表来节省内存空间了。





# **集合统计模式**

- **要想选择合适的集合，我们就得了解常用的集合统计模式。**这节课，我就给你介绍集合类 型常见的四种统计模式，包括聚合统计、排序统计、二值状态统计和基数统计
- **聚合统计**
  - 所谓的聚合统计，就是指统计多个集合元素的聚合结果，包括:统计多个集合的共有元素(交集统计);把两个集合相比，统计其中一个集合独有的元素(差集统计);统计多个集合的所有元素(并集统计)。
  - 当你需要对多个集合进行聚合计算时，Set 类型会是一个非常不错的选择。不过，我要提醒 你一下，这里有一个潜在的风险。
  - Set 的差集、并集和交集的计算复杂度较高，在数据量较大的情况下，如果直接执行这些计 算，会导致 Redis 实例阻塞。所以，我给你分享一个小建议:**你可以从主从集群中选择一 个从库，让它专门负责聚合计算，或者是把数据读取到客户端，在客户端来完成聚合统 计**，这样就可以规避阻塞主库实例和其他从库实例的风险了。
- **排序统计**
  - **List 是按照元素进入 List 的顺序进行排序的，而 Sorted Set 可以根据元素的权重来排序**
  - 在面对需要展示最新列表、排行榜等场景时，如果数据更新频繁或者需要分页显示，建议你优先考虑使用 Sorted Set
- **二值状态统计**
  - 这里的二值状态就是指集合元素的取 值就只有 0 和 1 两种。
  - 在签到统计时，每个用户一天的签到用 1 个 bit 位就能表示，一个月(假设是 31 天)的签 到情况用 31 个 bit 位就可以，而一年的签到也只需要用 365 个 bit 位，根本不用太复杂 的集合类型。这个时候，我们就可以选择 Bitmap。
  - **Bitmap 本身是用 String 类型作为底层数据结构实现的一种统计二值状态的数据类型。** String 类型是会保存为二进制的字节数组，所以，Redis 就把字节数组的每个 bit 位利用 起来，用来表示一个元素的二值状态。**你可以把 Bitmap 看作是一个 bit 数组。**
  - 所以，如果只需要统计数据的二值状态，例如商品有没有、用户在不在等，就可以使用 Bitmap，因为它只用一个 bit 位就能表示 0 或 1。在记录海量数据时，Bitmap 能够有效 地节省内存空间。
- **基数统计**
  - 基数统计就是指统计一个集合中不重复的元 素个数。对应到我们刚才介绍的场景中，就是统计网页的 UV。
  - 网页 UV 的统计有个独特的地方，就是需要去重，一个用户一天内的多次访问只能算作一 次。在 Redis 的集合类型中，Set 类型默认支持去重，所以看到有去重需求时，我们可能 第一时间就会想到用 Set 类型。当然，你也可以用 Hash 类型记录 UV。
  - 但是，和 Set 类型相似，当页面很多时，Hash 类型也会消耗很大的内存空间。那么，有什 么办法既能完成统计，还能节省内存吗?
  - 这时候，就要用到 Redis 提供的 HyperLogLog 了。
    - HyperLogLog 是一种用于统计基数的数据集合类型，它的最大优势就在于，当集合元素数 量非常多时，它计算基数所需的空间总是固定的，而且还很小。
    - 不过，有一点需要你注意一下，**HyperLogLog 的统计规则是基于概率完成的，所以它给出 的统计结果是有一定误差**的，标准误算率是 0.81%。这也就意味着，你使用HyperLogLog 统计的 UV 是 100 万，但实际的 UV 可能是 101 万。虽然误差率不算大， 但是，如果你需要精确统计结果的话，最好还是继续用 Set 或 Hash 类型。





# **删除数据**

- 明明做了数据删除，数据量已经不大 了，为什么使用 top 命令查看时，还会发现 Redis 占用了很多内存呢?

  - 当数据删除后，Redis 释放的内存空间会由内存分配器管理，并不会 立即返回给操作系统。所以，**操作系统仍然会记录着给 Redis 分配了大量内存。**
  - 但是，这往往会伴随一个潜在的风险点:**Redis 释放的内存空间可能并不是连续的**，那么，这些不连续的内存空间很有可能处于一种闲置的状态。这就会导致一个问题:虽然有 空闲空间，Redis 却无法用来保存数据，不仅会减少 Redis 能够实际保存的数据量，还会 降低 Redis 运行机器的成本回报率。

- **内存碎片是如何形成的?**

  - 其实，内存碎片的形成有内因和外因两个层面的原因。简单来说，**内因是操作系统的内存分配机制，外因是 Redis 的负载特征。**
  - 内因:内存分配器的分配策略
    - 内存分配器的分配策略就决定了操作系统无法做到“按需分配”。这是因为，**内存分配器一般是按固定大小来分配内存**，而不是完全按照应用程序申请的内存空间大小给程序分配。
    - Redis 可以使用 libc、jemalloc、tcmalloc 多种内存分配器来分配内存，默认使用 jemalloc。
    - jemalloc 的分配策略之一，是按照一系列固定的大小划分内存空间，例如 8 字节、16 字 节、32 字节、48 字节，..., 2KB、4KB、8KB 等。当程序申请的内存最接近某个固定值 时，jemalloc 会给它分配相应大小的空间。
  - 外因:键值对大小不一样和删改操作
    - 一方面，**如果修改后的键值对变大或变小了，就需要占用额外的空间或者释放不用的空间。**另一方面，删除的键值对就不再需要内存空间了，此时，就会把空间释放出来，形成空闲空间。

- 如何判断是否有内存碎片?

  - 为了让用户能监控到实时的内存使用情况，Redis 自身提供了 INFO 命令，可以用来查询内存使用的详细信息。

    > INFO memory

  - 这里有一个 mem_fragmentation_ratio 的指标，它表示的就是 Redis 当前的内存碎片 率。那么，这个碎片率是怎么计算的呢?其实，就是上面的命令中的两个指标 used_memory_rss 和 used_memory 相除的结果。

    - used_memory_rss 是操作系统实际分配给 Redis 的物理内存空间，里面就包含了碎片; 而 used_memory 是 Redis 为了保存数据实际申请使用的空间。
    - **mem_fragmentation_ratio 大于 1 但小于 1.5**。这种情况是合理的。
    - **mem_fragmentation_ratio 大于 1.5** 。这表明内存碎片率已经超过了 50%。一般情 况下，这个时候，我们就需要采取一些措施来降低内存碎片率了。

- **如何清理内存碎片?**

  - 当 Redis 发生内存碎片后，一个“简单粗暴”的方法就是**重启 Redis 实例**。当然，这并不是一个“优雅”的方法，毕竟，重启 Redis 会带来两个后果:
    - 如果 Redis 中的数据没有持久化，那么，数据就会丢失;
    - 即使 Redis 数据持久化了，我们还需要通过 AOF 或 RDB 进行恢复，恢复时长取决于 AOF 或 RDB 的大小，如果只有一个 Redis 实例，恢复阶段无法提供服务。
  - **从 4.0-RC3 版本以后，Redis 自身提供了一种内存碎片自动清理的方法**，我们 先来看这个方法的基本机制。
    - 当有数据把一块连续的内存空间分割成好几块不连续的空间时，操作系统就会把数据拷贝到别处。此时，数据拷贝需要能把这些数据原来占用的空间都空出来，把原本不连续的内存空间变成连续的空间。
    - 不过，需要注意的是:**碎片清理是有代价的**，操作系统需要把多份数据拷贝到新位置，把 原有空间释放出来，这会带来时间开销。**因为 Redis 是单线程，在数据拷贝时，Redis 只 能等着，这就导致 Redis 无法及时处理请求，性能就会降低**。而且，有的时候，数据拷贝 还需要注意顺序，就像刚刚说的清理内存碎片的例子，操作系统需要先拷贝 D，并释放 D 的空间后，才能拷贝 B。这种对顺序性的要求，会进一步增加 Redis 的等待时间，导致性 能降低。

- Redis 专门为自动内存碎片清 理功机制设置的参数了。我们可以通过设置参数，来控制碎片清理的开始和结束时机，以 及占用的 CPU 比例，从而减少碎片清理对 Redis 本身请求处理的性能影响。

  - 首先，Redis 需要启用自动内存碎片清理，可以把 activedefrag 配置项设置为 yes

    > config set activedefrag yes

  - 这个命令只是启用了自动清理功能，但是，具体什么时候清理，会受到下面这两个参数的控制。这两个参数分别设置了触发内存清理的一个条件，如果同时满足这两个条件，就开始清理。在清理的过程中，只要有一个条件不满足了，就停止自动清理。

    > **active-defrag-ignore-bytes 100mb**:表示内存碎片的字节数达到 100MB 时，开始 清理;
    >
    > **active-defrag-threshold-lower 10**:表示内存碎片空间占操作系统分配给 Redis 的 总空间比例达到 10% 时，开始清理。

  - 为了尽可能减少碎片清理对 Redis 正常请求处理的影响，自动内存碎片清理功能在执行 时，还会监控清理操作占用的 CPU 时间，而且还设置了两个参数，分别用于控制清理操作 占用的 CPU 时间比例的上、下限，既保证清理工作能正常进行，又避免了降低 Redis 性 能。这两个参数具体如下:

    > **active-defrag-cycle-min 25**: 表示自动清理过程所用 CPU 时间的比例不低于 25%，保证清理能正常开展;
    >
    > **active-defrag-cycle-max 75**:表示自动清理过程所用 CPU 时间的比例不高于 75%，一旦超过，就停止清理，从而避免在清理时，大量的内存拷贝阻塞 Redis，导致 响应延迟升高。

  - **自动内存碎片清理机制在控制碎片清理启停的时机上，既考虑了碎片的空间占比、对 Redis 内存使用效率的影响，还考虑了清理机制本身的 CPU 时间占比、对 Redis 性能的影响。**而且，清理机制还提供了 4 个参数，让我们可以根据实际应用中的数据量需求和性能 要求灵活使用。









# 异步机制

- 我们必须要重视所有可能影响 Redis 性能的因素(例如命令操作、系统配置、关键机制、硬件配置等)

  - 影响 Redis 性能的 5 大方面的潜在因素，分别是:	
    - Redis 内部的阻塞式操作;
    - CPU 核和 NUMA 架构的影响; 
    - Redis 关键系统配置;
    - Redis 内存碎片;
    - Redis 缓冲区。

- **Redis** **实例有哪些阻塞点**

  - 和 Redis 实例交互的对象，以及交互时会发生的操作。
    - **客户端**:网络 IO，键值对增删改查操作，数据库操作; 
    - **磁盘**:生成 RDB 快照，记录 AOF 日志，AOF 日志重写;
    - **主从节点**:主库生成、传输 RDB 文件，从库接收 RDB 文件、清空数据库、加载 RDB 文件;
    - **切片集群实例**:向其他实例传输哈希槽信息，数据迁移。

- **和客户端交互时的阻塞点**

  - 网络 IO 有时候会比较慢，但是 Redis 使用了 **IO 多路复用机制**，避免了主线程一直处在等待网络连接或请求到来的状态，所以，网络 IO 不是导致 Redis 阻塞的因素。
  - 键值对的增删改查操作是 Redis 和客户端交互的主要部分，也是 Redis 主线程执行的主要 任务。所以，**复杂度高的增删改查操作肯定会阻塞 Redis。**
    - 那么，怎么判断操作复杂度是不是高呢?这里有一个最基本的标准，就是**看操作的复杂度 是否为 O(N)**。
    - Redis 中涉及集合的操作复杂度通常为 O(N)，我们要在使用时重视起来。例如集合元素全量查询操作 HGETALL、SMEMBERS，以及集合的聚合统计操作，例如求交、并和差集。 这些操作可以作为 Redis 的**第一个阻塞点:集合全量查询和聚合操作**。
    - **除此之外，集合自身的删除操作同样也有潜在的阻塞风险**。
      - 其实，删除操作的本质是要释放键值对占用的内存空间。你可不要小瞧内存的释放过程。 **释放内存只是第一步，为了更加高效地管理内存空间，在应用程序释放内存时，操作系统 需要把释放掉的内存块插入一个空闲内存块的链表，以便后续进行管理和再分配。这个过 程本身需要一定时间，而且会阻塞当前释放内存的应用程序**，所以，如果一下子释放了大量内存，空闲内存块链表操作时间就会增加，相应地就会造成 Redis 主线程的阻塞。
      - 三个结论
        - **当元素数量从 10 万增加到 100 万时**，4 大集合类型的删除时间的增长幅度从 5 倍上升 到了近 20 倍;
        - 集合元素越大，删除所花费的时间就越长;
        - **当删除有 100 万个元素的集合时**，最大的删除时间绝对值已经达到了 1.98s(Hash 类 型)。Redis 的响应时间一般在微秒级别，所以，一个操作达到了近 2s，不可避免地会 阻塞主线程。
      - **bigkey 删除操作就是 Redis 的第二个阻塞点**
    - 既然频繁删除键值对都是潜在的阻塞点了，那么，在 Redis 的数据库级别操作中，清空数 据库(例如 FLUSHDB 和 FLUSHALL 操作)必然也是一个潜在的阻塞风险，因为它涉及到 删除和释放所有的键值对。所以，这就是 **Redis 的第三个阻塞点:清空数据库**。

- **和磁盘交互时的阻塞点**

  - Redis 开发者早已认识到磁盘 IO 会带来阻塞，所以就把 Redis 进一步设计为**采用子进程的方式生成 RDB 快照文件，以及执行 AOF 日志重写操作。这样一来，这两个操 作由子进程负责执行，慢速的磁盘 IO 就不会阻塞主线程了。**
  - **Redis 直接记录 AOF 日志时，会根据不同的写回策略对数据做落盘保存。**一个同步写磁盘的操作的耗时大约是 1~2ms，如果有大量的写操作需要记录在 AOF 日志中，并同步写回的话，就会阻塞主线程了。这就得到了 Redis 的**第四个阻塞点了:AOF 日志同步 写**。

- **主从节点交互时的阻塞点**

  - **在主从集群中，主库需要生成 RDB 文件，并传输给从库。主库在复制的过程中，创建和传 输 RDB 文件都是由子进程来完成的，不会阻塞主线程。**但是，**对于从库来说，它在接收了 RDB 文件后，需要使用 FLUSHDB 命令清空当前数据库**，这就正好撞上了刚才我们分析的 **第三个阻塞点。**
  - 此外，从库在清空当前数据库后，还需要把 RDB 文件加载到内存，这个过程的快慢和 RDB 文件的大小密切相关，RDB 文件越大，加载过程越慢，所以，**加载 RDB 文件就成为了 Redis 的第五个阻塞点**。

- **切片集群实例交互时的阻塞点**

  - 当我们部署 Redis 切片集群时，每个 Redis 实例上分配的哈希槽信息需要在不同实 例间进行传递，同时，当需要进行负载均衡或者有实例增删时，数据会在不同的实例间进 行迁移。**不过，哈希槽的信息量不大，而数据迁移是渐进式执行的，所以，一般来说，这 两类操作对 Redis 主线程的阻塞风险不大。**
  - **不过，如果你使用了 Redis Cluster 方案，而且同时正好迁移的是 bigkey 的话，就会造成 主线程的阻塞，因为 Redis Cluster 使用了同步迁移。**

- **哪些阻塞点可以异步执行?**

  - 如果一个操作能被异步执行，就意味着，它并不是 Redis 主线程的关键路径上的操作。
    - **关键路径上的操作：客户端把请求发送给 Redis 后，等着 Redis 返回数据结果的操作。**
  - 对于 Redis 来说，**读操作是典型的关键路径操作**，因为客户端发送了读操作之后，就会等 待读取的数据返回，以便进行后续的数据处理。而 Redis 的第一个阻塞点**“集合全量查询 和聚合操作”都涉及到了读操作，所以，它们是不能进行异步操作了。**
  - 我们再来看看删除操作。删除操作并不需要给客户端返回具体的数据结果，所以不算是关 键路径操作。而我们刚才总结的第二个阻塞点“bigkey 删除”，和第三个阻塞点“清空数 据库”，都是对数据做删除，并不在关键路径上。因此，**我们可以使用后台子线程来异步执行删除操作。**
  - 对于第四个阻塞点“AOF 日志同步写”来说，为了保证数据可靠性，Redis 实例需要保证 AOF 日志中的操作记录已经落盘，这个操作虽然需要实例等待，但它并不会返回具体的数 据结果给实例。所以，**我们也可以启动一个子线程来执行 AOF 日志的同步写，而不用让主线程等待 AOF 日志的写完成。**
  - 最后，我们再来看下“从库加载 RDB 文件”这个阻塞点。**从库要想对客户端提供数据存取 服务，就必须把 RDB 文件加载完成。所以，这个操作也属于关键路径上的操作，我们必须 让从库的主线程来执行。**

- **异步的子线程机制**

  - **Redis 主线程启动后，会使用操作系统提供的 pthread_create 函数创建 3 个子线程**，分别 由它们负责 AOF 日志写操作、键值对删除以及文件关闭的异步执行。
  - 主线程通过一个链表形式的任务队列和子线程进行交互。
    - **当收到键值对删除和清空数据库的操作时，主线程会把这个操作封装成一个任务，放入到任务队列中，然后给客户端返回一个完成信息，表明删除已经完成。**
      - **但实际上，这个时候删除还没有执行，等到后台子线程从任务队列中读取任务后，才开始 实际删除键值对**，并释放相应的内存空间。因此，我们把这种异步删除也称为惰性删除 (lazy free)。此时，删除或清空操作不会阻塞主线程，这就避免了对主线程的性能影 响。
    - **和惰性删除类似，当 AOF 日志配置成 everysec 选项后**，主线程会把 AOF 写日志操作封 装成一个任务，也放到任务队列中。后台子线程读取任务后，开始自行写入 AOF 日志，这 样主线程就不用一直等待 AOF 日志写完了。

- **异步的键值对删除和数据库清空操作是 Redis 4.0 后提供 的功能**，Redis 也提供了新的命令来执行这两个操作。

  - 键值对删除:当你的集合类型中有大量元素(例如有百万级别或千万级别元素)需要删 除时，我建议你使用 UNLINK 命令。
  - 清空数据库:可以在 FLUSHDB 和 FLUSHALL 命令后加上 ASYNC 选项，这样就可以让 后台子线程异步地清空数据库

  ​	





# CPU架构

- CPU 的多核架构以及多 CPU 架构，也会影响到 Redis 的性能。

- **主流的** **CPU** **架构**

  - **一个 CPU 处理器中一般有多个运行核心，我们把一个运行核心称为一个物理核，每个物理 核都可以运行应用程序**。每个物理核都拥有私有的一级缓存(Level 1 cache，简称 L1 cache)，包括一级指令缓存和一级数据缓存，以及私有的二级缓存(Level 2 cache，简 称 L2 cache)。
    - 物理核的私有缓存。它其实是指缓存空间只能被当前的这个物 理核使用，其他的物理核无法对这个核的缓存空间进行数据存取
  - 但是，这些 L1 和 L2 缓存的大小受限于处理器的制造技术，一般只有 KB 级别，存不下太 多的数据。如果 L1、L2 缓存中没有所需的数据，应用程序就需要访问内存来获取数据。
    - **所以，不同的物理核还会共享一个共同的三级缓存(Level 3 cache，简称为 L3 cache)。** L3 缓存能够使用的存储资源比较多，所以一般比较大，能达到几 MB 到几十 MB，这就能 让应用程序缓存更多的数据。当 L1、L2 缓存中没有数据缓存时，可以访问 L3，尽可能避 免访问内存。
  - 另外，现在主流的 CPU 处理器中，每个物理核通常都会运行两个超线程，也叫作逻辑核。 同一个物理核的逻辑核会共享使用 L1、L2 缓存。
  - **在主流的服务器上，一个 CPU 处理器会有 10 到 20 多个物理核。同时，为了提升服务器 的处理能力，服务器上通常还会有多个 CPU 处理器(也称为多 CPU Socket)**，每个处理器有自己的物理核(包括 L1、L2 缓存)，L3 缓存，以及连接的内存，同时，不同处理器 间通过总线连接。
    - **在多 CPU 架构上，应用程序可以在不同的处理器上运行**。
    - **如果应用程序先在一个 Socket 上运行，并且把数据保存 到了内存，然后被调度到另一个 Socket 上运行**，此时，应用程序再进行内存访问时，就需 要访问之前 Socket 上连接的内存，这种访问属于**远端内存访问**。**和访问 Socket 直接连接 的内存相比，远端内存访问会增加应用程序的延迟。**
    - 在多 CPU 架构下，一个应用程序访问所在 Socket 的本地内存和访问远端内存的延迟并不 一致，所以，我们也把这个架构称为**非统一内存访问架构(Non-Uniform Memory Access，NUMA 架构)。**

- **CPU** **多核对** **Redis** **性能的影响**

  - **在一个 CPU 核上运行时**，应用程序需要记录自身使用的软硬件资源信息(例如栈指针、 CPU 核的寄存器值等)，我们把这些信息称为**运行时信息**。同时，应用程序访问最频繁的 指令和数据还会被缓存到 L1、L2 缓存上，以便提升执行速度。

  - **但是，在多核 CPU 的场景下，一旦应用程序需要在一个新的 CPU 核上运行，那么，运行 时信息就需要重新加载到新的 CPU 核上**。而且，新的 CPU 核的 L1、L2 缓存也需要重新 加载数据和指令，这会导致程序的运行时间增加。

  - **context switch 是指线程的上下文切换**，这里的上下文就是线程的运行时信息。在 CPU 多 核的环境中，一个线程先在一个 CPU 核上运行，之后又切换到另一个 CPU 核上运行，这 时就会发生 context switch。

    - 当 context switch 发生后，Redis 主线程的运行时信息需要被重新加载到另一个 CPU 核 上，而且，此时，另一个 CPU 核上的 L1、L2 缓存中，并没有 Redis 实例之前运行时频繁 访问的指令和数据，所以，这些指令和数据都需要重新从 L3 缓存，甚至是内存中加载。
    - 如果在 CPU 多核场景下，Redis 实例被频繁调度到不同 CPU 核上运行的话，那么，对 Redis 实例的请求处理时间影响就更大了。**每调度一次，一些请求就会受到运行时信息、 指令和数据重新加载过程的影响，这就会导致某些请求的延迟明显高于其他请求**。

  - 所以，我们要避免 Redis 总是在不同 CPU 核上来回调度执行。于是，我们尝试着把 Redis 实例和 CPU 核绑定了，让一个 Redis 实例固定运行在一个 CPU 核上。我们可以使用 **taskset 命令把一个程序绑定在一个核上运行。**

    - ```shell
      # 把 Redis 实例绑在了 0 号核上，其中，“-c”选项用于 设置要绑定的核编号。
      taskset -c 0 ./redis-server
      ```

- **CPU** **的** **NUMA** **架构对** **Redis** **性能的影响**

  - 在实际应用 Redis 时，我经常看到一种做法，**为了提升 Redis 的网络性能，把操作系统的 网络中断处理程序和 CPU 核绑定。这个做法可以避免网络中断处理程序在不同核上来回调度执行，的确能有效提升 Redis 的网络处理性能。**

  - **Redis 实例和网络中断程序的数据交互**:网络中断处理程序从网卡硬件中读 取数据，并把数据写入到操作系统内核维护的一块**内存缓冲区**。**内核**会通过 **epoll 机制**触发事件，通知 Redis 实例，Redis 实例再把数据从内核的内存缓冲区拷贝到自己的内存空间。

  - 一个潜在的风险:**如果网络中断处理程序和 Redis 实例各自所绑的 CPU 核不在同一个 CPU Socket 上，那么，Redis 实例读取网络数据时，就需要跨 CPU Socket 访 问内存，这个过程会花费较多时间。**

  - 所以，为了避免 Redis 跨 CPU Socket 访问网络数据，我们最好把网络中断程序和 Redis 实例绑在同一个 CPU Socket 上，这样一来，Redis 实例就可以直接从本地内存读取网络数据了。

  - **在 CPU 的 NUMA 架构下，对 CPU 核的编号规则，并不是先把一 个 CPU Socket 中的所有逻辑核编完，再对下一个 CPU Socket 中的逻辑核编码，而是先给每个 CPU Socket 中每个物理核的第一个逻辑核依次编号，再给每个 CPU Socket 中的 物理核的第二个逻辑核依次编号。**

    - 我给你举个例子。假设有 2 个 CPU Socket，每个 Socket 上有 6 个物理核，每个物理核又有 2 个逻辑核，总共 24 个逻辑核。我们可以执行 **lscpu 命令**，查看到这些核的编号。

    - ```shell
      1 lscpu 2
      3 Architecture: x86_64
      4 ...
      5 NUMA node0 CPU(s): 0-5,12-17
      6 NUMA node1 CPU(s): 6-11,18-23
      ```

    - NUMA node0 的 CPU 核编号是 0 到 5、12 到 17。其中，0 到 5 是 node0 上的 6 个物理核中的第一个逻辑核的编号，12 到 17 是相应物理核中的第二个逻辑核编 号。NUMA node1 的 CPU 核编号规则和 node0 一样。

    - **所以，你一定要注意 NUMA 架构下 CPU 核的编号方法，这样才不会绑错核。**

- **绑核的风险和解决方案**

  - 当我们把 Redis 实例绑到一个 CPU 逻辑核上时，就会导致子进程、后台线程和 Redis 主 线程竞争 CPU 资源，一旦子进程或后台线程占用 CPU 时，主线程就会被阻塞，导致 Redis 请求延迟增加
  - **方案一:一个 Redis 实例对应绑一个物理核**
    - 在给 Redis 实例绑核时，我们不要把一个实例和一个逻辑核绑定，而要和一个物理核绑 定，也就是说，把一个物理核的 2 个逻辑核都用上。
  - **方案二:优化 Redis 源码**







# **消息队列**

- **消息队列的消息存取需求**

  - 不过，消息队列在存取消息时，**必须要满足三个需求，分别是消息保序、处理重复的消息和保证消息可靠性。**
  - Redis 的 List 和 Streams 两种数据类型，就可以满足消息队列的这三个需求

- **基于** **Streams** **的消息队列解决方案**

  - Streams 是 Redis 专门为消息队列设计的数据类型，它提供了丰富的消息队列操作命令。

  - > XADD:插入消息，保证有序，可以自动生成全局唯一 ID;
    >
    > XREAD:用于读取消息，可以按 ID 读取数据;
    >
    > XREADGROUP:按消费组形式读取消息;
    >
    > XPENDING 和 XACK:XPENDING 命令可以用来查询每个消费组内所有消费者已读取 但尚未确认的消息，而 XACK 命令用于向消息队列确认消息处理已完成。

  - 消息队列中的消息一旦被消费组里的一个消费者读取了，就不能再被该消 费组内的其他消费者读取了。

- 相比 Redis 来说，Kafka 和 RabbitMQ 一般被认为是重量级的消息队列。









# **缓存异常**

- 缓存和数据库的数据不一致是如何发生的?

  - 这里的“一致性”包含了两种情况:
    - 缓存中有数据，那么，缓存的数据值需要和数据库中的值相同;
    - 缓存中本身没有数据，那么，数据库中的值必须是最新值。
  - 当缓存的读写模 式不同时，缓存数据不一致的发生情况不一样，我们的应对方法也会有所不同

- 对于读写缓存来说，如果要对数据进行增删改，就需要在缓存中进行，同时还要根据采取的写回策略，决定是否同步写回到数据库中。

  - **同步直写策略**:写缓存时，也同步写数据库，缓存和数据库中的数据一致;
  - **异步写回策略**:写缓存时不同步写数据库，等到数据从缓存中淘汰时，再写回数据库。使用这种策略时，如果数据还没有写回数据库，缓存就发生了故障，那么，此时，数据库就没有最新的数据了。

- 对于读写缓存来说，**要想保证缓存和数据库中的数据一致，就要采用同步直写策略**。不过，需要注意的是，**如果采用这种策略，就需要同时更新缓存和数据库。所以，我们要在业务应用中使用事务机制**，来保证缓存和数据库的更新具有**原子性**。

  - 当然，**在有些场景下，我们对数据一致性的要求可能不是那么高**，比如说缓存的是电商商品的非关键属性或者短视频的创建或修改时间等，那么，我们可以使用异步写回策略。

- 只读缓存

  - **对于只读缓存来说，如果有数据新增，会直接写入数据库;而有数据删改时，就需要把只读缓存中的数据标记为无效。**这样一来，应用后续再访问这些增删改的数据时，因为缓存中没有相应的数据，就会发生缓存缺失。此时，应用再从数据库中把数据读入缓存，这样后续再访问数据时，就能够直接从缓存中读取了。

- 写入和删改数据

  - **如果是新增数据**，数据会直接写到数据库中，不用对缓存做任何操作，此时，缓存中本身 就没有新增数据，而数据库中是最新值，这种情况符合我们刚刚所说的一致性的第 2 种情 况，所以，此时，缓存和数据库的数据是一致的。
  - 如果发生删改操作，应用既要更新数据库，也要在缓存中删除数据。这两个操作如果无法保证原子性，也就是说，要不都完成，要不都没完成，此时，就会出现数据不一致问题了

- 重试机制

  - **具体来说，可以把要删除的缓存值或者是要更新的数据库值暂存到消息队列中(例如使用 Kafka 消息队列)。**当应用没有能够成功地删除缓存值或者是更新数据库值时，可以从消 息队列中重新读取这些值，然后再次进行删除或更新。
  - 如果能够成功地删除或更新，我们就要把这些值从消息队列中去除，以免重复操作，此时，我们也可以保证数据库和缓存的数据一致了。否则的话，我们还需要再次进行重试。如果重试超过的一定次数，还是没有成功，我们就需要向业务层发送报错信息了。
  - **实际上，即使这两个操作第一次执行时都没有失败，当有大量并发请求时，应用还是有可能读到不一致的数据。**

- **情况一:先删除缓存，再更新数据库。**

  - 假设线程 A 删除缓存值后，还没有来得及更新数据库(比如说有网络延迟)，线程 B 就开 始读取数据了，那么这个时候，线程 B 会发现缓存缺失，就只能去数据库读取。

    - 1. 线程 B 读取到了旧值;

      2. 线程 B 是在缓存缺失的情况下读取的数据库，所以，它还会把旧值写入缓存，这可能会 导致其他线程从缓存中读到旧值。

    - 等到线程 B 从数据库读取完数据、更新了缓存后，线程 A 才开始更新数据库，此时，缓存 中的数据是旧值，而数据库中的是最新值，两者就不一致了。

  - 解决方案

    - **在线程 A 更新完数据库值以后，我们可以让它先 sleep 一小段时间，再进行一次缓存删除 操作。**
    - 之所以要加上 sleep 的这段时间，就是为了让线程 B 能够先从数据库读取数据，再把缺失 的数据写入缓存，然后，线程 A 再进行删除。所以，线程 A sleep 的时间，就需要大于线 程 B 读取数据再写入缓存的时间。这个时间怎么确定呢?建议你在业务程序运行的时候， 统计下线程读数据和写缓存的操作时间，以此为基础来进行估算。
    - 这样一来，其它线程读取数据时，会发现缓存缺失，所以会从数据库中读取最新值。因为这个方案会在第一次删除缓存值后，延迟一段时间再次进行删除，所以我们也把它叫做“延迟双删”。

  - “延迟双删”方案的示例

    - ```sql
      redis.delKey(X) 
      db.update(X)
      Thread.sleep(N) 
      redis.delKey(X)
      ```

- **情况二:先更新数据库值，再删除缓存值。**

  - 如果线程 A 删除了数据库中的值，但还没来得及删除缓存值，线程 B 就开始读取数据了， 那么此时，线程 B 查询缓存时，发现缓存命中，就会直接从缓存中读取旧值。不过，在这 种情况下，**如果其他线程并发读缓存的请求不多，那么，就不会有很多请求读取到旧值。** 而且，线程 A 一般也会很快删除缓存值，这样一来，其他线程再次读取时，就会发生缓存 缺失，进而从数据库中读取最新值。所以，这种情况对业务的影响较小。

- 缓存和数据库的数据不一致一般是由两个原因导致的

  - 删除缓存值或更新数据库失败而导致数据不一致，你可以使用重试机制确保删除或更新操作成功。
  - 在删除缓存值、更新数据库的这两步操作中，有其他线程的并发读操作，导致其他线程读取到旧值，应对方案是延迟双删。









# 缓存异常的三个问题

- **缓存雪崩**
  - 缓存雪崩是指大量的应用请求无法在 Redis 缓存中进行处理，紧接着，应用将大量请求发 送到数据库层，导致数据库层的压力激增。
  - 第一个原因是:**缓存中有大量数据同时过期，导致大量请求无法得到处理。**
    - **针对大量数据同时失效带来的缓存雪崩问题，提供两种解决方案。**
      - **首先，我们可以避免给大量的数据设置相同的过期时间。**如果业务层的确要求有些数据同 时失效，你可以在用 EXPIRE 命令给每个数据设置过期时间时，给这些数据的过期时间增 加一个较小的随机数
      - 除了微调过期时间，我们还可以通过服务降级，来应对缓存雪崩。
        -  当业务应用访问的是非核心数据(例如电商商品属性)时，暂时停止从缓存中查询这些数据，而是直接返回预定义信息、空值或是错误信息;
        - 当业务应用访问的是核心数据(例如电商商品库存)时，仍然允许查询缓存，如果缓存缺失，也可以继续通过数据库读取。
  - 第二个原因是：**Redis 缓存实例发生故障宕机了**，无法处理请求，这就会导致大量请求一下子积压到数据库层， 从而发生缓存雪崩。
    - 提供两个建议。
      - **第一个建议，是在业务系统中实现服务熔断或请求限流机制。**
        - 所谓的服务熔断，是指在发生缓存雪崩时，为了防止引发连锁的数据库雪崩，甚至是整个 系统的崩溃，我们**暂停业务应用对缓存系统的接口访问。**再具体点说，就是业务应用调用 缓存接口时，缓存客户端并不把请求发给 Redis 缓存实例，而是直接返回，等到 Redis 缓 存实例重新恢复服务后，再允许应用请求发送到缓存系统。
        - **在业务系统运行时，我们可以监测 Redis 缓存所在机器和数据库所在机器的负载指标**，例 如每秒请求数、CPU 利用率、内存利用率等。如果我们发现 Redis 缓存实例宕机了，而数 据库所在机器的负载压力突然增加(例如每秒请求数激增)，此时，就发生缓存雪崩了。 大量请求被发送到数据库进行处理。我们可以启动服务熔断机制，暂停业务应用对缓存服 务的访问，从而降低对数据库的访问压力
      - **服务熔断虽然可以保证数据库的正常运行，但是暂停了整个缓存系统的访问，对业务应用的影响范围大。为了尽可能减少这种影响，我们也可以进行请求限流。**这里说的请求限流，就是指，我们在业务系统的请求入口前端控制每秒进入系统的请求数，避免过多的请求被发送到数据库。
    - **第二个建议就是事前预防**
      - 通过**主从节点**的方式构建 Redis 缓存高可靠集群。如果 Redis 缓存的主节点故障宕机了， 从节点还可以切换成为主节点，继续提供缓存服务，避免了由于缓存实例宕机而导致的缓 存雪崩问题。
- **缓存击穿**
  - **缓存击穿是指，针对某个访问非常频繁的热点数据的请求，无法在缓存中进行处理**，紧接着，访问该数据的大量请求，一下子都发送到了后端数据库，导致了数据库压力激增，会影响数据库处理其他请求。
  - **为了避免缓存击穿给数据库带来的激增压力，我们的解决方法也比较直接，对于访问特别 频繁的热点数据，我们就不设置过期时间了**。这样一来，对热点数据的访问请求，都可以 在缓存中进行处理，而 Redis 数万级别的高吞吐量可以很好地应对大量的并发请求访问。
- **缓存穿透**
  - **缓存穿透是指要访问的数据既不在 Redis 缓存中，也不在数据库中**，导致请求在访问缓存 时，发生缓存缺失，再去访问数据库时，发现数据库中也没有要访问的数据。
  - 缓存穿透有两种情况。
    - 业务层误操作:缓存中的数据和数据库中的数据被误删除了，所以缓存和数据库中都没有数据;
    - 恶意攻击:专门访问数据库中没有的数据。
  - 三种应对方案
    - **第一种方案是，缓存空值或缺省值。**
      - 一旦发生缓存穿透，我们就可以针对查询的数据，在 Redis 中缓存一个空值或是和业务层 协商确定的缺省值(例如，库存的缺省值可以设为 0)。紧接着，应用发送的后续请求再 进行查询时，就可以直接从 Redis 中读取空值或缺省值，返回给业务应用了
    - 第二种方案是，使用**布隆过滤器**快速判断数据是否存在，避免从数据库中查询数据是否存在，减轻数据库压力。
    - 最后一种方案是，在请求入口的**前端进行请求检测。**
      - 缓存穿透的一个原因是有大量的恶意 请求访问不存在的数据，所以，**一个有效的应对方案是在请求入口前端，对业务系统接收 到的请求进行合法性检测**，把恶意的请求(例如请求参数不合理、请求参数是非法值、请 求字段不存在)直接过滤掉，不让它们访问后端缓存和数据库。这样一来，也就不会出现 缓存穿透问题了。













# **缓存污染**

- 在一些场景下，有些数据被访问的次数非常少，甚至只会被访问一次。当这些数据服务完访问请求后，如果还继续留存在缓存中的话，就只会白白占用缓存
  空间。这种情况，就是缓存污染。
  - 缓存污染一旦变得严重后，就会有大量不再访问的数据滞留在缓存中。如果这时数据占满了缓存空间，**我们再往缓存中写入新数据时，就需要先把这些数据逐步淘汰出缓存，这就会引入额外的操作时间开销**，进而会影响应用的性能。
- **如何解决缓存污染问题?**
  - 8 种数据淘汰策略：它们分别是 noeviction、volatile-random、volatile-ttl、volatile-lru、volatile-lfu、allkeys-lru、 allkeys-random 和 allkeys-lfu 策略。
  - **volatile-random 和 allkeys-random** 这两种策略。它们都是采用随机 挑选数据的方式，来筛选即将被淘汰的数据。
    - 既然是随机挑选，那么 Redis 就不会根据数据的访问情况来筛选数据。如果被淘汰的数据 又被访问了，就会发生缓存缺失。所以，volatile-random 和 allkeys-random 策略，在避免缓 存污染这个问题上的效果非常有限。
  -  **volatile-ttl** 策略
    - 虽然 volatile-ttl 策略不再是随机选择淘汰数据了，但是剩余存活时间并不能直接反映数据 再次访问的情况。所以，按照 volatile-ttl 策略淘汰数据，和按随机方式淘汰数据类似，也 可能出现数据被淘汰后，被再次访问导致的缓存缺失问题。
  - **一种例外的情况:业务应用在给数据设置过期时间的时候，就明确知 道数据被再次访问的情况，并根据访问情况设置过期时间**。此时，Redis 按照数据的剩余 最短存活时间进行筛选，是可以把不会再被访问的数据筛选出来的，进而避免缓存污染。 例如，业务部门知道数据被访问的时长就是一个小时，并把数据的过期时间设置为一个小 时后。这样一来，被淘汰的数据的确是不会再被访问了。
- **LRU** **缓存策略**
  - 但是，也正是**因为只看数据的访问时间，使用 LRU 策略在处理扫描式单次查询操作时，无 法解决缓存污染**。所谓的扫描式单次查询操作，就是指应用对大量的数据进行一次全体读 取，每个数据都会被读取，而且只会被读取一次。此时，因为这些被查询的数据刚刚被访 问过，所以 lru 字段值都很大。
- **LFU** **缓存策略的优化**
  - **LFU 缓存策略是在 LRU 策略基础上，为每个数据增加了一个计数器，来统计这个数据的访 问次数。**当使用 LFU 策略筛选淘汰数据时，首先会根据数据的访问次数进行筛选，把访问 次数最低的数据淘汰出缓存。如果两个数据的访问次数相同，LFU 策略再比较这两个数据 的访问时效性，把距离上一次访问时间更久的数据淘汰出缓存。
  - 为了避免操作链表的开销，Redis 在实现 LRU 策略时使用了两个近似方法:
    - **Redis 是用 RedisObject 结构来保存数据的**，RedisObject 结构中设置了一个 lru 字 段，用来记录数据的访问时间戳;
    - **Redis 并没有为所有的数据维护一个全局的链表，而是通过随机采样方式**，选取一定数量(例如 10 个)的数据放入候选集合，后续在候选集合中根据 lru 字段值的大小进行筛选。
  - 在此基础上，**Redis 在实现 LFU 策略的时候，只是把原来 24bit 大小的 lru 字段，又进一 步拆分成了两部分**。
    - ldt 值:lru 字段的前 16bit，表示数据的访问时间戳; 
    - counter 值:lru 字段的后 8bit，表示数据的访问次数。
  - **Redis 只使用了 8bit 记录数据的访问次数，而 8bit 记录的最大值是 255**，这样可以吗?
    - 因此，**在实现 LFU 策略时，Redis 并没有采用数据每 被访问一次，就给对应的 counter 值加 1 的计数规则，而是采用了一个更优化的计数规 则**。
    - 简单来说，LFU 策略实现的计数规则是:每当数据被访问一次时，首先，用计数器当前的值乘以配置项 lfu_log_factor 再加 1，再取其倒数，得到一个 p 值;然后，把这个 p 值和 一个取值范围在(0，1)间的随机数 r 值比大小，只有 p 值大于 r 值时，计数器才加 1。
    - 使用了这种计算规则后，我们可以通过设置不同的 lfu_log_factor 配置项，来控制计数器 值增加的速度，避免 counter 值很快就到 255 了。
  - 应用负载的情况是很复杂的。在一些场景下，有些数据在短时间内被 大量访问后就不会再被访问了。那么再按照访问次数来筛选的话，这些数据会被留存在缓 存中，但不会提升缓存命中率。为此，**Redis 在实现 LFU 策略时，还设计了一个 counter 值的衰减机制。**
    - 简单来说，LFU 策略使用**衰减因子配置项 lfu_decay_time 来控制访问次数的衰减。**LFU 策 略会计算当前时间和数据最近一次访问时间的差值，并把这个差值换算成以分钟为单位。 然后，LFU 策略再把这个差值除以 lfu_decay_time 值，所得的结果就是数据 counter 要 衰减的值。
    - 简单举个例子，假设 lfu_decay_time 取值为 1，如果数据在 N 分钟内没有被访问，那么 它的访问次数就要减 N。如果 lfu_decay_time 取值更大，那么相应的衰减值会变小，衰减 效果也会减弱。所以，如果业务应用中有短时高频访问的数据的话，建议把 lfu_decay_time 值设置为 1，这样一来，LFU 策略在它们不再被访问后，会较快地衰减它 们的访问次数，尽早把它们从缓存中淘汰出去，避免缓存污染。







# **无锁的原子操作**

- **比如说如果多个用户同时下单，就会对缓存在 Redis 中的商品库存并发更新。一旦有了并发写操作，数据就会被修改**，如果我们没有对并发写请求做好控制，就可能导致数据被改错。
- 为了保证并发访问的正确性，Redis 提供了两种方法，分别是加锁和原子操作。
  - **原子操作是另一种提供并发访问控制的方法**。原子操作是指执行过程保持原子性的操作， 而且原子操作执行时并不需要再加锁，实现了无锁操作。这样一来，既能保证并发控制， 还能减少对系统并发性能的影响。
- **并发访问中需要对什么进行控制?**
  - 读取 - 修改 - 写回”操作(Read-Modify-Write，简称为 RMW 操 作)。**当有多个客户端对同一份数据执行 RMW 操作的话，我们就需要让 RMW 操作涉及 的代码以原子性方式执行。**访问同一份数据的 RMW 操作代码，就叫做临界区代码。
- **Redis** **的两种原子操作方法**
  - 把多个操作在 Redis 中实现成一个操作，也就是单命令操作;
    - Redis 提供了 INCR/DECR 命令，把这三个操作转变为一个原子操作了。 INCR/DECR 命令可以对数据进行**增值 / 减值**操作，而且它们本身就是单个命令操作， Redis 在执行它们时，本身就具有互斥性。
  - 把多个操作写到一个 Lua 脚本中，以原子性方式执行单个 Lua 脚本。
    - 如果我们要执行的操作不是简单地增减数据，而是有更加复杂的判断逻辑或者是其 他操作，那么，Redis 的单命令操作已经无法保证多个操作的互斥执行了。所以，这个时 候，我们需要使用第二个方法，也就是 Lua 脚本。
    - **在编写 Lua 脚本时，你要避免把不需要做并发控制的操作写入脚本中**。







# **分布式锁**

- 在分布式系统中，当有多个客户端需要获取锁时，我们需要分布式锁。此时，锁是保存在一个共享存储系统中的，可以被多个客户端共享访问和获取。

- 实现分布式锁的两个要求

  - 要求一:分布式锁的加锁和释放锁的过程，涉及多个操作。所以，**在实现分布式锁时，我们需要保证这些锁操作的原子性;**
  - 要求二:共享存储系统保存了锁变量，如果共享存储系统发生故障或宕机，那么客户端也就无法进行锁操作了。**在实现分布式锁时，我们需要考虑保证共享存储系统的可靠性，进而保证锁的可靠性。**

- **基于单个** **Redis** **节点实现分布式锁**

  - 加锁包含了三个操作(读取锁变量、判断锁变量值以及把锁变量值设置为 1)，而这 三个操作在执行时需要保证原子性。那怎么保证原子性呢？

  - 有两种通用的方法，分别是使用 Redis 的单命令操作和使用 Lua 脚本。

    - 首先是 SETNX 命令，它用于设置键值对的值。具体来说，就是这个命令在执行时会判断键 值对是否存在，如果不存在，就设置键值对的值，如果存在，就不做任何设置。**总结来说，我们就可以用 SETNX 和 DEL 命令组合来实现加锁和释放锁操作。**

      - 不过，使用 SETNX 和 DEL 命令组合实现分布锁，存在两个潜在的风险。

      - **第一个风险是，假如某个客户端在执行了 SETNX 命令、加锁之后，紧接着却在操作共享数 据时发生了异常，结果一直没有执行最后的 DEL 命令释放锁**。因此，锁就一直被这个客户 端持有，其它客户端无法拿到锁，也无法访问共享数据和执行后续操作，这会给业务应用 带来影响。

        - > 线程1加锁成功，结果处理业务发生耗时，超过了锁的过期时间，锁自动过期了。
          >
          > 此时线程2进来，加锁成功，开始处理业务，之后线程1处理业务结束了，释放的是线程2的锁，那线程2的锁相当于失效了。

        - 针对这个问题，一个有效的解决方法是，**给锁变量设置一个过期时间**。

      - **第二个风险。如果客户端 A 执行了 SETNX 命令加锁后，假设客户端 B 执行了 DEL 命令释放锁**，此时，客户端 A 的锁就被误释放了。如果客户端 C 正好也在申请加锁， 就可以成功获得锁，进而开始操作共享数据。这样一来，客户端 A 和 C 同时在对共享数据 进行操作，数据就会被修改错误，这也是业务层不能接受的。

        - 在使用 SETNX 命令进行加锁的方法中，我们通过把锁变量值设置为 1 或 0，表示是否加锁 成功。1 和 0 只有两种状态，无法表示究竟是哪个客户端进行的锁操作。所以，**我们在加 锁操作时，可以让每个客户端给锁变量设置一个唯一值，**这里的唯一值就可以用来标识当 前操作的客户端。在释放锁操作时，客户端需要判断，当前锁变量的值是否和自己的唯一 标识相等，只有在相等的情况下，才能释放锁。这样一来，就不会出现误释放锁的问题 了。

  - **在释放锁操作中，我们使用了 Lua 脚本，这是因为，释放锁操作的逻 辑也包含了读取锁变量、判断值、删除锁变量的多个操作，**而 Redis 在执行 Lua 脚本时， 可以以原子性的方式执行，从而保证了锁释放操作的原子性。

- **基于多个** **Redis** **节点实现高可靠的分布式锁**

  - **Redlock 算法的基本思路，是让客户端和多个独立的 Redis 实例依次请求加锁**，如果客户 端能够和半数以上的实例成功地完成加锁操作，那么我们就认为，客户端成功地获得分布 式锁了，否则加锁失败。
  - Redlock 算法的实现需要有 N 个独立的 Redis 实例。接下来，我们可以分成 3 步来完成加锁操作。
    - 第一步是，客户端获取当前时间。
    - 第二步是，客户端按顺序依次向 N 个 Redis 实例执行加锁操作。
    - 第三步是，一旦客户端完成了和所有 Redis 实例的加锁操作，客户端就要计算整个加锁过 程的总耗时。
      - 客户端只有在满足下面的这两个条件时，才能认为是加锁成功。
        - 条件一:客户端从**超过半数**(大于等于 N/2+1)的 Redis 实例上成功获取到了锁; 
        - 条件二:客户端获取锁的**总耗时没有超过锁的有效时间**。
  - 在 Redlock 算法中，释放锁的操作和在单实例上释放锁的操作一样，只要执行释放锁的 Lua 脚本就可以了。这样一来，只要 N 个 Redis 实例中的半数以上实例能正常工作，就能 保证分布式锁的正常工作了。

- 简单总结，基于 Redis 使用分布锁的注意点：

  1、使用 SET $lock_key $unique_val EX $second NX 命令保证加锁原子性，并为锁设置过期时间

  2、锁的过期时间要提前评估好，要大于操作共享资源的时间

  3、每个线程加锁时设置随机值，释放锁时判断是否和加锁设置的值一致，防止自己的锁被别人释放

  4、释放锁时使用 Lua 脚本，保证操作的原子性

  5、基于多个节点的 Redlock，加锁时超过半数节点操作成功，并且获取锁的耗时没有超过锁的有效时间才算加锁成功

  6、Redlock 释放锁时，要对所有节点释放（即使某个节点加锁失败了），因为加锁时可能发生服务端加锁成功，由于网络问题，给客户端回复网络包失败的情况，所以需要把所有节点可能存的锁都释放掉

  7、使用 Redlock 时要避免机器时钟发生跳跃，需要运维来保证，对运维有一定要求，否则可能会导致 Redlock 失效。例如共 3 个节点，线程 A 操作 2 个节点加锁成功，但其中 1 个节点机器时钟发生跳跃，锁提前过期，线程 B 正好在另外 2 个节点也加锁成功，此时 Redlock 相当于失效了（Redis 作者和分布式系统专家争论的重要点就在这）

  8、如果为了效率，使用基于单个 Redis 节点的分布式锁即可，此方案缺点是允许锁偶尔失效，优点是简单效率高

  9、如果是为了正确性，业务对于结果要求非常严格，建议使用 Redlock，但缺点是使用比较重，部署成本高





# **事务机制**

- **Redis** **的事务机制能保证哪些属性?**
  - **原子性**
    - MULTI 和 EXEC 配合使用，就可以保证多 个操作都完成。但是，如果事务执行发生错误了，原子性还能保证吗?我们需要分三种情 况来看。
    - 第一种情况是，**在执行 EXEC 命令前，客户端发送的操作命令本身就有错误**(比如语法错 误，使用了不存在的命令)，在命令入队时就被 Redis 实例判断出来了。保证了原子 性。
    - 和第一种情况不同的是，**事务操作入队时，命令和操作的数据类型不匹配，但 Redis 实例 没有检查出错误**。但是，在执行完 EXEC 命令以后，Redis 实际执行这些事务操作时，就会报错。不过，需要注意的是，虽然 Redis 会对错误命令报错，但还是会把正确的命令执 行完。在这种情况下，事务的原子性就无法得到保证了。
      - 其实，Redis 中并没有提供回滚机制。虽然 Redis 提供了 DISCARD 命令，但是，这个命 令只能用来主动放弃事务执行，把暂存的命令队列清空，起不到回滚的效果。
    - 第三种情况:**在执行事务的 EXEC 命令时，Redis 实例发生了故障， 导致事务执行失败**。
      - 在这种情况下，如果 Redis 开启了 AOF 日志，那么，只会有部分的事务操作被记录到 AOF 日志中。我们需要使用 redis-check-aof 工具检查 AOF 日志文件，这个工具可以把 已完成的事务操作从 AOF 文件中去除。这样一来，我们使用 AOF 恢复实例后，事务操作 不会再被执行，从而保证了原子性。
      - 当然，如果 AOF 日志并没有开启，那么实例重启后，数据也都没法恢复了，此时，也就谈 不上原子性了。
  - **一致性**
    - 情况一:命令入队时就报错
      - 在这种情况下，事务本身就会被放弃执行，所以可以保证数据库的一致性。
    - 情况二:命令入队时没报错，实际执行时报错
      - 在这种情况下，有错误的命令不会被执行，正确的命令可以正常执行，也不会改变数据库的一致性。
    - **情况三:EXEC 命令执行时实例发生故障**
      - 在这种情况下，实例故障后会进行重启，这就和数据恢复的方式有关了，我们要根据实例 是否开启了 RDB 或 AOF 来分情况讨论下。
      - **如果我们没有开启 RDB 或 AOF**，那么，实例故障重启后，数据都没有了，数据库是一致 的。
      - **如果我们使用了 RDB 快照**，因为 RDB 快照不会在事务执行时执行，所以，事务命令操作 的结果不会被保存到 RDB 快照中，使用 RDB 快照进行恢复时，数据库里的数据也是一致 的。
      - **如果我们使用了 AOF 日志**，而事务操作还没有被记录到 AOF 日志时，实例就发生了故 障，那么，使用 AOF 日志恢复的数据库数据是一致的。如果只有部分操作被记录到了 AOF 日志，我们可以使用 redis-check-aof 清除事务中已经完成的操作，数据库恢复后也 是一致的。
  - **隔离性**
    - 事务的隔离性保证，会受到和事务一起执行的并发操作的影响。而事务执行又可以分成命 令入队(EXEC 命令执行前)和命令实际执行(EXEC 命令执行后)两个阶段，所以，我们 就针对这两个阶段，分成两种情况来分析:
      - 并发操作在 EXEC 命令前执行，此时，隔离性的保证要使用 WATCH 机制来实现，否则 隔离性无法保证;
        - **WATCH 机制的作用是，在事务执行前，监控一个或多个键的值变化情况**，当事务调用 EXEC 命令执行时，WATCH 机制会先检查监控的键是否被其它客户端修改了。如果修改了，就放弃事务执行，避免事务的隔离性被破坏。然后，客户端可以再次执行事务，此 时，如果没有并发修改事务数据的操作了，事务就能正常执行，隔离性也得到了保证。
        - **如果没有使用 WATCH 机制，在 EXEC 命令前执行的并发操作是会对数据进行读写 的。**而且，在执行 EXEC 命令的时候，事务要操作的数据已经改变了，在这种情况下， Redis 并没有做到让事务对其它操作隔离，隔离性也就没有得到保障
      - 并发操作在 EXEC 命令后执行，此时，隔离性可以保证。
        - 刚刚说的是并发操作在 EXEC 命令前执行的情况，第二种情况:**并发操作在 EXEC 命令之后被服务器端接收并执行**。
        - 因为 Redis 是用单线程执行命令，而且，EXEC 命令执行后，Redis 会保证先把命令队列中的所有命令执行完。所以，在这种情况下，并发操作不会破坏事务的隔离性。
  - **持久性**
    - 因为 Redis 是内存数据库，所以，数据是否持久化保存完全取决于 Redis 的持久化配置模 式。
      - **如果 Redis 没有使用 RDB 或 AOF**，那么事务的持久化属性肯定得不到保证。
      - **如果 Redis 使用了 RDB 模式**，那么，在一个事务执行后，而下一次的 RDB 快照还未执行前，如果发 生了实例宕机，这种情况下，事务修改的数据也是不能保证持久化的。
      - **如果 Redis 采用了 AOF 模式**，因为 AOF 模式的三种配置选项 no、everysec 和 always 都会存在数据丢失的情况，所以，事务的持久性属性也还是得不到保证。
    - **所以，不管 Redis 采用什么持久化模式，事务的持久性属性是得不到保证的。**







# **Redis**主从同步与故障切换

- **主从数据不一致**
  - 主从数据不一致，就是指客户端从从库中读取到的值和主库中的最新值并不一致。
  - 其实这是因为**主从库间的命令复制是异步进行的**。
  - 这里主要有**两个原因**。
    - 一方面，主从库间的网络可能会有**传输延迟**，所以从库不能及时地收到主库发送的命令，从库上执行同步命令的时间就会被延后。
    - 另一方面，**即使从库及时收到了主库的命令，但是，也可能会因为正在处理其它复杂度高的命令(例如集合操作命令)而阻塞**。此时，从库需要处理完当前的命令，才能执行主库发送的命令操作，这就会造成主从数据不一致。而在主库命令被滞后处理的这段时间内，主库本身可能又执行了新的写操作。这样一来，主从库间的数据不一致程度就会进一步加剧。
  - 提供**两种方法**
    - 首先，**在硬件环境配置方面，我们要尽量保证主从库间的网络连接状况良好**。例如，我们要避免把主从库部署在不同的机房，或者是避免把网络通信密集的应用(例如数据分析应 用)和 Redis 主从库部署在一起。
    - 另外，**我们还可以开发一个外部程序来监控主从库间的复制进度**。
      - Redis 的 INFO replication 命令可以查看主库接收写命令的进度信息 (master_repl_offset)和从库复制写命令的进度信息(slave_repl_offset)。
      - 我们 就可以开发一个监控程序，先用 INFO replication 命令查到主、从库的进度，然后，我们 用 master_repl_offset 减去 slave_repl_offset，这样就能得到从库和主库间的复制进度差 值了。
      - 如果某个从库的进度差值大于我们预设的阈值，我们可以让客户端不再和这个从库连接进行数据读取，这样就可以减少读到不一致数据的情况。不过，为了避免出现客户端和所有从库都不能连接的情况，我们需要把复制进度差值的阈值设置得大一些。
      - 当然，监控程序可以一直监控着从库的复制进度，当从库的复制进度又赶上主库时，我们就允许客户端再次跟这些从库连接。
- **读取过期数据**
  - 其实，这是由 Redis 的过期数据删除策略引起的
  - **Redis 同时使用了两种策略来删除过期的数据，分别是惰性删除策略和定期删除策略**。
    - 惰性删除策略。当一个数据的过期时间到了以后，并不会立即删除数据，而是等到再有请求来读写这个数据时，对数据进行检查，如果发现数据已经过期了，再删除这个数据。
      - 这个策略会导致大量已经过期的数据留存在内存中，占用 较多的内存资源。所以，Redis 在使用这个策略的同时，还使用了第二种策略:定期删除 策略。
    - 定期删除策略是指，Redis 每隔一段时间(默认 100ms)，就会随机选出一定数量的数 据，检查它们是否过期，并把其中过期的数据删除，这样就可以及时释放一些内存。
      - **首先，虽然定期删除策略可以释放一些内存，但是，Redis 为了避免过多删除操作对性能 产生影响，每次随机检查数据的数量并不多**。如果过期数据很多，并且一直没有再被访问 的话，这些数据就会留存在 Redis 实例中。业务应用之所以会读到过期数据，这些留存数 据就是一个重要因素。
      - 其次，惰性删除策略实现后，数据只有被再次访问时，才会被实际删除。如果客户端从主库上读取留存的过期数据，主库会触发删除操作，此时，客户端并不会读到过期数据。但是，从库本身不会执行删除操作，如果客户端在从库中访问留存的过期数据，从库并不会触发数据删除。
      - 在 3.2 版本后，Redis 做了改进，如果读取的数据已经过期了，从库虽然不会删除，但是会返回空值，这就避免了客户端读到过期数据。所以，**在应用主从集群时，尽量使用 Redis 3.2 及以上版本**。
  - 只要使用了 Redis 3.2 后的版本，就不会读到过期数据了吗?其实还是会 的。
    - 设置数据过期时间的命令一共有 4 个，我们可以把它们分成两 类:
      - EXPIRE 和 PEXPIRE:它们给数据设置的是**从命令执行时开始计算的存活时间**; 
      - EXPIREAT 和 PEXPIREAT:**它们会直接把数据的过期时间设置为具体的一个时间点**。
    - 假设当前时间是 2020 年 10 月 24 日上午 9 点，主从库正在同步，主库收到了一条命令: EXPIRE testkey 60，这就表示，testkey 的过期时间就是 24 日上午 9 点 1 分，主库直接 执行了这条命令。
      - 但是，主从库全量同步花费了 2 分钟才完成。等从库开始执行这条命令时，时间已经是 9 点 2 分了。而 EXPIRE 命令是把 testkey 的过期时间设置为当前时间的 60s 后，也就是 9 点 3 分。如果客户端在 9 点 2 分 30 秒时在从库上读取 testkey，仍然可以读到 testkey 的值。但是，testkey 实际上已经过期了。
    - 为了避免这种情况，我给你的建议是，**在业务应用中使用 EXPIREAT/PEXPIREAT 命令， 把数据的过期时间设置为具体的时间点，避免读到过期数据。**
    - 你也可以使用 EXPIREAT/PEXPIREAT 命令设置过期时间，避免从库上的数据过 期时间滞后。不过，这里有个地方需要注意下，**因为 EXPIREAT/PEXPIREAT 设置的是 时间点，所以，主从节点上的时钟要保持一致，具体的做法是，让主从节点和相同的 NTP 服务器(时间服务器)进行时钟同步**。
- **不合理配置项导致的服务挂掉**
  - **protected-mode 配置项**
    - 这个配置项的作用是限定哨兵实例能否被其他服务器访问。当这个配置项设置为 yes 时， 哨兵实例只能在部署的服务器本地进行访问。当设置为 no 时，其他服务器也可以访问这 个哨兵实例。
    - **我们在应用主从集群时，要注意将 protected-mode 配置项设置为 no，并且将 bind 配置项设置为其它哨兵实例的 IP 地址。**这样一来，只有在 bind 中设置了 IP 地址的 哨兵，才可以访问当前实例，既保证了实例间能够通信进行主从切换，也保证了哨兵的安 全性。
  - **cluster-node-timeout 配置项**
    - **这个配置项设置了 Redis Cluster 中实例响应心跳消息的超时时间**。
    - 当我们在 Redis Cluster 集群中为每个实例配置了“一主一从”模式时，如果主实例发生故 障，从实例会切换为主实例，受网络延迟和切换操作执行的影响，切换时间可能较长，就 会导致实例的心跳超时(超出 cluster-node-timeout)。实例超时后，就会被 Redis Cluster 判断为异常。而 Redis Cluster 正常运行的条件就是，有半数以上的实例都能正常 运行。
    - 所以，如果执行主从切换的实例超过半数，而主从切换时间又过长的话，就可能有半数以 上的实例心跳超时，从而可能导致整个集群挂掉。所以，**我建议你将 cluster-node- timeout 调大些(例如 10 到 20 秒)**。





# 脑裂

- 所谓的脑裂，就是指在主从集群中，同时有两个主节点，它们都能接收写请求。而脑裂最直接的影响，就是客户端不知道应该往哪个主节点写入数据，结果就是不同的客户端会往不同的主节点上写入数据。而且，严重的话，脑裂会进一步导致数据丢失。
- **为什么会发生脑裂?**
  - 第一步:确认是不是数据同步出现了问题
    - 在主从集群中发生数据丢失，最常见的原因就是**主库的数据还没有同步到从库，结果主库 发生了故障，等从库升级为主库后，未同步的数据就丢失了。**
    - **如果是这种情况的数据丢失，我们可以通过比对主从库上的复制进度差值来进行判断**，也 就是计算 master_repl_offset 和 slave_repl_offset 的差值。如果从库上的 slave_repl_offset 小于原主库的 master_repl_offset，那么，我们就可以认定数据丢失是 由数据同步未完成导致的。
  - 第二步:排查客户端的操作日志，发现脑裂现象
    - 在排查客户端的操作日志时，我们发现，在主从切换后的一段时间内，有一个客户端仍然在和原主库通信，并没有和升级的新主库进行交互。这就相当于主从集群中同时有了两个主库。根据这个迹象，我们就想到了在分布式主从集群发生故障时会出现的一个问题:脑裂。
    - 但是，不同客户端给两个主库发送数据写操作，按道理来说，只会导致新数据会分布在不同的主库上，并不会造成数据丢失。那么，为什么我们的数据仍然丢失了呢?
  - 第三步:发现是原主库假故障导致的脑裂
    - 在切换过程中，既然客户端仍然和原主库通信，这就表明，**原主库并没有真的发生 故障**(例如主库进程挂掉)。我们猜测，主库是由于某些原因无法处理请求，也没有响应 哨兵的心跳，才被哨兵错误地判断为客观下线的。结果，在被判断下线之后，原主库又重 新开始处理请求了，而此时，哨兵还没有完成主从切换，客户端仍然可以和原主库通信， 客户端发送的写操作就会在原主库上写入数据了。
- **为什么脑裂会导致数据丢失?**
  - **主从切换后，从库一旦升级为新主库，哨兵就会让原主库执行 slave of 命令，和新主库重新进行全量同步。**而在全量同步执行的最后阶段，原主库需要清空本地的数据，加载新主 库发送的 RDB 文件，这样一来，原主库在主从切换期间保存的新写数据就丢失了。
  - 在主从切换的过程中，如果原主库只是“假故障”，它会触发哨兵启动主从切换，一旦等它从假故障中恢复后，又开始处理请求，这样一来，就会和新主库同时存在，形成脑裂。等到哨兵让原主库和新主库做全量同步后，原主库在切换期间保存的数据就丢失了。
- **如何应对脑裂问题?**
  - 既然问题是出在原主库发生假故障后仍然能接收请求上，我们就开始在主从集群机制的配置项中查找是否有限制主库接收请求的设置。
  - Redis 已经提供了两个配置项来限制主库的请求处理，分别是 min- slaves-to-write 和 min-slaves-max-lag。
    - min-slaves-to-write:这个配置项设置了主库能进行数据同步的最少从库数量;
    - min-slaves-max-lag:这个配置项设置了主从库间进行数据复制时，从库给主库发送 ACK 消息的最大延迟(以秒为单位)。
  - 我们可以把 min-slaves-to-write 和 min-slaves-max-lag 这两个配置项搭配起来使用， 分别给它们设置一定的阈值，假设为 N 和 T。**这两个配置项组合后的要求是，主库连接的 从库中至少有 N 个从库，和主库进行数据复制时的 ACK 消息延迟不能超过 T 秒，否则， 主库就不会再接收客户端的请求了。**
    - 即使原主库是假故障，它在假故障期间也无法响应哨兵心跳，也不能和从库进行同步，自 然也就无法和从库进行 ACK 确认了。这样一来，min-slaves-to-write 和 min-slaves- max-lag 的组合要求就无法得到满足，原主库就会被限制接收客户端请求，客户端也就不 能在原主库中写入新数据了。
    - 等到新主库上线时，就只有新主库能接收和处理客户端请求，此时，新写的数据会被直接写到新主库中。而原主库会被哨兵降为从库，即使它的数据被清空了，也不会有新数据丢失。
    - 假设我们将 min-slaves-to-write 设置为 1，把 min-slaves-max-lag 设置为 12s，把哨兵 的 down-after-milliseconds 设置为 10s，主库因为某些原因卡住了 15s，导致哨兵判断 主库客观下线，开始进行主从切换。同时，因为原主库卡住了 15s，没有一个从库能和原 主库在 12s 内进行数据复制，原主库也无法接收客户端请求了。这样一来，主从切换完成 后，也只有新主库能接收请求，不会发生脑裂，也就不会发生数据丢失的问题了。
  - 假设我们将 min-slaves-to-write 设置为 1，min-slaves-max-lag 设置为 15s，哨兵的 down-after-milliseconds 设置为 10s，哨兵主从切换需要 5s。主库因为某些原因卡住了 12s，此时，还会发生脑裂吗？主从切换完成后，数据会丢失吗？
  - 主库卡住 12s，达到了哨兵设定的切换阈值，所以哨兵会触发主从切换。但哨兵切换的时间是 5s，也就是说哨兵还未切换完成，主库就会从阻塞状态中恢复回来，而且也没有触发 min-slaves-max-lag 阈值，所以主库在哨兵切换剩下的 3s 内，依旧可以接收客户端的写操作，如果这些写操作还未同步到从库，哨兵就把从库提升为主库了，那么此时也会出现脑裂的情况，之后旧主库降级为从库，重新同步新主库的数据，新主库也会发生数据丢失。
  - **由此也可以看出，即使 Redis 配置了 min-slaves-to-write 和 min-slaves-max-lag，当脑裂发生时，还是无法严格保证数据不丢失，它只能是尽量减少数据的丢失。**
  - 其实在这种情况下，新主库之所以会发生数据丢失，是因为旧主库从阻塞中恢复过来后，收到的写请求还没同步到从库，从库就被哨兵提升为主库了。如果哨兵在提升从库为新主库前，主库及时把数据同步到从库了，那么从库提升为主库后，也不会发生数据丢失。但这种临界点的情况还是有发生的可能性，因为 **Redis 本身不保证主从同步的强一致。**
- **还有一种发生脑裂的情况，就是网络分区**：主库和客户端、哨兵和从库被分割成了 2 个网络，主库和客户端处在一个网络中，从库和哨兵在另一个网络中，此时哨兵也会发起主从切换，出现 2 个主库的情况，而且客户端依旧可以向旧主库写入数据。等网络恢复后，主库降级为从库，新主库丢失了这期间写操作的数据。
- Redis无法完全避免脑裂的产生，因为其不保证主从的强一致，所以必然有产生脑裂的可能性。
  - **脑裂产生问题的本质原因是，Redis 主从集群内部没有通过共识算法，来维护多个节点数据的强一致性。**它不像 Zookeeper 那样，每次写请求必须大多数节点写成功后才认为成功。当脑裂发生时，Zookeeper 主节点被孤立，此时无法写入大多数节点，写请求会直接返回失败，因此它可以保证集群数据的一致性。
  - 另外关于 min-slaves-to-write，有一点也需要注意：如果只有 1 个从库，当把 min-slaves-to-write 设置为 1 时，在运维时需要小心一些，当日常对从库做维护时，例如更换从库的实例，需要先添加新的从库，再移除旧的从库才可以，或者使用 config set 修改 min-slaves-to-write 为 0 再做操作，否则会导致主库拒绝写，影响到业务。







# **数据分布优化**

- 在切片集群中，数据会按照一定的分布规则分散到不同的实例上保存。比如，在使用 Redis Cluster 或 Codis 时，数据都会先按照 CRC 算法的计算值对 Slot(逻辑槽)取模， 同时，所有的 Slot 又会由运维管理员分配到不同的实例上。这样，数据就被保存到相应的 实例上了。
  - 虽然这种方法实现起来比较简单，但是很容易导致一个问题:数据倾斜。
  - 数据倾斜有两类
    - **数据量倾斜**:在某些情况下，实例上的数据分布不均衡，某个实例上的数据特别多。
    - **数据访问倾斜**:虽然每个集群实例上的数据量相差不大，但是某个实例上的数据是热点 数据，被访问得非常频繁。
- **数据量倾斜的成因和应对方法**
  - 当数据量倾斜发生时，这主要有三个原因，分别是某个实例上保存了 bigkey、Slot 分配不均衡以及 Hash Tag。
  - **bigkey** **导致倾斜**
    - 某个实例上正好保存了 bigkey。bigkey 的 value 值很大(String 类 型)，或者是 bigkey 保存了大量集合元素(集合类型)，会导致这个实例的数据量增加， 内存资源消耗也相应增加。
    - bigkey 的操作一般都会造成实例 IO 线程阻塞，如果 bigkey 的访问量比较大，就 会影响到这个实例上的其它请求被处理的速度。
    - 为了避免 bigkey 造成的数据 倾斜，一个根本的应对方法是，**我们在业务层生成数据时，要尽量避免把过多的数据保存 在同一个键值对中**。
    - 此外，**如果 bigkey 正好是集合类型，我们还有一个方法，就是把 bigkey 拆分成很多个 小的集合类型数据，分散保存在不同的实例上**。
  - **Slot** **分配不均衡导致倾斜**
    - Redis Cluster 一共有 16384 个 Slot，假设集群一共有 5 个实例，其中，实例 1 的硬件配 置较高，运维人员在给实例分配 Slot 时，就可能会给实例 1 多分配些 Slot，把实例 1 的 资源充分利用起来。
      - 但是，我们其实并不知道数据和 Slot 的对应关系，这种做法就可能会导致大量数据正好被 映射到实例 1 上的 Slot，造成数据倾斜，给实例 1 带来访问压力。
    - 为了应对这个问题，我们可以通过运维规范，在分配之前，我们就要避免把过多的 Slot 分 配到同一个实例。**如果是已经分配好 Slot 的集群，我们可以先查看 Slot 和实例的具体分 配关系**，从而判断是否有过多的 Slot 集中到了同一个实例。如果有的话，就将部分 Slot 迁移到其它实例，从而避免数据倾斜。
      - 不同集群上查看 Slot 分配情况的方式不同:如果是 Redis Cluster，就用 **CLUSTER SLOTS 命令**;如果是 Codis，就可以在 codis dashboard 上查看。
    - 如果某一个实例上有太多的 Slot，我们就可以使用迁移命令把这些 Slot 迁移到其它实例 上。在 Redis Cluster 中，我们可以使用 3 个命令完成 Slot 迁移。
      - CLUSTER SETSLOT:使用不同的选项进行三种设置，分别是设置 Slot 要迁入的目标实 例，Slot 要迁出的源实例，以及 Slot 所属的实例。
      - CLUSTER GETKEYSINSLOT:获取某个 Slot 中一定数量的 key。 
      - MIGRATE:把一个 key 从源实例实际迁移到目标实例。
    - 从 Redis 3.0.6 开始，你也可以使用 KEYS 选项，一次迁移多个 key(key1、2、3)，这 样可以提升迁移效率
  - **Hash Tag** **导致倾斜**
    - Hash Tag 是指加在键值对 key 中的一对花括号{}。这对括号会把 key 的一部分括起来， 客户端在计算 key 的 CRC16 值时，只对 Hash Tag 花括号中的 key 内容进行计算。如果 没用 Hash Tag 的话，客户端计算整个 key 的 CRC16 的值。
    - 使用 Hash Tag 的好处是，如果不同 key 的 Hash Tag 内容都是一样的，那么，这些 key 对应的数据会被映射到同一个 Slot 中，同时会被分配到同一个实例上。
    - **Hash Tag 一般用在什么场景呢?其实，它主要是用在 Redis Cluster 和 Codis 中，支持事务操作和范围查询。**因为 Redis Cluster 和 Codis 本身并不支持跨实例的事务 操作和范围查询，当业务应用有这些需求时，就只能先把这些数据读取到业务层进行事务 处理，或者是逐个查询每个实例，得到范围查询的结果。**这样操作起来非常麻烦，所以，我们可以使用 Hash Tag 把要执行事务操作或是范围查询 的数据映射到同一个实例上，这样就能很轻松地实现事务或范围查询了。**
    - 但是，使用 Hash Tag 的潜在问题，就是大量的数据可能被集中到一个实例上，导致数据 倾斜，集群中的负载不均衡。那么，该怎么应对这种问题呢?我们就需要在范围查询、事 务执行的需求和数据倾斜带来的访问压力之间，进行取舍了。
      - 如果使用 Hash Tag 进行切片的数据会带来较大的访问压力，就优先考虑避 免数据倾斜，最好不要使用 Hash Tag 进行数据切片。因为事务和范围查询都还可以放在 客户端来执行，而数据倾斜会导致实例不稳定，造成服务不可用。
- **数据访问倾斜的成因和应对方法**
  - 发生数据访问倾斜的根本原因，就是实例上存在热点数据(比如新闻应用中的热点新闻内容、电商促销活动中的热门商品信息，等等)。
  - 和数据量倾斜不同，热点数据通常是一个或几个数据，所以，直接重新分配 Slot 并不能解 决热点数据的问题。
  - 通常来说，热点数据以服务读操作为主，在这种情况下，我们可以采用**热点数据多副本**的 方法来应对。
    - **这个方法的具体做法是，我们把热点数据复制多份，在每一个数据副本的 key 中增加一个 随机前缀，让它和其它副本数据不会被映射到同一个 Slot 中。**这样一来，热点数据既有多 个副本可以同时服务请求，同时，这些副本数据的 key 又不一样，会被映射到不同的 Slot 中。在给这些 Slot 分配实例时，我们也要注意把它们分配到不同的实例上，那么，热点数 据的访问压力就被分散到不同的实例上了。
    - 这里，有个地方需要注意下，**热点数据多副本方法只能针对只读的热点数据**。如果热点数 据是有读有写的话，就不适合采用多副本方法了，因为要保证多副本间的数据一致性，会 带来额外的开销。
  - **对于有读有写的热点数据，我们就要给实例本身增加资源了**，例如使用配置更高的机器，来应对大量的访问压力。







# 进程内缓存

- 进程内缓存的实现载体，最简单的，可以是一个**带锁的Map**。又或者，可以使用**第三方库**，例如leveldb。

  - **与没有缓存相比**，进程内缓存的好处是，数据读取不再需要访问后端，例如数据库。
  - 与进程外缓存相比（例如redis/memcache），**进程内缓存省去了网络开销**，所以一来节省了内网带宽，二来响应时延会更低。

- **进程内缓存有什么缺点？**

  - 如果数据缓存在站点和服务的多个节点内，数据存了多份，一致性比较难保障。

- **如何保证进程内缓存的数据一致性？**

  - **第一种方案**，**可以通过单节点通知其他节点**。写请求发生在server1，在修改完自己内存数据与数据库中的数据之后，可以主动通知其他server节点，也修改内存的数据。
    - 这种方案的**缺点**是：同一功能的一个集群的多个节点，相互耦合在一起，特别是节点较多时，网状连接关系极其复杂。
  - **第二种方案**，**可以通过MQ通知其他节点**。写请求发生在server1，在修改完自己内存数据与数据库中的数据之后，给MQ发布数据变化通知，其他server节点订阅MQ消息，也修改内存数据。
    - 这种方案虽然解除了节点之间的耦合，但引入了MQ，使得系统更加复杂。
  - 前两种方案，节点数量越多，数据冗余份数越多，数据同时更新的原子性越难保证，一致性也就越难保证。
  - **第三种方案**，为了避免耦合，降低复杂性，干脆放弃了“实时一致性”，**每个节点启动一个timer，定时从后端拉取最新的数据，更新内存缓存**。在有节点更新后端数据，而其他节点通过timer更新数据之间，会读到脏数据。

- **为什么不能频繁使用进程内缓存？**

  - **分层架构设计，有一条准则**：站点层、服务层要做到无数据无状态，这样才能任意的加节点水平扩展，数据和状态尽量存储到后端的数据存储服务，例如数据库服务或者缓存服务。

- **什么时候可以使用进程内缓存？**

  - **情况一**，只读数据，可以考虑在进程启动时加载到内存。
    - *画外音：此时也可以把数据加载到redis / memcache，进程外缓存服务也能解决这类问题。*
  - **情况二**，极其高并发的，如果透传后端压力极大的场景，可以考虑使用进程内缓存。
    - 例如，秒杀业务，并发量极高，需要站点层挡住流量，可以使用内存缓存。
  - **情况三**，一定程度上允许数据不一致业务。
    - 例如，有一些计数场景，运营场景，页面对数据一致性要求较低，可以考虑使用进程内页面缓存。

- 再次强调，进程内缓存的适用场景并不如redis/memcache广泛，不要为了炫技而使用。

  































