

## 容器网络（Veth Pair）

- Linux 容器能看见的“网络栈”，实际上是被隔离在它自己的 Network Namespace 当中的。
  - 所谓“网络栈”，就包括了：网卡（Network Interface）、回环设备（LoopbacDevice）、路由表（Routing Table）和 iptables 规则。对于一个进程来说，这些要素，其实就构成了它发起和响应网络请求的基本环境。
  - 器要想跟外界进行通信，它发出的 IP 包就必须从它的 NetworNamespace 里出来，来到宿主机上。
    - 解决这个问题的方法就是：为容器创建一个一端在容器里充当默认网卡、另一端在宿主机上的Veth Pair 设备。
- 这个被隔离的容器进程，该如何跟其他 Network Namespace 里的容器进程进行交互呢？
  - 如果你想要实现多台主机之间的通信，那就需要用网线，把它们连接在一台交换机上。在 Linux 中，能够起到虚拟交换机作用的网络设备，是网桥（Bridge）
  - 为了实现上述目的，Docker 项目会默认在宿主机上创建一个名叫 docker0 的网桥，凡是连接在 docker0 网桥上的容器，就可以通过它来进行通信。
- Veth Pair的虚拟设备
  - 如何把这些容器“连接”到 docker0 网桥上
  - 被创建出来后，总是以两张虚拟网卡（Veth Peer）的形式成对出现的。
  - 从其中一个“网卡”发出的数据包，可以直接出现在与它对应的另一张“网卡”上，哪怕这两个“网卡”在不同的 Network Namespace 里。
  - 因为这个容器里有一张叫作 eth0 的网卡，它正是一个 Veth Pair 设备在容器里的这一端，这个 Veth Pair 设备的另一端，则在宿主机上。所以同一宿主机上的两个容器默认就是相互连通
  - 一旦一张虚拟网卡被“插”在网桥上，它就会变成该网桥的“从设备”。从设备会被“剥夺”调用网络协议栈处理数据包的资格，从而“降级”成为网桥上的一个端口
- 如果我们通过软件的方式，创建一个整个集群“公用”的网桥，然后把集群里的所有容器都连接到这个网桥上，不就可以相互通信了吗？
  - 构建这种容器网络的核心在于：我们需要在已有的宿主机网络上，再通过软件构建一个覆盖在已有宿主机网络之上的、可以把所有容器连通在一起的虚拟网络。所以，这种技术就被称为：Overlay Network（覆盖网络）。
  - 这个 Overlay Network 本身，可以由每台宿主机上的一个“特殊网桥”共同组成







## 容器跨主机网络（Flannel）

- 要理解容器“跨主通信”的原理，就一定要先从 Flannel 这个项目说起。
  - 事实上，Flannel 项目本身只是一个框架，真正为我们提供容器网络功能的，是 Flannel 的后端实现。
  - 我们在进行系统级编程的时候，有一个非常重要的优化原则，就是要减少用户态到内核态的切换次数，并且把核心的处理逻辑都放在内核态进行。这也是为什么，Flannel 后来支持的VXLAN 模式，逐渐成为了主流的容器网络方案的原因。
  - VXLAN，即 Virtual Extensible LAN（虚拟可扩展局域网），是 Linux 内核本身就支持的一种网络虚似化技术。所以说，VXLAN 可以完全在内核态实现上述封装和解封装的工作，从而通过与前面相似的“隧道”机制，构建出覆盖网络（Overlay Network）。
- 设计思想
  - 为了能够在二层网络上打通“隧道”，VXLAN 会在宿主机上设置一个特殊的网络设备作为“隧道”的两端。这个设备就叫作 VTEP(虚拟隧道端点)
    - 这些 VTEP 设备之间，就需要想办法组成一个虚拟的二层网络，即：通过二层数据帧进行通信。
    - “源 VTEP 设备”收到“原始 IP 包”后，就要想办法把“原始 IP 包”加上一个目的 MAC 地址，封装成一个二层数据帧，然后发送给“目的 VTEP 设备”
      - Linux 内核还需要再把“内部数据帧”进一步封装成为宿主机网络里的一个普通的数据帧，好让它“载着”“内部数据帧”，通过宿主机的 eth0 网卡进行传输。
      - 然后，Linux 内核会把这个数据帧封装进一个 UDP 包里发出去。
    - 接下来的流程，就是一个正常的、宿主机网络上的封包工作。
  - VXLAN 模式组建的覆盖网络，其实就是一个由不同宿主机上的 VTEP 设备，也就是 flannel.1 设备组成的虚拟二层网络。对于 VTEP 设备来说，它发出的“内部数据帧”就仿佛是一直在这个虚拟的二层网络上流动。这，也正是覆盖网络的含义。







## Kubernetes网络模型与CNI网络插件

- 以 Flannel 项目为例，容器跨主机网络的两种实现方法：UDP 和 VXLAN

  - 这些例子有一个共性，那就是用户的容器都连接在 docker0 网桥上
    - 网络插件则在宿主机上创建了一个特殊的设备
    - 然后，网络插件真正要做的事情，则是通过某种方法，把不同宿主机上的特殊设备连通，从而达到容器跨主机通信的目的。
    - docker0 与这个设备之间，通过 IP 转发（路由表）进行协作。
  - Kubernetes 是通过一个叫作 CNI 的接口，维护了一个单独的网桥来代替 docker0。这个网桥的名字就叫作：CNI 网桥，它在宿主机上的设备名称默认是：cni0。

- CNI 网桥

  - CNI 网桥只是接管所有 CNI 插件负责的、即 Kubernetes 创建的容器（Pod）。
  - Kubernetes 之所以要设置这样一个与 docker0 网桥功能几乎一样的 CNI 网桥，主要原因包括两个方面：
    - 一方面，Kubernetes 项目并没有使用 Docker 的网络模型（CNM）
    - 另一方面，这还与 Kubernetes 如何配置 Pod，也就是 Infra 容器的 Network Namespace密切相关。
  - CNI 的设计思想，就是：Kubernetes 在启动 Infra 容器之后，就可以直接调用 CNI 网络插件，为这个 Infra 容器的 Network Namespace，配置符合预期的网络栈。

- CNI 插件的工作原理

  - ```
    CNI配置文件内容如下：
    
    ```

cat /etc/cni/net.d/10-flannel.conflist 
    {
  "name": "cbr0",
      "plugins": [
    {
          "type": "flannel",
      "delegate": {
            "hairpinMode": true,
        "isDefaultGateway": true
          }
    },
        {
        "type": "portmap",
          "capabilities": {
            "portMappings": true
        }
        }
  ]
    }



  - 需要注意的是Kubernetes 目前不支持多个CNI混用，如果在/etc/cni/net.d/里放置多个CNI配置文件的话，只会加载字母排序第一个配置文件。但另一方面，CNI允许你在一个CNI配置文件里，通过plugins字段，定义多个插件协作。假如我们定义了flannel和protmap这两个插件，这个时候dockershim会把这个CNI配置文件加载起来，并把列表第一个插件，就是flannel插件设置为默认插件，然后执行过程中flannel和portmap会按照定义顺序被调用，从而一次完成“配置容器网络”和“配置端口映射”。

  - 当kubelet组件需要创建pod时，他一个创建的一定是infra容器，所以这一步，dockershim 就会先调用Docker API创建并启动infra容器，紧着执行一个叫做SetUpPod的方法，这个方法的作用是：为CNI插件准备构建参数，然后调用CNI插件为infra容器配置网络。这里要调用CNI插件是/opt/cni/bin/flannel;而调用它所需的参数，分两部分：

  - 第一部分，是由dockershim设置的一组CNI环境变量。其中，最重要的环境变量参数叫做：CNI_COMMAND。他的取值只有两种ADD和DEL。这个ADD和DEL操作，就是CNI插件唯一实现的两种方法。

    - ADD 把容器添加CNI网络里
    - DEL 把容器从CNI网络里移除

    在网桥类型的CNI里，这两个操作说明把容器以“Veth Pair”方式插在CNI网桥上，或者从网桥上拔掉。

  - 第二部分，是dockershim从CNI配置文件里加载到的默认插件的配置信息。这个配置信息在CNI中被叫作Network Configuration,dockershim会把Network Configuration 通过JSON数据的方式，通过标准输入的方式传递给Flannel CNI插件。

  - Delegate 字段的意思是，这个 CNI 插件并不会自己做事儿，而是会调用 Delegate 指定的某种CNI 内置插件来完成。对于 Flannel 来说，它调用的 Delegate 插件，就是前面介绍到的 CNIbridge 插件。

  - 所以说，dockershim 对 Flannel CNI 插件的调用，其实就是走了个过场。Flannel CNI 插件唯一需要做的，就是对 dockershim 传来的 Network Configuration 进行补充。比如，将Delegate 的 Type 字段设置为 bridge，将 Delegate 的 IPAM 字段设置为 host-local 等。

  - 在 Kubernetes 中，处理容器网络相关的逻辑并不会在 kubelet 主干代码里执行，而是会在具体的 CRI（Container Runtime Interface，容器运行时接口）实现里完成。对于 Docker 项目来说，它的 CRI 实现叫作 dockershim

  - CNI bridge 插件

    - 功能：“将容器加入到CNI 网络里”

    - CNI bridge 插件会在宿主机上检查 CNI 网桥是否存在。如果没有的话，那就创建它

    - CNI bridge 插件会通过 Infra 容器的 Network Namespace 文件，进入到这个Network Namespace 里面，然后创建一对 Veth Pair 设备。

    - 紧接着，它会把这个 Veth Pair 的其中一端，“移动”到宿主机上。

      - ```
        这相当于在容器里执行如下所示的命令：
        # 在宿主机上
        $ ip link add cni0 type bridge
        $ ip link set cni0 up
        # 在容器里
        # 创建一对 Veth Pair 设备。其中一个叫作 eth0，另一个叫作 vethb4963f3
        $ ip link add eth0 type veth peer name vethb4963f3
        # 启动 eth0 设备
        $ ip link set eth0 up 
        # 将 Veth Pair 设备的另一端（也就是 vethb4963f3 设备）放到宿主机（也就是 Host Namespace）里
        $ ip link set vethb4963f3 netns $HOST_NS
        # 通过 Host Namespace，启动宿主机上的 vethb4963f3 设备
        $ ip netns exec $HOST_NS ip link set vethb4963f3 up
        ```

        

    - 接下来，CNI bridge 插件会调用 CNI ipam 插件，为容器分配一个可用的 IP 地址。然后，CNI bridge 插件就会把这个 IP 地址添加在容器的 eth0 网卡上，同时为容器设置默认路由

      - > 在容器里
        >
        > $ ip addr add 10.244.0.2/24 dev eth0
        > $ ip route add default via 10.244.0.1 dev eth0

    - 最后，CNI bridge 插件会为 CNI 网桥添加 IP 地址。

      - > 在宿主机上
        >
        > $ ip addr add 10.244.0.1/24 dev cni0

    - 在执行完上述操作之后，CNI 插件会把容器的 IP 地址等信息返回给 dockershim，然后被kubelet 添加到 Pod 的 Status 字段。

    - 至此，CNI 插件的 ADD 方法就宣告结束了。接下来的流程，就跟我们上一篇文章中容器跨主机通信的过程完全一致了。

- “Kubernetes 网络模型”

  1. 所有容器都可以直接使用 IP 地址与其他容器通信，而无需使用 NAT。

  2. 所有宿主机都可以直接使用 IP 地址与所有容器通信，而无需使用 NAT。反之亦然。

  3. 容器自己“看到”的自己的 IP 地址，和别人（宿主机或者容器）看到的地址是完全一样的。

  





## Service底层（IPVS、Informer）、DNS与服务发现

- Kubernetes 之所以需要 Service，一方面是因为 Pod 的 IP 不是固定的，另一方面则是因为一组 Pod 实例之间总会有负载均衡的需求。

  - > 这个 Service 的 80 端口，代理的是 Pod 的9376 端口apiVersion: v1
    >
    > kind: Service
    >
    > metadata:
    >
    > 	name: hostnames
    >
    > spec:
    >
    > 	selector:
    > 	
    > 		app: hostnames
    >
    > ​	 ports:
    >
    > ​	 \- name: default
    >
    > 	 protocol: TCP
    >
    > ​	   port: 80
    >
    > 	 targetPort: 9376

  - 我们的应用的 Deployment

    > apiVersion: apps/v1
    >
    > kind: Deployment
    >
    > metadata:
    >
    > 	name: hostnames
    >
    > spec:
    >
    > 	selector:
    > 	
    > 		matchLabels:
    > 	
    > 			app: hostnames
    > 	
    > 	replicas: 3
    > 	
    > 	template:
    > 	
    > 		metadata:
    > 	
    > 			labels:
    > 	
    > 				app: hostnames
    > 	
    > 		spec:
    > 	
    > 			containers:
    > 	
    > 			\- name: hostnames
    > 	
    > 			image: k8s.gcr.io/serve_hostname
    > 	
    > 			ports:
    > 	
    > 				\- containerPort: 9376
    > 	
    > 				protocol: TCP

- Kubernetes 里的 Service 究竟是如何工作的呢？

  - **实际上，Service 是由 kube-proxy 组件，加上 iptables 来共同实现的。**

    - > Service一旦它被提交给Kubernetes， kube-proxy 就可以通过 Service 的 Informer 感知到这样一个 Service 对象的添加，而作为对这个事件的响应，它就会在宿主机上创建这样一条 iptables 规则
      >
      > 
      >
      > 凡是目的地址是 10.0.1.175、目的端口是 80 的 IP包，都应该跳转到另外一条名叫 KUBE-SVC-NWV5X2332I4OT4T3

    - 实际上，它是一组规则的集合，这一组规则（ DNAT 规则），实际上是一组随机模式（–mode random）的 iptables 链。

      - 这条链指向的最终目的地，其实就是这个 Service 代理的Pod
      - DNAT 规则的作用，就是在路由之前，将流入 IP 包的目的地址和端口，改成–to-destination 所指定的新的目的地址和端口。可以看到，这个目的地址和端口，正是被代理 Pod 的 IP 地址和端口。

  - 所以这一组规则，就是 Service 实现负载均衡的位置。

- **IPVS**

  - kube-proxy 通过 iptables 处理 Service 的过程，其实需要在宿主机上设置相当多的 iptables 规则。而且，kube-proxy 还需要在控制循环里不断地刷新这些规则来确保它们始终是正确的
    - 大量 Pod不断地被刷新，会大量占用该宿主机的 CPU 资源，基于iptables 的 Service 实现，都是制约 Kubernetes 项目承载更多量级的 Pod 的主要障碍。
    - IPVS 模式的 Service，就是解决这个问题的一个行之有效的方法。
  - IPVS 模式的工作原理，其实跟 iptables 模式类似。当我们创建了前面的 Service 之后，kube-proxy 首先会在宿主机上创建一个虚拟网卡（叫作：kube-ipvs0），并为它分配 Service VIP作为 IP 地址
    - kube-proxy 就会通过 Linux 的 IPVS 模块，为这个 IP 地址设置三个 IPVS 虚拟主机，并设置这三个虚拟主机之间使用轮询模式 (rr) 来作为负载均衡策略
    - 这三个 IPVS 虚拟主机的 IP 地址和端口，对应的正是三个被代理的 Pod。
    - 这时候，任何发往 10.102.128.4:80 的请求，就都会被 IPVS 模块转发到某一个后端 Pod 上了
    - 而相比于 iptables，IPVS 在内核中的实现其实也是基于 Netfilter 的 NAT 模式，所以在转发这一层上，理论上 IPVS 并没有显著的性能提升。但是，IPVS 并不需要在宿主机上为每个 Pod 设置 iptables 规则，而是把对这些“规则”的处理放到了内核态，从而极大地降低了维护这些规则的代价。这也正印证了我在前面提到过的，“将重要操作放入内核态”是提高性能的重要手段。
  - 不过需要注意的是，IPVS 模块只负责上述的负载均衡和代理功能。而一个完整的 Service 流程正常工作所需要的包过滤、SNAT 等操作，还是要靠 iptables 来实现。只不过，这些辅助性的iptables 规则数量有限，也不会随着 Pod 数量的增加而增加。
    - 所以，在大规模集群里，我非常建议你为 kube-proxy 设置–proxy-mode=ipvs 来开启这个功能。它为 Kubernetes 集群规模带来的提升，还是非常巨大的。

- Service 与 DNS 的关系

  - 在 Kubernetes 中，Service 和 Pod 都会被分配对应的 DNS A 记录（从域名解析 IP 的记录）

  - ClusterIP 模式的 Service 为你提供的，就是一个 Pod 的稳定的 IP 地址，即 VIP。并且，这里 Pod 和 Service 的关系是可以通过 Label 确定的。

  - 而 Headless Service 为你提供的，则是一个 Pod 的稳定的 DNS 名字，并且，这个名字是可以通过 Pod 名字和 Service 名字拼接出来的。

  - 但如果你为 Pod 指定了 Headless Service，并且 Pod 本身声明了 hostname 和 subdomain字段，那么这时候 Pod 的 A 记录就会变成：<pod 的 hostname>...svc.cluster.local

    - ```
      apiVersion: v1
      kind: Service
      metadata:
       	name: default-subdomain
      spec:
       	selector:
       		name: busybox
       clusterIP: None
       ports:
       - name: foo
       	 port: 1234
       	 targetPort: 1234
      ---
      apiVersion: v1
      kind: Pod
      metadata:
       name: busybox1
       labels:
       		name: busybox
      spec:
       hostname: busybox-1
       subdomain: default-subdomain
       containers:
       - image: busybox
       	 command:
       			- sleep
            - "3600"
       	 name: busybox
      ```

  - 在上面这个 Service 和 Pod 被创建之后，你就可以通过 busybox-1.default.subdomain.default.svc.cluster.local 解析到这个 Pod 的 IP 地址了。

- Service 机制，以及Kubernetes 里的 DNS 插件，都是在帮助你解决同样一个问题，即：如何找到我的某一个容器？

  - 这个问题在平台级项目中，往往就被称作服务发现，即：当我的一个服务（Pod）的 IP 地址是不固定的且没办法提前获知时，我该如何通过一个固定的方式访问到这个 Pod 呢？







## Service

- 所谓 Service 的访问入口，其实就是每台宿主机上由 kube-proxy 生成的iptables 规则，以及 kube-dns 生成的 DNS 记录。而一旦离开了这个集群，这些信息对用户来说，也就自然没有作用了。

  - 在使用 Kubernetes 的 Service 时，一个必须要面对和解决的问题就是：如何从外部（Kubernetes 集群之外），访问到 Kubernetes 里创建的 Service？

- **最常用的一种方式就是：NodePort**

  - ```yaml
    spec:
     type: NodePort
     ports:
     - nodePort: 8080
       targetPort: 80
       protocol: TCP
       name: http
     - nodePort: 443
       protocol: TCP
       name: https
    ```

    

  - 需要注意的是，在 NodePort 方式下，Kubernetes 会在 IP 包离开宿主机发往目的 Pod 时，对这个 IP 包做一次 SNAT 操作

    - 将这个 IP 包的源地址替换成了这台宿主机上的 CNI 网桥地址，或者宿主机本身的 IP 地址（如果 CNI 网桥不存在的话）。

- 从外部访问 Service 的第二种方式，适用于公有云上的 Kubernetes 服务。这时候，你可以指定一个 **LoadBalancer 类型的 Service**

  - ```yaml
    spec:
     ports:
     - port: 8765
       targetPort: 9376
     selector:
      app: example
     type: LoadBalancer
    ```

- 第三种方式，是 Kubernetes 在 1.7 之后支持的一个新特性，叫作 **ExternalName**

  - ```yaml
    # 当你通过 Service 的 DNS 名字访问它的时候，比如访问：my-service.default.svc.cluster.local。那么，Kubernetes 为你返回的就是my.database.example.com。
    metadata:
     name: my-service
    spec:
     type: ExternalName
     externalName: my.database.example.com
     
     
     # Kubernetes 的 Service 还允许你为 Service 分配公有 IP 地址
     spec:
      selector:
       app: MyApp
      ports:
      - name: http
        protocol: TCP
        port: 80
        targetPort: 9376
        externalIPs:
         - 80.11.12.10
    ```

- 很多与 Service 相关的问题，其实都可以通过分析 Service 在宿主机上对应的 iptables 规则（或者 IPVS 配置）得到解决。

  - 比如，当你的 Service 没办法通过 DNS 访问到的时候。你就需要区分到底是 Service 本身的配置问题，还是集群的 DNS 出了问题。一个行之有效的方法，就是检查 Kubernetes 自己的Master 节点的 Service DNS 是否正常：

    - > 在一个 Pod 里执行
      >
      > $ nslookup kubernetes.default
      > 如果上面访问 kubernetes.default 返回的值都有问题，那你就需要检查 kube-dns 的运行状态和日志了。否则的话，你应该去检查自己的 Service 定义是不是有问题

  - 而如果你的 Service 没办法通过 ClusterIP 访问到的时候，你首先应该检查的是这个 Service 是否有 Endpoints

    - > $ kubectl get endpoints hostnames
      > NAME ENDPOINTS
      > hostnames 10.244.0.5:9376,10.244.0.6:9376,10.244.0.7:9376
      > 如果 Endpoints 正常，那么你就需要确认 kube-proxy 是否在正确运行
      > 如果 kube-proxy 一切正常，你就应该仔细查看宿主机上的 iptables 了。

  - 还有一种典型问题，就是 Pod 没办法通过 Service 访问到自己

    - 你只需要确保将 kubelet 的 hairpin-mode 设置为 hairpin-veth 或者promiscuous-bridge 即可。

- 所谓 Service，其实就是 Kubernetes 为 Pod 分配的固定的、基于iptables（或者 IPVS）的访问入口。而这些访问入口代理的 Pod 信息，则来自于 Etcd，由kube-proxy 通过控制循环来维护。

  - Kubernetes 里面的 Service 和 DNS 机制，也都不具备强多租户能力。比如，在多租户情况下，每个租户应该拥有一套独立的 Service 规则
  - 再比如 DNS，在多租户情况下，每个租户应该拥有自己的 kube-dns
  - 多租户是指软件架构支持一个实例服务多个用户（Customer），每一个用户被称之为租户（tenant），软件给予租户可以对系统进行部分定制的能力







## Service与Ingress

- 作为用户，我其实更希望看到 Kubernetes 为我内置一个全局的负载均衡器。然后，通过我访问的 URL，把请求转发给不同的后端 Service。

  - 这种全局的、为了代理不同后端 Service 而设置的负载均衡服务，就是 Kubernetes 里的Ingress 服务。
  - 所谓 Ingress，就是 Service 的“Service”

- 我如何能使用 Kubernetes 的 Ingress 来创建一个统一的负载均衡器，从而实现当用户访问不同的域名时，能够访问到不同的 Deployment 呢？

  - 上述功能，在 Kubernetes 里就需要通过 Ingress 对象来描述

    ```yaml
    apiVersion: extensions/v1beta1
    kind: Ingress
    metadata:
     name: cafe-ingress
    spec:
     tls:
     	- hosts:
     		- cafe.example.com
     		secretName: cafe-secret
     	rules:
     	- host: cafe.example.com
     		http:
     			paths:
     			- path: /tea
     				backend:
     					serviceName: tea-svc
     					servicePort: 80
         - path: /coffee
     				backend:
     						serviceName: coffee-svc
    						 servicePort: 80
    ```

    

  - IngressRule 的 Key，就叫做：host。是这个 Ingress 的入口

  - IngressRule 规则的定义，则依赖于 path 字段，你可以简单地理解为，这里的每一个path 都对应一个后端 Service。

- 所谓 Ingress 对象，其实就是 Kubernetes 项目对“反向代理”的一种抽象。

  - 实际的使用中，你只需要从社区里选择一个具体的 Ingress Controller，把它部署在Kubernetes 集群里即可。
  - 然后，这个 Ingress Controller 会根据你定义的 Ingress 对象，提供对应的代理能力。

- Ingress Controller

  - Nginx 官方为你维护的 Ingress Controller 的定义的YAML 文件中定义了一个使用 nginx-ingress-controller 镜像的Pod。

    - 当一个新的 Ingress 对象由用户创建后，nginx-ingress-controller 就会根据 Ingress 对象里定义的内容，生成一份对应的 Nginx 配置文件（/etc/nginx/nginx.conf），并使用这个配置文件启动一个 Nginx 服务。
    - nginx-ingress-controller 还允许你通过 Kubernetes 的 ConfigMap 对象来对上述Nginx 配置文件进行定制
    - 一个 Nginx Ingress Controller 为你提供的服务，其实是一个可以根据 Ingress对象和被代理后端 Service 的变化，来自动进行更新的 Nginx 负载均衡器。

  - 为了让用户能够用到这个 Nginx，我们就需要创建一个 Service 来把 Nginx Ingress Controller 管理的 Nginx 服务暴露出去

    - 由于我们使用的是 Bare-metal 环境，所以 service-nodeport.yaml 文件里的内容，就是一个NodePort 类型的 Service

    - ```
      apiVersion: v1
      kind: Service
      metadata:
       	name: ingress-nginx
       	namespace: ingress-nginx
       	labels:
        	app.kubernetes.io/name: ingress-nginx
       		app.kubernetes.io/part-of: ingress-nginx
      spec:
      	 type: NodePort
      	 ports:
      	 	- name: http
       		port: 80
       		targetPort: 80
       		protocol: TCP
       		- name: https
       		port: 443
      	  targetPort: 443
         	protocol: TCP
       	selector:
       		app.kubernetes.io/name: ingress-nginx
       		app.kubernetes.io/part-of: ingress-nginx
      ```

    - 可以看到，这个 Service 的唯一工作，就是将所有携带 ingress-nginx 标签的 Pod 的 80 和433 端口暴露出去。

    - **上述操作完成后，你一定要记录下这个 Service 的访问入口，即：宿主机的地址和 NodePort的端口**

      - $ kubectl get svc -n ingress-nginx

- 步骤

  - 首先，我们要在集群里部署我们的应用 Pod 和它们对应的 Service
    - $ kubectl create -f cafe.yaml
  - 然后，我们需要创建 Ingress 所需的 SSL 证书（tls.crt）和密钥（tls.key），这些信息都是通过Secret 对象定义好的
    - $ kubectl create -f cafe-secret.yaml
  - 这一步完成后，我们就可以创建在本篇文章一开始定义的 Ingress 对象了
    - $ kubectl create -f cafe-ingress.yaml
  - 这时候，我们就可以查看一下这个 Ingress 对象的信息
    - $ kubectl get ingress
    - $ kubectl describe ingress cafe-ingress
    - 可以看到，这个 Ingress 对象最核心的部分，正是 Rules 字段。其中，我们定义的 Host 是cafe.example.com，它有两条转发规则（Path），分别转发给 tea-svc 和 coffee-svc。
  - 接下来，我们就可以通过访问这个 Ingress 的地址和端口，访问到我们前面部署的应用了

- 我们就可以通过访问这个 Ingress 的地址和端口，访问到我们前面部署的应用了

  - 目前，Ingress 只能工作在七层，而 Service 只能工作在四层。所以当你想要在 Kubernetes 里为应用进行 TLS 配置等 HTTP 相关的操作时，都必须通过 Ingress 来进行。
  - Ingress 简单的理解就是你原来需要改 Nginx 配置，然后配置各种域名对应哪个 Service，现在把这个动作抽象出来，变成一个 Ingress 对象，你可以用 yaml 创建，每次不要去改 Nginx 了，直接改 yaml 然后创建/更新就行了；
  - 那么问题来了：”Nginx 该怎么处理？”Ingress Controller 这东西就是解决 “Nginx 的处理方式” 的；Ingress Controoler 通过与 Kubernetes API 交互，动态的去感知集群中 Ingress 规则变化，然后读取他，按照他自己模板生成一段 Nginx 配置，再写到 Nginx Pod 里，最后 reload 一下



## 





##  Kubernetes的资源模型与资源管理

- 在 Kubernetes 里，Pod 是最小的原子调度单位。这也就意味着，所有跟调度和资源管理相关的属性都应该是属于 Pod 对象的字段。而这其中最重要的部分，就是 Pod 的 CPU 和内存配置

```
piVersion: v1
kind: Pod
metadata:
  name: frontend
spec:
  containers:
  - name: app
    image: images.my-company.example/app:v4
    env:
    - name: MYSQL_ROOT_PASSWORD
      value: "password"
    resources:
      requests:
        memory: "64Mi"
        cpu: "250m"
      limits:
        memory: "128Mi"
        cpu: "500m"
  - name: log-aggregator
    image: images.my-company.example/log-aggregator:v6
    resources:
      requests:
        memory: "64Mi"
        cpu: "250m"
      limits:
        memory: "128Mi"
        cpu: "500m"
```

- 在 Kubernetes 中，像 CPU 这样的资源被称作“可压缩资源”（compressible resources）。它的典型特点是，当可压缩资源不足时，Pod 只会“饥饿”，但不会退出。
- 而像内存这样的资源，则被称作“不可压缩资源（incompressible resources）。当不可压缩资源不足时，Pod 就会因为 OOM（Out-Of-Memory）被内核杀掉。
- 而由于 Pod 可以由多个 Container 组成，所以 CPU 和内存资源的限额，是要配置在每个Container 的定义上的。这样，Pod 整体的资源配置，就由这些 Container 的配置值累加得到。
- 其中，Kubernetes 里为 CPU 设置的单位是“CPU 的个数”。比如，cpu=1 指的就是，这个Pod 的 CPU 限额是 1 个 CPU。当然，具体“1 个 CPU”在宿主机上如何解释，是 1 个 CPU核心，还是 1 个 vCPU，还是 1 个 CPU 的超线程（Hyperthread），完全取决于宿主机的CPU 实现方式。Kubernetes 只负责保证 Pod 能够使用到“1 个 CPU”的计算能力。
- 此外，Kubernetes 允许你将 CPU 限额设置为分数，比如在我们的例子里，CPU limits 的值就是 500m。所谓 500m，指的就是 500 millicpu，也就是 0.5 个 CPU 的意思。这样，这个 Pod就会被分配到 1 个 CPU 一半的计算能力。

- **你也可以直接把这个配置写成 cpu=0.5。但在实际使用时，我还是推荐你使用 500m 的写法，毕竟这才是 Kubernetes 内部通用的 CPU 表示方式。**

- 而对于内存资源来说，它的单位自然就是 bytes。Kubernetes 支持你使用 Ei、Pi、Ti、Gi、Mi、Ki（或者 E、P、T、G、M、K）的方式来作为 bytes 的值。比如，在我们的例子里，Memory requests 的值就是 64MiB (2 的 26 次方 bytes) 。这里要注意区分MiB（mebibyte）和 MB（megabyte）的区别。

  - 1Mi=1024*1024；1M=1000*1000

- 此外，不难看到，**Kubernetes 里 Pod 的 CPU 和内存资源，实际上还要分为 limits 和requests 两种情况**，

  - ```
    spec.containers[].resources.limits.cpu
    spec.containers[].resources.limits.memory
    spec.containers[].resources.requests.cpu
    spec.containers[].resources.requests.memory
    ```

  - 这两者的区别其实非常简单：在调度的时候，kube-scheduler 只会按照 requests 的值进行计算。而在真正设置 Cgroups 限制的时候，kubelet 则会按照 limits 的值来进行设置。

  - 更确切地说，当你指定了 requests.cpu=250m 之后，相当于将 Cgroups 的 cpu.shares 的值设置为 (250/1000)*1024。而当你没有设置 requests.cpu 的时候，cpu.shares 默认则是1024。这样，Kubernetes 就通过 cpu.shares 完成了对 CPU 时间的按比例分配。

  - 而如果你指定了 limits.cpu=500m 之后，则相当于将 Cgroups 的 cpu.cfs_quota_us 的值设置为 (500/1000)*100ms，而 cpu.cfs_period_us 的值始终是 100ms。这样，Kubernetes 就为你设置了这个容器只能用到 CPU 的 50%。

  - 而对于内存来说，当你指定了 limits.memory=128Mi 之后，相当于将 Cgroups 的memory.limit_in_bytes 设置为 128 * 1024 * 1024。而需要注意的是，在调度的时候，调度器只会使用 requests.memory=64Mi 来进行判断。

- **Kubernetes 这种对 CPU 和内存资源限额的设计，实际上参考了 Borg 论文中对“动态资源边界”的定义**，既：容器化作业在提交时所设置的资源边界，并不一定是调度系统所必须严格遵守的，这是因为在实际场景中，大多数作业使用到的资源其实远小于它所请求的资源限额。

  -  Kubernetes 的 requests+limits 的做法，其实就是上述思路的一个简化版：用户在提交Pod 时，可以声明一个相对较小的 requests 值供调度器使用，而 Kubernetes 真正设置给容器Cgroups 的，则是相对较大的 limits 值。不难看到，这跟 Borg 的思路相通的。

-  Kubernetes 里的 QoS 模型

  - 在Kubernetes 中，不同的 requests 和 limits 的设置方式，其实会将这个 Pod 划分到不同的QoS 级别当中。

  - **当 Pod 里的每一个 Container 都同时设置了 requests 和 limits，并且 requests 和 limits值相等的时候，这个 Pod 就属于 Guaranteed 类别**

  - ```
    apiVersion: v1
    kind: Pod
    metadata:
      name: qos-demo
      namespace: qos-example
    spec:
      containers:
      - name: qos-demo-ctr
        image: nginx
        resources:
          limits:
            memory: "200Mi"
            cpu: "700m"
          requests:
            memory: "200Mi"
            cpu: "700m"
    ```

    - 当这个 Pod 创建之后，它的 qosClass 字段就会被 Kubernetes 自动设置为 Guaranteed。需要注意的是，当 Pod 仅设置了 limits 没有设置 requests 的时候，Kubernetes 会自动为它设置与 limits 相同的 requests 值，所以，这也属于 Guaranteed 情况。

  - **而当 Pod 不满足 Guaranteed 的条件，但至少有一个 Container 设置了 requests。那么这个 Pod 就会被划分到 Burstable 类别**

    - ```
      apiVersion: v1
      kind: Pod
      metadata:
        name: qos-demo-2
        namespace: qos-example
      spec:
        containers:
        - name: qos-demo-2-ctr
          image: nginx
          resources:
            limits
              memory: "200Mi"
            requests:
              memory: "100Mi
      ```

  - **而如果一个 Pod 既没有设置 requests，也没有设置 limits，那么它的 QoS 类别就是BestEffort**。

    - ```
      apiVersion: v1
      kind: Pod
      metadata:
        name: qos-demo-3
        namespace: qos-example
      spec:
        containers:
        - name: qos-demo-3-ctr
          image: nginx
      ```

- 那么，Kubernetes 为 Pod 设置这样三种 QoS 类别，具体有什么作用呢？

  - 实际上，**QoS 划分的主要应用场景，是当宿主机资源紧张的时候，kubelet 对 Pod 进行Eviction（即资源回收）时需要用到的。**

  - 具体地说，当 Kubernetes 所管理的宿主机上不可压缩资源短缺时，就有可能触发 Eviction。比如，可用内存（memory.available）、可用的宿主机磁盘空间（nodefs.available），以及容器运行时镜像存储空间（imagefs.available）等等。

  - 目前，Kubernetes 为你设置的 Eviction 的默认阈值如下所示：

    - ```
      memory.available<100Mi
      nodefs.available<10%
      nodefs.inodesFree<5%
      imagefs.available<15%
      ```

  - **Eviction 在 Kubernetes 里其实分为 Soft 和 Hard 两种模式**
    - 其中，Soft Eviction 允许你为 Eviction 过程设置一段“优雅时间”，比如上面例子里的 imagefs.available=2m，就意味着当 imagefs 不足的阈值达到 2 分钟之后，kubelet 才会开始 Eviction 的过程。
    - 而 Hard Eviction 模式下，Eviction 过程就会在阈值达到之后立刻开始。

- 而当 Eviction 发生的时候，kubelet 具体会挑选哪些 Pod 进行删除操作，就需要参考这些 Pod的 QoS 类别了。

  - 首当其冲的，自然是 BestEffort 类别的 Pod。
  - 其次，是属于 Burstable 类别、并且发生“饥饿”的资源使用量已经超出了 requests 的Pod。
  - 最后，才是 Guaranteed 类别。并且，Kubernetes 会保证只有当 Guaranteed 类别的 Pod的资源使用量超过了其 limits 的限制，或者宿主机本身正处于 Memory Pressure 状态时，Guaranteed 的 Pod 才可能被选中进行 Eviction 操作。

- cpuset 的设置

  - 在使用容器的时候，你可以通过设置 cpuset 把容器绑定到某个 CPU 的核上，而不是像 cpushare 那样共享 CPU 的计算能力。

  - 这种情况下，由于操作系统在 CPU 之间进行上下文切换的次数大大减少，容器里应用的性能会得到大幅提升。事实上，**cpuset 方式，是生产环境里部署在线应用类型的 Pod 时，非常常用的一种方式。**

  - 首先，你的 Pod 必须是 Guaranteed 的 QoS 类型；

  - 然后，你只需要将 Pod 的 CPU 资源的 requests 和 limits 设置为同一个相等的整数值即可。

    - ```
      spec:
        containers:
        - name: nginx
          image: nginx
          resources:
            limits:
              memory: "200Mi"
              cpu: "2"
            requests:
              memory: "200Mi"
              cpu: "2"
      ```

  - 这时候，该 Pod 就会被绑定在 2 个独占的 CPU 核上。当然，具体是哪两个 CPU 核，是由kubelet 为你分配的。



## Kubernetes默认调度器

- **在 Kubernetes 项目中，默认调度器的主要职责，就是为一个新创建出来的 Pod，寻找一个最合适的节点（Node）。**

  - 而这里“最合适”的含义，包括两层：

    1. 从集群所有的节点中，根据调度算法挑选出所有可以运行该 Pod 的节点；

    2. 从第一步的结果中，再根据调度算法挑选一个最符合条件的节点作为最终结果。

  - 所以在具体的调度流程中，默认调度器会首先调用一组叫作 Predicate 的调度算法，来检查每个Node。然后，再调用一组叫作 Priority 的调度算法，来给上一步得到的结果里的每个 Node 打分。最终的调度结果，就是得分最高的那个 Node。

  - 调度器对一个 Pod 调度成功，实际上就是将它的spec.nodeName 字段填上调度结果的节点名字

- Kubernetes 的调度器的核心，实际上就是两个相互独立的控制循环

  - **第一个控制循环，我们可以称之为 Informer Path**。它的主要目的，是启动一系列Informer，用来监听（Watch）Etcd 中 Pod、Node、Service 等与调度相关的 API 对象的变化。比如，当一个待调度 Pod（即：它的 nodeName 字段是空的）被创建出来之后，调度器就会通过 Pod Informer 的 Handler，将这个待调度 Pod 添加进调度队列。
    - 在默认情况下，Kubernetes 的调度队列是一个 PriorityQueue（优先级队列），并且当某些集群信息发生变化的时候，调度器还会对调度队列里的内容进行一些特殊操作。这里的设计，主要是出于调度优先级和抢占的考虑，我会在后面的文章中再详细介绍这部分内容。
    - 此外，Kubernetes 的默认调度器还要负责对调度器缓存（即：scheduler cache）进行更新。事实上，Kubernetes 调度部分进行性能优化的一个最根本原则，就是尽最大可能将集群信息Cache 化，以便从根本上提高 Predicate 和 Priority 调度算法的执行效率。
  - **第二个控制循环，是调度器负责 Pod 调度的主循环，我们可以称之为 Scheduling Path**
    - Scheduling Path 的主要逻辑，就是不断地从调度队列里出队一个 Pod。然后，调用Predicates 算法进行“过滤”。这一步“过滤”得到的一组 Node，就是所有可以运行这个Pod 的宿主机列表。当然，Predicates 算法需要的 Node 信息，都是从 Scheduler Cache 里直接拿到的，这是调度器保证算法执行效率的主要手段之一。
    - 接下来，调度器就会再调用 Priorities 算法为上述列表里的 Node 打分，分数从 0 到 10。得分最高的 Node，就会作为这次调度的结果。
  - 调度算法执行完成后，调度器就需要将 Pod 对象的 nodeName 字段的值，修改为上述 Node的名字。**这个步骤在 Kubernetes 里面被称作 Bind。**
  - 但是，为了不在关键调度路径里远程访问 APIServer，Kubernetes 的默认调度器在 Bind 阶段，只会更新 Scheduler Cache 里的 Pod 和 Node 的信息。**这种基于“乐观”假设的 API 对象更新方式，在 Kubernetes 里被称作 Assume。**
  - Assume 之后，调度器才会创建一个 Goroutine 来异步地向 APIServer 发起更新 Pod 的请求，来真正完成 Bind 操作。如果这次异步的 Bind 过程失败了，其实也没有太大关系，等Scheduler Cache 同步之后一切就会恢复正常。
  - 当然，正是由于上述 Kubernetes 调度器的“乐观”绑定的设计，当一个新的 Pod 完成调度需要在某个节点上运行起来之前，该节点上的 kubelet 还会通过一个叫作 Admit 的操作来再次验证该 Pod 是否确实能够运行在该节点上。这一步 Admit 操作，实际上就是把一组叫作GeneralPredicates 的、最基本的调度算法，比如：“资源是否可用”“端口是否冲突”等再执行一遍，作为 kubelet 端的二次确认。
  - **除了上述的“Cache 化”和“乐观绑定”，Kubernetes 默认调度器还有一个重要的设计，那就是“无锁化”。**
    - Kubernetes 调度器只有对调度队列和 Scheduler Cache 进行操作时，才需要加锁。而这两部分操作，都不在 Scheduling Path 的算法执行路径上

- 而现在，随着 Kubernetes 项目逐步趋于稳定，越来越多的用户开始把 Kubernetes 用在规模更大、业务更加复杂的私有集群当中。很多以前的 Mesos 用户，也开始尝试使用 Kubernetes 来替代其原有架构。在这些场景下，对默认调度器进行扩展和重新实现，就成了社区对Kubernetes 项目最主要的一个诉求。



##  Kubernetes默认调度器调度策略

- Predicates
  - **Predicates 在调度过程中的作用，可以理解为 Filter**，即：它按照调度策略，从当前集群的所有节点中，“过滤”出一系列符合条件的节点。这些节点，都是可以运行待调度 Pod 的宿主机。
- 而在 Kubernetes 中，默认的调度策略有如下三种。
  - **第一种类型，叫作 GeneralPredicates**
    - 这一组过滤规则，负责的是最基础的调度策略。比如，PodFitsResources 计算的就是宿主机的 CPU 和内存资源等是否够用。
  - **第二种类型，是与 Volume 相关的过滤规则**
    - 这一组过滤规则，负责的是跟容器持久化 Volume 相关的调度策略。
  - **第三种类型，是宿主机相关的过滤规则**
    - 这一组规则，主要考察待调度 Pod 是否满足 Node 本身的某些条件。
  - **第四种类型，是 Pod 相关的过滤规则**
    - 这一组规则，跟 GeneralPredicates 大多数是重合的。而比较特殊的，是PodAffinityPredicate。这个规则的作用，是检查待调度 Pod 与 Node 上的已有 Pod 之间的亲密（affinity）和反亲密（anti-affinity）关系。
- 上面这四种类型的 Predicates，就构成了调度器确定一个 Node 可以运行待调度 Pod 的基本策略。
  - **在具体执行的时候， 当开始调度一个 Pod 时，Kubernetes 调度器会同时启动 16 个Goroutine，来并发地为集群里的所有 Node 计算 Predicates，最后返回可以运行这个 Pod的宿主机列表。**

-  Priorities
  - 在 Predicates 阶段完成了节点的“过滤”之后，Priorities 阶段的工作就是为这些节点打分。这里打分的范围是 0-10 分，得分最高的节点就是最后被 Pod 绑定的最佳节点。
  - Priorities 里最常用到的一个打分规则，是 LeastRequestedPriority，这个算法实际上就是在选择空闲资源（CPU 和 Memory）最多的宿主机。
  - 而与 LeastRequestedPriority 一起发挥作用的，还有 BalancedResourceAllocation
    - BalancedResourceAllocation 选择的，其实是调度完成后，所有节点里各种资源分配最均衡的那个节点，从而避免一个节点上 CPU 被大量分配、而 Memory 大量剩余的情况。
    - 此外，还有 NodeAffinityPriority、TaintTolerationPriority 和 InterPodAffinityPriority 这三种 Priority。顾名思义，它们与前面的 PodMatchNodeSelector、PodToleratesNodeTaints和 PodAffinityPredicate 这三个 Predicate 的含义和计算方法是类似的。但是作为 Priority，一个 Node 满足上述规则的字段数目越多，它的得分就会越高
  - 在默认 Priorities 里，还有一个叫作 ImageLocalityPriority 的策略。它是在 Kubernetes v1.12里新开启的调度规则，即：如果待调度 Pod 需要使用的镜像很大，并且已经存在于某些 Node上，那么这些 Node 的得分就会比较高。
- **在实际的执行过程中，调度器里关于集群和 Pod 的信息都已经缓存化，所以这些算法的执行过程还是比较快的**

- 需要注意的是，除了本篇讲述的这些规则，Kubernetes 调度器里其实还有一些默认不会开启的策略。你可以通过为 kube-scheduler 指定一个配置文件或者创建一个 ConfigMap ，来配置哪些规则需要开启、哪些规则需要关闭。并且，你可以通过为 Priorities 设置权重，来控制调度器的调度行为。



## Kubernetes 默认调度器的优先级与抢占机制

- 首先需要明确的是，优先级和抢占机制，解决的是 Pod 调度失败时该怎么办的问题。

  - 正常情况下，当一个 Pod 调度失败后，它就会被暂时“搁置”起来，直到 Pod 被更新，或者集群状态发生变化，调度器才会对这个 Pod 进行重新调度。
  - 但在有时候，我们希望的是这样一个场景。当一个高优先级的 Pod 调度失败后，该 Pod 并不会被“搁置”，而是会“挤走”某个 Node 上的一些低优先级的 Pod 。这样就可以保证这个高优先级 Pod 的调度成功。这个特性，其实也是一直以来就存在于 Borg 以及 Mesos 等项目里的一个基本功能。

- 要使用这个机制，你首先需要在 Kubernetes 里提交一个 PriorityClass 的定义

  - ```
    apiVersion: scheduling.k8s.io/v1beta1
    kind: PriorityClass
    metadata:
      name: high-priority
    value: 1000000
    globalDefault: false
    ```

  - **Kubernetes 规定，优先级是一个 32 bit 的整数，最大值不超过 1000000000（10 亿，1billion），并且值越大代表优先级越高**

  - 而一旦上述 YAML 文件里的 globalDefault 被设置为 true 的话，那就意味着这个PriorityClass 的值会成为系统的默认值。而如果这个值是 false，就表示我们只希望声明使用该PriorityClass 的 Pod 拥有值为 1000000 的优先级，而对于没有声明 PriorityClass 的 Pod 来说，它们的优先级就是 0。

- 在创建了 PriorityClass 对象之后，Pod 就可以声明使用它了

  - ```
    apiVersion: v1
    kind: Pod
    metadata:
      name: nginx
      labels:
        env: test
    spec:
      containers:
      - name: nginx
        image: nginx
        imagePullPolicy: IfNotPresent
      priorityClassName: high-priority
    ```

- 调度器里维护着一个调度队列。所以，当 Pod 拥有了优先级之后，高优先级的 Pod 就可能会比低优先级的 Pod 提前出队，从而尽早完成调度过程。**这个过程，就是“优先级”这个概念在 Kubernetes 里的主要体现。**

- 而当一个高优先级的 Pod 调度失败的时候，调度器的抢占能力就会被触发。这时，调度器就会试图从当前集群里寻找一个节点，使得当这个节点上的一个或者多个低优先级 Pod 被删除后，待调度的高优先级 Pod 就可以被调度到这个节点上。**这个过程，就是“抢占”这个概念在Kubernetes 里的主要体现。**

- 为了方便叙述，我接下来会把待调度的高优先级 Pod 称为“抢占者”（Preemptor）。

  - 当上述抢占过程发生时，抢占者并不会立刻被调度到被抢占的 Node 上。事实上，调度器只会将抢占者的 spec.nominatedNodeName 字段，设置为被抢占的 Node 的名字。然后，抢占者会重新进入下一个调度周期，然后在新的调度周期里来决定是不是要运行在被抢占的节点上。这当然也就意味着，即使在下一个调度周期，调度器也不会保证抢占者一定会运行在被抢占的节点上。
  - 这样设计的一个重要原因是，调度器只会通过标准的 DELETE API 来删除被抢占的 Pod，所以，这些 Pod 必然是有一定的“优雅退出”时间（默认是 30s）的。而在这段时间里，其他的节点也是有可能变成可调度的，或者直接有新的节点被添加到这个集群中来。所以，鉴于优雅退出期间，集群的可调度性可能会发生的变化，**把抢占者交给下一个调度周期再处理，是一个非常合理的选择。**

- Kubernetes 调度器里的抢占机制，又是如何设计的呢

  - 抢占发生的原因，一定是一个高优先级的 Pod 调度失败。这一次，我们还是称这个 Pod 为“抢占者”，称被抢占的 Pod 为“牺牲者”（victims）。
  - 而 Kubernetes 调度器实现抢占算法的一个最重要的设计，就是在调度队列的实现里，使用了两个不同的队列。
    - **第一个队列，叫作 activeQ。**凡是在 activeQ 里的 Pod，都是下一个调度周期需要调度的对象。所以，当你在 Kubernetes 集群里新创建一个 Pod 的时候，调度器会将这个 Pod 入队到activeQ 里面。而我在前面提到过的、调度器不断从队列里出队（Pop）一个 Pod 进行调度，实际上都是从 activeQ 里出队的。
    - **第二个队列，叫作 unschedulableQ**，专门用来存放调度失败的 Pod。而这里的一个关键点就在于，当一个 unschedulableQ 里的 Pod 被更新之后，调度器会自动把这个 Pod 移动到 activeQ 里，从而给这些调度失败的 Pod “重新做人”的机会。调度失败之后，抢占者就会被放进 unschedulableQ 里面。



## Prometheus、Metrics Server与Kubernetes监控体系

- 作为一个监控系统，Prometheus 项目的作用和工作方式，其实可以用如下所示的一张官方示意图来解释。
  - ![img](https://static001.geekbang.org/resource/image/2a/d3/2ada1ece66fcc81d704c2ba46f9dd7d3.png)
  - 可以看到，Prometheus 项目工作的核心，是使用 Pull （抓取）的方式去搜集被监控对象的 Metrics 数据（监控指标数据），然后，再把这些数据保存在一个 TSDB （时间序列数据库，比如 OpenTSDB、InfluxDB 等）当中，以便后续可以按照时间进行检索。
  - 有了这套核心监控机制， Prometheus 剩下的组件就是用来配合这套机制的运行。比如 Pushgateway，可以允许被监控对象以 Push 的方式向 Prometheus 推送 Metrics 数据。而 Alertmanager，则可以根据 Metrics 信息灵活地设置报警。当然， Prometheus 最受用户欢迎的功能，还是通过 Grafana 对外暴露出的、可以灵活配置的监控数据可视化界面。
  - 有了 Prometheus 之后，我们就可以按照 Metrics 数据的来源，来对 Kubernetes 的监控体系做一个汇总了。

- **第一种 Metrics，是宿主机的监控数据。** 这部分数据的提供，需要借助一个由 Prometheus 维护的[Node Exporter](https://github.com/prometheus/node_exporter) 工具。一般来说，Node Exporter 会以 DaemonSet 的方式运行在宿主机上。其实，所谓的 Exporter，就是代替被监控对象来对 Prometheus 暴露出可以被“抓取”的 Metrics 信息的一个辅助进程。
  
  - 而 Node Exporter 可以暴露给 Prometheus 采集的 Metrics 数据， 也不单单是节点的负载（Load）、CPU 、内存、磁盘以及网络这样的常规信息，它的 Metrics 指标可以说是“包罗万象”，你可以查看[这个列表](https://github.com/prometheus/node_exporter#enabled-by-default)来感受一下。
- **第二种 Metrics，是来自于 Kubernetes 的 API Server、kubelet 等组件的 /metrics API**。除了常规的 CPU、内存的信息外，这部分信息还主要包括了各个组件的核心监控指标。比如，对于 API Server 来说，它就会在 /metrics API 里，暴露出各个 Controller 的工作队列（Work Queue）的长度、请求的 QPS 和延迟数据等等。这些信息，是检查 Kubernetes 本身工作情况的主要依据。
- **第三种 Metrics，是 Kubernetes 相关的监控数据。** 这部分数据，一般叫作 Kubernetes 核心监控数据（core metrics）。这其中包括了 Pod、Node、容器、Service 等主要 Kubernetes 核心概念的 Metrics。
  
  - 其中，容器相关的 Metrics 主要来自于 kubelet 内置的 cAdvisor 服务。在 kubelet 启动后，cAdvisor 服务也随之启动，而它能够提供的信息，可以细化到每一个容器的 CPU 、文件系统、内存、网络等资源的使用情况。
- 需要注意的是，这里提到的 Kubernetes 核心监控数据，其实使用的是 Kubernetes 的一个非常重要的扩展能力，叫作 Metrics Server。

- 有了 Metrics Server 之后，用户就可以通过标准的 Kubernetes API 来访问到这些监控数据了

  - http://127.0.0.1:8001/apis/metrics.k8s.io/v1beta1/namespaces/<namespace-name>/pods/<pod-name>

- 需要指出的是， Metrics Server 并不是 kube-apiserver 的一部分，而是通过 Aggregator 这种插件机制，在独立部署的情况下同 kube-apiserver 一起统一对外服务的。

- Aggregator 功能开启之后，你只需要将 Metrics Server 的 YAML 文件部署起来

  - ```
    
    $ git clone https://github.com/kubernetes-incubator/metrics-server
    $ cd metrics-server
    $ kubectl create -f deploy/1.8+/
    ```

  - 在理解了 Prometheus 关心的三种监控数据源，以及 Kubernetes 的核心 Metrics 之后，作为用户，你其实要做的就是将 Prometheus Operator 在 Kubernetes 集群里部署起来。然后，按照本篇文章一开始介绍的架构，把上述 Metrics 源配置起来，让 Prometheus 自己去进行采集即可。

- 最后，在具体的监控指标规划上，我建议你遵循业界通用的 USE 原则和 RED 原则。
- 其中，USE 原则指的是，按照如下三个维度来规划资源监控指标：利用率（Utilization），资源被有效利用起来提供服务的平均时间占比；饱和度（Saturation），资源拥挤的程度，比如工作队列的长度；错误率（Errors），错误的数量。
- 而 RED 原则指的是，按照如下三个维度来规划服务监控指标：每秒请求数量（Rate）；每秒错误数量（Errors）；服务响应时间（Duration）。
- 不难发现， USE 原则主要关注的是“资源”，比如节点和容器的资源使用情况，而 RED 原则主要关注的是“服务”，比如 kube-apiserver 或者某个应用的工作情况。这两种指标，在我今天为你讲解的 Kubernetes + Prometheus 组成的监控体系中，都是可以完全覆盖到的。









## **容器日志收集与管理**

- 首先需要明确的是，Kubernetes 里面对容器日志的处理方式，都叫作 cluster-level-logging，即：这个日志处理系统，与容器、Pod 以及 Node 的生命周期都是完全无关的。这种设计当然是为了保证，无论是容器挂了、Pod 被删除，甚至节点宕机的时候，应用的日志依然可以被正常获取到。

  - 而对于一个容器来说，当应用把日志输出到 stdout 和 stderr 之后，容器项目在默认情况下就会把这些日志输出到宿主机上的一个 JSON 文件里。这样，你通过 kubectl logs 命令就可以看到这些容器的日志了。

- 而 Kubernetes 本身，实际上是不会为你做容器日志收集工作的，所以为了实现上述 clusterlevel-logging，你需要在部署集群的时候，提前对具体的日志方案进行规划。而 Kubernetes项目本身，主要为你推荐了三种日志方案。

  - **第一种，在 Node 上部署 logging agent，将日志文件转发到后端存储里保存起来**

    - ![image.png](https://img.hacpai.com/file/2020/01/image-f881d69a.png?imageView2/2/interlace/1/format/jpg)
    - 不难看到，这里的核心就在于 logging agent ，它一般都会以 DaemonSet 的方式运行在节点上，然后将宿主机上的容器日志目录挂载进去，最后由 logging-agent 把日志转发出去。
    - 举个例子，我们可以通过+Fluentd+项目作为宿主机上的+logging-agent，然后把日志转发到远端的+ElasticSearch+里保存起来供将来进行检索。具体的操作过程[官网](https://link.juejin.im/?target=https%3A%2F%2Fkubernetes.io%2Fdocs%2Ftasks%2Fdebug-application-cluster%2Flogging-elasticsearch-kibana%2F)很多kubernetes的部署里，会自动为你启用logrotate，在日志文件找过10MB的时候自动对日志文件进行roate操作。
    - 可以看到在Node上部署logging agent最大的优点，在于只需要部署一个agent，并且不会对应用和pod有任何入侵性。所以这个方案在这区里是最常用的一种。
    - 但是这种方案的补助支出就在于，它要求应用输出的日志，都必须是直接输出到容器的stdout和stderr里

  - **Kubernetes 容器日志方案的第二种，就是对这种特殊情况的一个处理，即：当容器的日志只能输出到某些文件里的时候，我们可以通过一个 sidecar 容器把这些日志文件重新输出到sidecar 的 stdout 和 stderr 上，这样就能够继续使用第一种方案了**

    - ![image.png](https://img.hacpai.com/file/2020/01/image-f881d69a.png?imageView2/2/interlace/1/format/jpg)

    - 在这种情况下，你用 kubectl logs 命令是看不到应用的任何日志的。而且我们前面讲解的、最常用的方案一，也是没办法使用的。

      - ```
        
        apiVersion: v1
        kind: Pod
        metadata:
          name: counter
        spec:
          containers:
          - name: count
            image: busybox
            args:
            - /bin/sh
            - -c
            - >
              i=0;
              while true;
              do
                echo "$i: $(date)" >> /var/log/1.log;
                echo "$(date) INFO $i" >> /var/log/2.log;
                i=$((i+1));
                sleep 1;
              done
            volumeMounts:
            - name: varlog
              mountPath: /var/log
          volumes:
          - name: varlog
        
        ```

        

    - 那么这个时候，我们就可以为这个 Pod 添加两个 sidecar 容器，分别将上述两个日志文件里的内容重新以 stdout 和 stderr 的方式输出出来

      - ```
        apiVersion: v1
        kind: Pod
        metadata:
          name: counter
        spec:
          containers:
          - name: count
            image: busybox
            args:
            - /bin/sh
            - -c
            - >
              i=0;
              while true;
              do
                echo "$i: $(date)" >> /var/log/1.log;
                echo "$(date) INFO $i" >> /var/log/2.log;
                i=$((i+1));
                sleep 1;
              done
            volumeMounts:
            - name: varlog
              mountPath: /var/log
          - name: count-log-1
            image: busybox
            args: [/bin/sh, -c, 'tail -n+1 -f /var/log/1.log']
            volumeMounts:
            - name: varlog
              mountPath: /var/log
          - name: count-log-2
            image: busybox
            args: [/bin/sh, -c, 'tail -n+1 -f /var/log/2.log']
            volumeMounts:
            - name: varlog
              mountPath: /var/log
          volumes:
          - name: varlog
            emptyDir: {}
        
        ```

        

    - 由于 sidecar 跟主容器之间是共享 Volume 的，所以这里的 sidecar 方案的额外性能损耗并不高，也就是多占用一点 CPU 和内存罢了。

    - 但需要注意的是，这时候，宿主机上实际上会存在两份相同的日志文件：一份是应用自己写入的；另一份则是 sidecar 的 stdout 和 stderr 对应的 JSON 文件。这对磁盘是很大的浪费。

    - 所以说，除非万不得已或者应用容器完全不可能被修改，否则我还是建议你直接使用方案一，或者直接使用下面的第三种方案。

  - **第三种方案，就是通过一个 sidecar 容器，直接把应用的日志文件发送到远程存储里面去。**也就是相当于把方案一里的 logging agent，放在了应用 Pod 里。

    - ![image.png](https://img.hacpai.com/file/2020/01/image-b3be12e3.png?imageView2/2/interlace/1/format/jpg)
    - 在这种方案里，你的应用还可以直接把日志输出到固定的文件里而不是 stdout，你的 loggingagent 还可以使用 fluentd，后端存储还可以是 ElasticSearch。只不过， fluentd 的输入源，变成了应用的日志文件。一般来说，我们会把 fluentd 的输入源配置保存在一个 ConfigMap里
    - 然后，我们在应用 Pod 的定义里，就可以声明一个 Fluentd 容器作为 sidecar，专门负责将应用生成的 1.log 和 2.log 转发到 ElasticSearch 当中。
    - 可以看到，这个 Fluentd 容器使用的输入源，就是通过引用我们前面编写的 ConfigMap 来指定的。这里我用到了 Projected Volume 来把 ConfigMap 挂载到 Pod 里。

- 综合对比以上三种方案，我比较建议你将应用日志输出到 stdout 和 stderr，然后通过在宿主机上部署logging-agent 的方式来集中处理日志。这种方案不仅管理简单，kubectl logs 也可以用，而且可靠性高，并且宿主机本身，很可能就自带了 rsyslogd 等非常成熟的日志收集组件来供你使用

- 除此之外，还有一种方式就是在编写应用的时候，就直接指定好日志的存储后端

  - 在这种方案下，Kubernetes 就完全不必操心容器日志的收集了，这对于本身已经有完善的日志处理系统的公司来说，是一个非常好的选择。
  - 最后需要指出的是，无论是哪种方案，你都必须要及时将这些日志文件从宿主机上清理掉，或者给日志目录专门挂载一些容量巨大的远程盘。否则，一旦主磁盘分区被打满，整个系统就可能会陷入奔溃状态，这是非常麻烦的。







## k8s容灾

**有效的Kubernetes容灾恢复方案必须针对容器化架构进行重新设计，并按Kubernetes的原生方式来运行。**

一个有效的Kubernetes容灾解决方案需要具备：

- 容器粒度的控制
  - 容器粒度控制容灾方案意味着用户可以备份特定的Pod或Pod组，而不是备份整个VM或服务器。这使得用户可以仅快照属于该应用程序的容器
  - 
- 能够备份数据和配置
- Kubernetes命名空间感知
- 针对多云和混合云架构的优化
- 保持应用的一致性





## Kubernetes生产架构

首先，我们来梳理下Kubernetes生产环境一个粗略的业务架构，如下图所示。
[![image](https://xuchao918.github.io/images/k8s-jiagou.png)](https://xuchao918.github.io/images/k8s-jiagou.png)

在该架构中，我们可以将其分为四层，如下：

- Client层：即Kubernetes集群外部用户、客户端等；
- 服务访问层：即由Traefik ingress实现服务发现、负载均衡和路由规则定义等；
- 业务应用层：即基于Kubernetes平台构建和运行企业业务应用，如CI/CD持续集成、微服务项目、监控告警和日志管理、私有镜像仓库等服务；
- 基础设施层：即由Kubernetes容器管理平台和Ceph数据持久化存储等系统组成的基础设施服务。

下面，我们分别来谈谈各层的具体实现方案。

### 基础设施层

**Kubernetes平台**

- 部署管理：Kubernetes平台除了直接使用公有云如阿里云、AWS等云服务提供商的K8s服务外，我们还可以自己部署和管理等，如使用Kubespray工具。
- 网络通信：在容器和容器之间、容器和主机网络方面，可以使用Calico或Flannel等方案。
- HA高可用：Kubernetes节点分为Master和Node两种类型节点，前者负责运行集群相关的控制管理服务，而后者负责运行Pod容器。在多Node节点模式下，由于Kubernetes Pod具有天然的容灾冗余HA高可用实现，因此，我们并不需要关心Node节点的HA高可用，而只需关心Master节点的HA即可，Master节点的HA高可用，通过多Master节点+HAProxy方案实现即可。从Kubernetes 1.12版本起，kube-proxy服务默认使用ipvs实现，取消了之前的iptables。这有助于提升K8s大规模集群环境下的性能和稳定性。
- Docker和操作系统优化：在生产环境中，Docker和操作系统版本应当使用较新的release版本。并且，主机操作系统应当做一定程度的优化配置，如关闭swap内存交换分区，预留一定的CPU核数和内存资源给宿主机使用等。

**Ceph/NFS数据存储**
Kubernetes平台的数据持久化存储，可以使用Ceph、NFS等存储方案。其中，Ceph适用于有其技术背景或大容量存储需求的公司；而NFS适用于存储容量需求相对较小，无专业存储技术背景的公司。

### 业务应用层

- 镜像管理：使用Harbor私有镜像仓库服务；
- 日志管理：使用Elasticsearch、Filebeat 和 Kibana技术栈；
- 监控告警管理：使用Cadvisor、Prometheus和Grafana技术栈；
- 微服务架构：使用Service Mesh服务网格中的Istio方案；
- Devops：使用Gitlab、Jenkins等持续集成工具；
- 单体应用：无状态类服务使用deployment，有状态类服务则使用Statefulset，如果关联的服务较多且复杂则使用Helm。
- 规划好Namespace：应当做到每个namespace专属用于某类型的应用，如monitor namespace统一管理诸如监控告警和日志管理方面的pod、service、pvc、ingress等资源。这样，可以较为方便的管理和区分K8s上的各种应用。

### 服务访问层

外部客户端访问K8s集群内的服务、负载均衡和路由规则定义使用Traefik Ingress实现。此外，应当实现Ingress服务HA高可用，可以想象在K8s集群中，大量的出入口流量都进过Ingress，其负载是非常大的，其重要程度不言而喻，因此实现HA就非常重要。ingress controller节点（无论是基于nginx还是traefik实现）应当至少为2个节点，并在这些节点上，部署Keepalived和HAproxy共同维护一个VIP地址，将其提供给ingress使用。

架构如下图所示。
[![image](https://xuchao918.github.io/images/ingress-ha.png)](https://xuchao918.github.io/images/ingress-ha.png)

在该架构中，Ingress节点一般使用独立的服务器部署在K8s平台的边缘节点，即只做将集群外部流量接入到集群内部。除了使用external Ip来暴露ingress的Service到集群外部，还可以使用hostNetwork，如果是公有云，还可以使用LoadBalance。这样Ingress Controller将监听节点的80和443端口。

如上图所示，当外部客户端访问域名”[www.taobao.com“时（举例而已，切勿当真），首先经过LVS/F5第一层负载均衡，然后经由两个Traefik](http://www.taobao.com“时（举例而已，切勿当真），首先经过LVS/F5第一层负载均衡，然后经由两个Traefik) Ingress集群做第二层负载分发（每个集群由2个节点实现主备HA高可用），其次客户端流量请求转发到第三层负载均衡Service，最后转发到后端对应的Pod。K8s平台和后端业务平台均是内网环境，通过第一层负载均衡的公网IP进行转发或映射，实现内外网通信和安全隔离。















​	